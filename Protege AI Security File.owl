Prefix(:=<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES/>)
Prefix(hes:=<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#>)
Prefix(owl:=<http://www.w3.org/2002/07/owl#>)
Prefix(rdf:=<http://www.w3.org/1999/02/22-rdf-syntax-ns#>)
Prefix(xml:=<http://www.w3.org/XML/1998/namespace>)
Prefix(xsd:=<http://www.w3.org/2001/XMLSchema#>)
Prefix(rdfs:=<http://www.w3.org/2000/01/rdf-schema#>)
Prefix(swrl:=<http://www.w3.org/2003/11/swrl#>)
Prefix(swrla:=<http://swrl.stanford.edu/ontologies/3.3/swrla.owl#>)
Prefix(swrlb:=<http://www.w3.org/2003/11/swrlb#>)
Prefix(aisecurityknowledgebase:=<http://www.semanticweb.org/ubaidullah/ontologies/AISecurityKnowledgeBase#>)


Ontology(<http://www.semanticweb.org/ubaidullah/ontologies/AISecurityKnowledgeBase>

Declaration(Class(hes:AIEnabledSystems))
Declaration(Class(hes:AILifeCycle))
Declaration(Class(hes:AMLA0001))
Declaration(Class(hes:AMLA0002))
Declaration(Class(hes:AMLA0003))
Declaration(Class(hes:AMLA0004))
Declaration(Class(hes:AMLA0005))
Declaration(Class(hes:AMLA0006))
Declaration(Class(hes:AMLA0007))
Declaration(Class(hes:AMLA0008))
Declaration(Class(hes:AMLA0009))
Declaration(Class(hes:AMLA0010))
Declaration(Class(hes:AMLA0011))
Declaration(Class(hes:AMLA0012))
Declaration(Class(hes:AMLA0013))
Declaration(Class(hes:AMLA0014))
Declaration(Class(hes:AMLA0015))
Declaration(Class(hes:AMLA0016))
Declaration(Class(hes:AMLA0017))
Declaration(Class(hes:AMLA0018))
Declaration(Class(hes:AMLA0019))
Declaration(Class(hes:AMLA0020))
Declaration(Class(hes:AMLA0021))
Declaration(Class(hes:AMLA0022))
Declaration(Class(hes:AMLA0023))
Declaration(Class(hes:AccessControlTechniques))
Declaration(Class(hes:AdversarialSamples))
Declaration(Class(hes:Adversary))
Declaration(Class(hes:Algorithm))
Declaration(Class(hes:AlgorithmWeaknesses))
Declaration(Class(hes:AnomalyDetector))
Declaration(Class(hes:ArbitraryCodeExecution))
Declaration(Class(hes:Assessment))
Declaration(Class(hes:Assets))
Declaration(Class(hes:AttackStaging))
Declaration(Class(hes:AuthenticationEvasion))
Declaration(Class(hes:Backdoor))
Declaration(Class(hes:BackdoorDetector))
Declaration(Class(hes:BotDetector))
Declaration(Class(hes:CAPECA0001))
Declaration(Class(hes:CAPECA0002))
Declaration(Class(hes:CAPECA0003))
Declaration(Class(hes:CAPECA0004))
Declaration(Class(hes:CAPECA0005))
Declaration(Class(hes:CAPECA0006))
Declaration(Class(hes:CAPECA0007))
Declaration(Class(hes:CAPECA0008))
Declaration(Class(hes:CAPECA0009))
Declaration(Class(hes:CAPECA0010))
Declaration(Class(hes:CIA))
Declaration(Class(hes:ChatBot))
Declaration(Class(hes:ChatSystem))
Declaration(Class(hes:Collection))
Declaration(Class(hes:CompromisedDuringDevelopment))
Declaration(Class(hes:ConcurrentExecution))
Declaration(Class(hes:CraftedPrompts))
Declaration(Class(hes:CredentialAccess))
Declaration(Class(hes:CryptographicTechniques))
Declaration(Class(hes:Data))
Declaration(Class(hes:DataWeaknesses))
Declaration(Class(hes:DeepLearningDetector))
Declaration(Class(hes:DefenseEvasion))
Declaration(Class(hes:DeserializationOfUntrustedData))
Declaration(Class(hes:Discovery))
Declaration(Class(hes:EmbeddedSecurity))
Declaration(Class(hes:ExcessiveAgency))
Declaration(Class(hes:Execution))
Declaration(Class(hes:Exfiltration))
Declaration(Class(hes:FaultyInputs))
Declaration(Class(hes:Hardware))
Declaration(Class(hes:HardwareSecurity))
Declaration(Class(hes:HardwareWeaknesses))
Declaration(Class(hes:Human-centricFactors))
Declaration(Class(hes:Impact))
Declaration(Class(hes:Impacts))
Declaration(Class(hes:ImproperAuthentication))
Declaration(Class(hes:ImproperControl))
Declaration(Class(hes:ImproperInputValidation))
Declaration(Class(hes:ImproperNeutralization))
Declaration(Class(hes:ImproperPrivilegeManagement))
Declaration(Class(hes:IncorrectAuthorization))
Declaration(Class(hes:Information))
Declaration(Class(hes:InfrastructureWeaknesses))
Declaration(Class(hes:InitialAccess))
Declaration(Class(hes:InsecureObjectHandling))
Declaration(Class(hes:InsecurePluginDesign))
Declaration(Class(hes:IntegerOverflow))
Declaration(Class(hes:IntrusionDetector))
Declaration(Class(hes:IntrusionPrevention))
Declaration(Class(hes:LLM))
Declaration(Class(hes:Library))
Declaration(Class(hes:Likelihood))
Declaration(Class(hes:MalaciousActivity))
Declaration(Class(hes:MaliciousData))
Declaration(Class(hes:MaliciousLogic))
Declaration(Class(hes:MalwareDetector))
Declaration(Class(hes:Misconfiguation))
Declaration(Class(hes:MissingAuthentication))
Declaration(Class(hes:MissingAuthorization))
Declaration(Class(hes:Mitigations))
Declaration(Class(hes:Model))
Declaration(Class(hes:ModelAccess))
Declaration(Class(hes:ModelDenialOfService))
Declaration(Class(hes:ModelTheft))
Declaration(Class(hes:ModelWeaknesses))
Declaration(Class(hes:Network))
Declaration(Class(hes:NetworkSecurity))
Declaration(Class(hes:NetworkWeaknesses))
Declaration(Class(hes:OrganizationalWeaknesses))
Declaration(Class(hes:Out-of-boundsRead))
Declaration(Class(hes:Out-of-boundsWrite))
Declaration(Class(hes:Overreliance))
Declaration(Class(hes:Persistence))
Declaration(Class(hes:Platform))
Declaration(Class(hes:Plugins))
Declaration(Class(hes:PrivilegeEscalation))
Declaration(Class(hes:PromptInjection))
Declaration(Class(hes:Protocol))
Declaration(Class(hes:Reconnaissance))
Declaration(Class(hes:Resource))
Declaration(Class(hes:ResourceDevelopment))
Declaration(Class(hes:Risk))
Declaration(Class(hes:RiskAssessment))
Declaration(Class(hes:RootkitHunter))
Declaration(Class(hes:SecurityMechanism))
Declaration(Class(hes:SecurityThreats))
Declaration(Class(hes:SensitiveInformationDisclosure))
Declaration(Class(hes:Software))
Declaration(Class(hes:SoftwareWeaknesses))
Declaration(Class(hes:SourceFile))
Declaration(Class(hes:Sourcecode))
Declaration(Class(hes:SpywareScanner))
Declaration(Class(hes:SupplyChainVulnerabilities))
Declaration(Class(hes:System))
Declaration(Class(hes:Tactics))
Declaration(Class(hes:Techniques))
Declaration(Class(hes:Tool))
Declaration(Class(hes:TrainingDataPoisioning))
Declaration(Class(hes:UnauthorizedResources))
Declaration(Class(hes:UseOfHard-codedCredentials))
Declaration(Class(hes:VulnerabilityAssessment))
Declaration(Class(hes:Weaknesses))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#1.Business&DataUnderstanding>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#2.DataPreparation>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#3.ModelEngineering>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#4.ModelEvaluation>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#5.Deployment>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#6.Monitoring&Maintenance>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.AdditionalPhases>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.1.AI-BasedAccessManagement>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.2.AI-BasedSoftwareSecurityEngineering>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.3.AI-BasedHardwareSecurityEngineering>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.4.AdditionalActivities>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackComplexity(AC)>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackVector(AV)>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Availability(A)>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Confidentiality(C)>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Integrity(I)>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#PrivilegesRequired(PR)>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Scope(S)>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Server-SideRequestForgery(SSRF)>))
Declaration(Class(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#UserInteraction(UI)>))
Declaration(Class(owl:Thing))
Declaration(ObjectProperty(hes:Allocates))
Declaration(ObjectProperty(hes:AlterCommunicationOf))
Declaration(ObjectProperty(hes:AreIncludedIn))
Declaration(ObjectProperty(hes:Cause))
Declaration(ObjectProperty(hes:ComposedOf))
Declaration(ObjectProperty(hes:ConsistOf))
Declaration(ObjectProperty(hes:Contains))
Declaration(ObjectProperty(hes:Exploits))
Declaration(ObjectProperty(hes:ExposedBy))
Declaration(ObjectProperty(hes:Has))
Declaration(ObjectProperty(hes:HasWeaknesses))
Declaration(ObjectProperty(hes:Injects))
Declaration(ObjectProperty(hes:IsAccessedBy))
Declaration(ObjectProperty(hes:IsBypassedBy))
Declaration(ObjectProperty(hes:IsDeducedBy))
Declaration(ObjectProperty(hes:IsDetectedBy))
Declaration(ObjectProperty(hes:IsDisruptedBy))
Declaration(ObjectProperty(hes:IsEvadedBy))
Declaration(ObjectProperty(hes:IsExploitedBy))
Declaration(ObjectProperty(hes:IsHijakedBy))
Declaration(ObjectProperty(hes:IsImpactedBy))
Declaration(ObjectProperty(hes:IsIncludedIn))
Declaration(ObjectProperty(hes:IsInitiatedBy))
Declaration(ObjectProperty(hes:IsInjectedInto))
Declaration(ObjectProperty(hes:IsInjectedWith))
Declaration(ObjectProperty(hes:IsInsertedInto))
Declaration(ObjectProperty(hes:IsManipulatedBy))
Declaration(ObjectProperty(hes:IsMisconfigured))
Declaration(ObjectProperty(hes:IsMitigatedBy))
Declaration(ObjectProperty(hes:IsPoisionedBy))
Declaration(ObjectProperty(hes:IsPreventedBy))
Declaration(ObjectProperty(hes:IsProtectedBy))
Declaration(ObjectProperty(hes:IsReplicatedBy))
Declaration(ObjectProperty(hes:IsReverseEngineersBy))
Declaration(ObjectProperty(hes:IsThreatenedBy))
Declaration(ObjectProperty(hes:IsUnder))
Declaration(ObjectProperty(hes:IsUnderDoSAttack))
Declaration(ObjectProperty(hes:IsVulnerableTo))
Declaration(ObjectProperty(hes:LeadsTo))
Declaration(ObjectProperty(hes:Performs))
Declaration(ObjectProperty(hes:ProtectsFrom))
Declaration(ObjectProperty(hes:SWRLRuleProperties))
Declaration(ObjectProperty(hes:Uses))
Declaration(ObjectProperty(hes:UsesTactic))
Declaration(ObjectProperty(hes:Violates))
Declaration(NamedIndividual(hes:AISystem))
Declaration(NamedIndividual(hes:AML.A0001))
Declaration(NamedIndividual(hes:AML.A0002))
Declaration(NamedIndividual(hes:AML.A0003))
Declaration(NamedIndividual(hes:AML.A0004))
Declaration(NamedIndividual(hes:AML.A0005))
Declaration(NamedIndividual(hes:AML.A0006))
Declaration(NamedIndividual(hes:AML.A0007))
Declaration(NamedIndividual(hes:AML.A0008))
Declaration(NamedIndividual(hes:AML.A0009))
Declaration(NamedIndividual(hes:AML.A0010))
Declaration(NamedIndividual(hes:AML.A0011))
Declaration(NamedIndividual(hes:AML.A0012))
Declaration(NamedIndividual(hes:AML.A0013))
Declaration(NamedIndividual(hes:AML.A0014))
Declaration(NamedIndividual(hes:AML.A0015))
Declaration(NamedIndividual(hes:AML.A0016))
Declaration(NamedIndividual(hes:AML.A0017))
Declaration(NamedIndividual(hes:AML.A0018))
Declaration(NamedIndividual(hes:AML.A0019))
Declaration(NamedIndividual(hes:AML.A0020))
Declaration(NamedIndividual(hes:AML.A0021))
Declaration(NamedIndividual(hes:AML.A0022))
Declaration(NamedIndividual(hes:AML.A0023))
Declaration(NamedIndividual(hes:AML.M0001))
Declaration(NamedIndividual(hes:AML.M0002))
Declaration(NamedIndividual(hes:AML.M0003))
Declaration(NamedIndividual(hes:AML.M0004))
Declaration(NamedIndividual(hes:AML.M0005))
Declaration(NamedIndividual(hes:AML.M0006))
Declaration(NamedIndividual(hes:AML.M0007))
Declaration(NamedIndividual(hes:AML.M0008))
Declaration(NamedIndividual(hes:AML.M0009))
Declaration(NamedIndividual(hes:AML.M0010))
Declaration(NamedIndividual(hes:AML.M0011))
Declaration(NamedIndividual(hes:AML.M0012))
Declaration(NamedIndividual(hes:AML.M0013))
Declaration(NamedIndividual(hes:AML.M0014))
Declaration(NamedIndividual(hes:AML.M0015))
Declaration(NamedIndividual(hes:AML.M0016))
Declaration(NamedIndividual(hes:AML.M0017))
Declaration(NamedIndividual(hes:AML.M0018))
Declaration(NamedIndividual(hes:AML.M0019))
Declaration(NamedIndividual(hes:AML.M0020))
Declaration(NamedIndividual(hes:AML.M0021))
Declaration(NamedIndividual(hes:AML.M0022))
Declaration(NamedIndividual(hes:AML.M0023))
Declaration(NamedIndividual(hes:AML.M0024))
Declaration(NamedIndividual(hes:AML.M0025))
Declaration(NamedIndividual(hes:AML.M0026))
Declaration(NamedIndividual(hes:AML.M0027))
Declaration(NamedIndividual(hes:AML.M0028))
Declaration(NamedIndividual(hes:AML.M0029))
Declaration(NamedIndividual(hes:AML.M0030))
Declaration(NamedIndividual(hes:AML.M0031))
Declaration(NamedIndividual(hes:AML.M0032))
Declaration(NamedIndividual(hes:AML.M0033))
Declaration(NamedIndividual(hes:AML.M0034))
Declaration(NamedIndividual(hes:AML.M0035))
Declaration(NamedIndividual(hes:AML.M0036))
Declaration(NamedIndividual(hes:AML.M0037))
Declaration(NamedIndividual(hes:AML.M0038))
Declaration(NamedIndividual(hes:AML.M0039))
Declaration(NamedIndividual(hes:AML.M0040))
Declaration(NamedIndividual(hes:AML.T0000))
Declaration(NamedIndividual(hes:AML.T0000.000))
Declaration(NamedIndividual(hes:AML.T0000.001))
Declaration(NamedIndividual(hes:AML.T0000.002))
Declaration(NamedIndividual(hes:AML.T0001))
Declaration(NamedIndividual(hes:AML.T0002))
Declaration(NamedIndividual(hes:AML.T0002.000))
Declaration(NamedIndividual(hes:AML.T0002.001))
Declaration(NamedIndividual(hes:AML.T0003))
Declaration(NamedIndividual(hes:AML.T0004))
Declaration(NamedIndividual(hes:AML.T0005))
Declaration(NamedIndividual(hes:AML.T0005.000))
Declaration(NamedIndividual(hes:AML.T0005.001))
Declaration(NamedIndividual(hes:AML.T0005.002))
Declaration(NamedIndividual(hes:AML.T0006))
Declaration(NamedIndividual(hes:AML.T0007))
Declaration(NamedIndividual(hes:AML.T0008))
Declaration(NamedIndividual(hes:AML.T0008.000))
Declaration(NamedIndividual(hes:AML.T0008.001))
Declaration(NamedIndividual(hes:AML.T0010))
Declaration(NamedIndividual(hes:AML.T0010.000))
Declaration(NamedIndividual(hes:AML.T0010.001))
Declaration(NamedIndividual(hes:AML.T0010.002))
Declaration(NamedIndividual(hes:AML.T0010.003))
Declaration(NamedIndividual(hes:AML.T0011))
Declaration(NamedIndividual(hes:AML.T0011.000))
Declaration(NamedIndividual(hes:AML.T0012))
Declaration(NamedIndividual(hes:AML.T0013))
Declaration(NamedIndividual(hes:AML.T0014))
Declaration(NamedIndividual(hes:AML.T0015))
Declaration(NamedIndividual(hes:AML.T0016))
Declaration(NamedIndividual(hes:AML.T0016.000))
Declaration(NamedIndividual(hes:AML.T0016.001))
Declaration(NamedIndividual(hes:AML.T0017))
Declaration(NamedIndividual(hes:AML.T0017.000))
Declaration(NamedIndividual(hes:AML.T0018))
Declaration(NamedIndividual(hes:AML.T0018.000))
Declaration(NamedIndividual(hes:AML.T0018.001))
Declaration(NamedIndividual(hes:AML.T0019))
Declaration(NamedIndividual(hes:AML.T0020))
Declaration(NamedIndividual(hes:AML.T0021))
Declaration(NamedIndividual(hes:AML.T0024))
Declaration(NamedIndividual(hes:AML.T0024.000))
Declaration(NamedIndividual(hes:AML.T0024.001))
Declaration(NamedIndividual(hes:AML.T0024.002))
Declaration(NamedIndividual(hes:AML.T0025))
Declaration(NamedIndividual(hes:AML.T0029))
Declaration(NamedIndividual(hes:AML.T0031))
Declaration(NamedIndividual(hes:AML.T0034))
Declaration(NamedIndividual(hes:AML.T0035))
Declaration(NamedIndividual(hes:AML.T0036))
Declaration(NamedIndividual(hes:AML.T0037))
Declaration(NamedIndividual(hes:AML.T0040))
Declaration(NamedIndividual(hes:AML.T0041))
Declaration(NamedIndividual(hes:AML.T0042))
Declaration(NamedIndividual(hes:AML.T0043))
Declaration(NamedIndividual(hes:AML.T0043.000))
Declaration(NamedIndividual(hes:AML.T0043.001))
Declaration(NamedIndividual(hes:AML.T0043.002))
Declaration(NamedIndividual(hes:AML.T0043.003))
Declaration(NamedIndividual(hes:AML.T0043.004))
Declaration(NamedIndividual(hes:AML.T0044))
Declaration(NamedIndividual(hes:AML.T0046))
Declaration(NamedIndividual(hes:AML.T0047))
Declaration(NamedIndividual(hes:AML.T0048))
Declaration(NamedIndividual(hes:AML.T0048.000))
Declaration(NamedIndividual(hes:AML.T0048.001))
Declaration(NamedIndividual(hes:AML.T0048.002))
Declaration(NamedIndividual(hes:AML.T0048.003))
Declaration(NamedIndividual(hes:AML.T0048.004))
Declaration(NamedIndividual(hes:AML.T0049))
Declaration(NamedIndividual(hes:AML.T0050))
Declaration(NamedIndividual(hes:AML.T0051))
Declaration(NamedIndividual(hes:AML.T0051.000))
Declaration(NamedIndividual(hes:AML.T0051.001))
Declaration(NamedIndividual(hes:AML.T0052))
Declaration(NamedIndividual(hes:AML.T0052.000))
Declaration(NamedIndividual(hes:AML.T0053))
Declaration(NamedIndividual(hes:AML.T0054))
Declaration(NamedIndividual(hes:AML.T0055))
Declaration(NamedIndividual(hes:AML.T0056))
Declaration(NamedIndividual(hes:AML.T0057))
Declaration(NamedIndividual(hes:AccessControlTechniques))
Declaration(NamedIndividual(hes:AccessManagement))
Declaration(NamedIndividual(hes:AdditionalActivities))
Declaration(NamedIndividual(hes:AdversarialSamples))
Declaration(NamedIndividual(hes:Algorithm))
Declaration(NamedIndividual(hes:AnamalyDetector))
Declaration(NamedIndividual(hes:ApplicationProgrammingInterfaces))
Declaration(NamedIndividual(hes:ArbitraryCodeExecution))
Declaration(NamedIndividual(hes:AuthenticationEvasion))
Declaration(NamedIndividual(hes:Availability))
Declaration(NamedIndividual(hes:AzureService))
Declaration(NamedIndividual(hes:Backdoor))
Declaration(NamedIndividual(hes:BackdoorDetector))
Declaration(NamedIndividual(hes:BotDetector))
Declaration(NamedIndividual(hes:Bots))
Declaration(NamedIndividual(hes:Browser))
Declaration(NamedIndividual(hes:CAPEC.A0001))
Declaration(NamedIndividual(hes:CAPEC.A0002))
Declaration(NamedIndividual(hes:CAPEC.A0003))
Declaration(NamedIndividual(hes:CAPEC.A0004))
Declaration(NamedIndividual(hes:CAPEC.A0005))
Declaration(NamedIndividual(hes:CAPEC.A0006))
Declaration(NamedIndividual(hes:CAPEC.A0007))
Declaration(NamedIndividual(hes:CAPEC.A0008))
Declaration(NamedIndividual(hes:CAPEC.A0009))
Declaration(NamedIndividual(hes:CAPEC.A0010))
Declaration(NamedIndividual(hes:CAPEC.M0001))
Declaration(NamedIndividual(hes:CAPEC.M0002))
Declaration(NamedIndividual(hes:CAPEC.M0003))
Declaration(NamedIndividual(hes:CAPEC.M0004))
Declaration(NamedIndividual(hes:CAPEC.M0005))
Declaration(NamedIndividual(hes:CWE-1188))
Declaration(NamedIndividual(hes:CWE-1189))
Declaration(NamedIndividual(hes:CWE-1191))
Declaration(NamedIndividual(hes:CWE-1236))
Declaration(NamedIndividual(hes:CWE-1244))
Declaration(NamedIndividual(hes:CWE-125))
Declaration(NamedIndividual(hes:CWE-1256))
Declaration(NamedIndividual(hes:CWE-1260))
Declaration(NamedIndividual(hes:CWE-1272))
Declaration(NamedIndividual(hes:CWE-1274))
Declaration(NamedIndividual(hes:CWE-1300))
Declaration(NamedIndividual(hes:CWE-1327))
Declaration(NamedIndividual(hes:CWE-16))
Declaration(NamedIndividual(hes:CWE-190))
Declaration(NamedIndividual(hes:CWE-20))
Declaration(NamedIndividual(hes:CWE-200))
Declaration(NamedIndividual(hes:CWE-264))
Declaration(NamedIndividual(hes:CWE-269))
Declaration(NamedIndividual(hes:CWE-284))
Declaration(NamedIndividual(hes:CWE-285))
Declaration(NamedIndividual(hes:CWE-287))
Declaration(NamedIndividual(hes:CWE-306))
Declaration(NamedIndividual(hes:CWE-307))
Declaration(NamedIndividual(hes:CWE-322))
Declaration(NamedIndividual(hes:CWE-353))
Declaration(NamedIndividual(hes:CWE-362))
Declaration(NamedIndividual(hes:CWE-419))
Declaration(NamedIndividual(hes:CWE-420))
Declaration(NamedIndividual(hes:CWE-494))
Declaration(NamedIndividual(hes:CWE-502))
Declaration(NamedIndividual(hes:CWE-532))
Declaration(NamedIndividual(hes:CWE-611))
Declaration(NamedIndividual(hes:CWE-693))
Declaration(NamedIndividual(hes:CWE-778))
Declaration(NamedIndividual(hes:CWE-787))
Declaration(NamedIndividual(hes:CWE-798))
Declaration(NamedIndividual(hes:CWE-862))
Declaration(NamedIndividual(hes:CWE-863))
Declaration(NamedIndividual(hes:CWE-89))
Declaration(NamedIndividual(hes:CWE-916))
Declaration(NamedIndividual(hes:CWE-918))
Declaration(NamedIndividual(hes:CWE-924))
Declaration(NamedIndividual(hes:CWE-94))
Declaration(NamedIndividual(hes:CWE-940))
Declaration(NamedIndividual(hes:CWE-941))
Declaration(NamedIndividual(hes:ChatSystem))
Declaration(NamedIndividual(hes:Chatbot))
Declaration(NamedIndividual(hes:CloudComputingResources))
Declaration(NamedIndividual(hes:CloudPlatforms))
Declaration(NamedIndividual(hes:Colab))
Declaration(NamedIndividual(hes:CollaborativePlatforms))
Declaration(NamedIndividual(hes:Competitors))
Declaration(NamedIndividual(hes:Compromise))
Declaration(NamedIndividual(hes:Confidentiality))
Declaration(NamedIndividual(hes:CraftedPrompts))
Declaration(NamedIndividual(hes:CriticalSeverity))
Declaration(NamedIndividual(hes:CryptographicTechniques))
Declaration(NamedIndividual(hes:DataPreparation))
Declaration(NamedIndividual(hes:DataRepositories))
Declaration(NamedIndividual(hes:DeepLearningDetector))
Declaration(NamedIndividual(hes:DeepLearningModels))
Declaration(NamedIndividual(hes:Deployment))
Declaration(NamedIndividual(hes:DevelopmentPlatforms))
Declaration(NamedIndividual(hes:DigitalData))
Declaration(NamedIndividual(hes:DisgruntledEmployee))
Declaration(NamedIndividual(hes:EmailProtectionSystem))
Declaration(NamedIndividual(hes:EmbeddedSecurity))
Declaration(NamedIndividual(hes:FaceIdentificationSystem))
Declaration(NamedIndividual(hes:FacialRecognitionSystem))
Declaration(NamedIndividual(hes:FacialRecognitionSystems))
Declaration(NamedIndividual(hes:FaultyInputs))
Declaration(NamedIndividual(hes:Gateways))
Declaration(NamedIndividual(hes:GoogleColab))
Declaration(NamedIndividual(hes:GraphicalProcessingUnits))
Declaration(NamedIndividual(hes:Hacker))
Declaration(NamedIndividual(hes:HardwareInformation))
Declaration(NamedIndividual(hes:HardwareSecurity))
Declaration(NamedIndividual(hes:HardwareSecurityEngineering))
Declaration(NamedIndividual(hes:Has))
Declaration(NamedIndividual(hes:High-PerformanceCPUs))
Declaration(NamedIndividual(hes:High-SpeedSSDs))
Declaration(NamedIndividual(hes:High.Impact))
Declaration(NamedIndividual(hes:High.Likelihood))
Declaration(NamedIndividual(hes:HighRisk))
Declaration(NamedIndividual(hes:HighSeverity))
Declaration(NamedIndividual(hes:IdentityVerificationSystem))
Declaration(NamedIndividual(hes:IdentityVerificationSystem.))
Declaration(NamedIndividual(hes:Impact))
Declaration(NamedIndividual(hes:Integrity))
Declaration(NamedIndividual(hes:IntrusionDetectionSystem))
Declaration(NamedIndividual(hes:IntrusionPreventionSystem))
Declaration(NamedIndividual(hes:IoTPlatforms))
Declaration(NamedIndividual(hes:LLM01-PromptInjectionVulnerabiliy))
Declaration(NamedIndividual(hes:LLM02-InsecureOutputHandling))
Declaration(NamedIndividual(hes:LLM03-TrainingDataPoisoning))
Declaration(NamedIndividual(hes:LLM04-ModelDenialOfService))
Declaration(NamedIndividual(hes:LLM05-SupplyChainVulnerabilitieç))
Declaration(NamedIndividual(hes:LLM06-SensitiveInformationDisclosure))
Declaration(NamedIndividual(hes:LLM07-InsecurePluginDesign))
Declaration(NamedIndividual(hes:LLM08-ExcessiveAgency))
Declaration(NamedIndividual(hes:LLM09-Overreliance))
Declaration(NamedIndividual(hes:LLM10-ModelTheft))
Declaration(NamedIndividual(hes:Large-CapacityHDDs))
Declaration(NamedIndividual(hes:Library))
Declaration(NamedIndividual(hes:Likelihood))
Declaration(NamedIndividual(hes:LoadBalancers))
Declaration(NamedIndividual(hes:Low))
Declaration(NamedIndividual(hes:Low.Impact))
Declaration(NamedIndividual(hes:Low.Likelihood))
Declaration(NamedIndividual(hes:LowRisk))
Declaration(NamedIndividual(hes:MachineTranslationServices))
Declaration(NamedIndividual(hes:MalaciousLogic))
Declaration(NamedIndividual(hes:MaliciousData))
Declaration(NamedIndividual(hes:MalwareDetector))
Declaration(NamedIndividual(hes:Medium))
Declaration(NamedIndividual(hes:Medium.Impact))
Declaration(NamedIndividual(hes:Medium.Likelihood))
Declaration(NamedIndividual(hes:MediumRisk))
Declaration(NamedIndividual(hes:MediumSeverity))
Declaration(NamedIndividual(hes:MicrosoftService))
Declaration(NamedIndividual(hes:MobileApp))
Declaration(NamedIndividual(hes:MobileApplications))
Declaration(NamedIndividual(hes:Model))
Declaration(NamedIndividual(hes:ModelEngineering))
Declaration(NamedIndividual(hes:ModelEvaluation))
Declaration(NamedIndividual(hes:ModelSourcecode))
Declaration(NamedIndividual(hes:NetworkSecurity))
Declaration(NamedIndividual(hes:NeuralNetworks))
Declaration(NamedIndividual(hes:NeuromorphicChips))
Declaration(NamedIndividual(hes:OWASP.M0001))
Declaration(NamedIndividual(hes:OWASP.M0002))
Declaration(NamedIndividual(hes:OWASP.M0003))
Declaration(NamedIndividual(hes:OWASP.M0004))
Declaration(NamedIndividual(hes:OWASP.M0005))
Declaration(NamedIndividual(hes:OWASP.M0006))
Declaration(NamedIndividual(hes:OWASP.M0007))
Declaration(NamedIndividual(hes:OWASP.M0008))
Declaration(NamedIndividual(hes:OWASP.M0009))
Declaration(NamedIndividual(hes:OWASP.M0010))
Declaration(NamedIndividual(hes:OWASP.M0011))
Declaration(NamedIndividual(hes:OWASP.M0012))
Declaration(NamedIndividual(hes:OWASP.M0013))
Declaration(NamedIndividual(hes:OWASP.M0014))
Declaration(NamedIndividual(hes:OWASP.M0015))
Declaration(NamedIndividual(hes:OWASP.M0016))
Declaration(NamedIndividual(hes:OWASP.M0017))
Declaration(NamedIndividual(hes:OWASP.M0018))
Declaration(NamedIndividual(hes:OWASP.M0019))
Declaration(NamedIndividual(hes:OWASP.M0020))
Declaration(NamedIndividual(hes:OWASP.M0021))
Declaration(NamedIndividual(hes:OWASP.M0022))
Declaration(NamedIndividual(hes:OWASP.M0023))
Declaration(NamedIndividual(hes:OWASP.M0024))
Declaration(NamedIndividual(hes:OWASP.M0025))
Declaration(NamedIndividual(hes:OWASP.M0026))
Declaration(NamedIndividual(hes:OWASP.M0027))
Declaration(NamedIndividual(hes:OWASP.M0028))
Declaration(NamedIndividual(hes:OWASP.M0029))
Declaration(NamedIndividual(hes:OWASP.M0030))
Declaration(NamedIndividual(hes:OWASP.M0031))
Declaration(NamedIndividual(hes:OWASP.M0032))
Declaration(NamedIndividual(hes:OWASP.M0033))
Declaration(NamedIndividual(hes:OWASP.M0034))
Declaration(NamedIndividual(hes:OWASP.M0035))
Declaration(NamedIndividual(hes:Packages))
Declaration(NamedIndividual(hes:PhishingAgents))
Declaration(NamedIndividual(hes:Plugin))
Declaration(NamedIndividual(hes:Plugins))
Declaration(NamedIndividual(hes:ProcessingPlatforms))
Declaration(NamedIndividual(hes:ProcessingTools))
Declaration(NamedIndividual(hes:Protocol))
Declaration(NamedIndividual(hes:Pytorch))
Declaration(NamedIndividual(hes:QuantumProcessors))
Declaration(NamedIndividual(hes:Repository))
Declaration(NamedIndividual(hes:ResearchPlatforms))
Declaration(NamedIndividual(hes:RootkitHunter))
Declaration(NamedIndividual(hes:Routers))
Declaration(NamedIndividual(hes:SensitiveInformation))
Declaration(NamedIndividual(hes:SharedResource))
Declaration(NamedIndividual(hes:SoftwareFile))
Declaration(NamedIndividual(hes:SoftwareSecurityEngineering))
Declaration(NamedIndividual(hes:SpywarScanner))
Declaration(NamedIndividual(hes:SystemCode))
Declaration(NamedIndividual(hes:SystemFile))
Declaration(NamedIndividual(hes:TensorProcessingUnits))
Declaration(NamedIndividual(hes:Third-PartyVendors))
Declaration(NamedIndividual(hes:TrainingData))
Declaration(NamedIndividual(hes:Translator))
Declaration(NamedIndividual(hes:UserInterfaces))
Declaration(NamedIndividual(hes:ValidationData))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#5GNetworks>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AdjacentNetwork(A)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Business&DataUnderstanding>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Changed(C)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#ContentDeliveryNetworks(CDNs)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#FieldProgrammableGateArrays(FPGAs)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#GenerativePre-trainedTransformer(GPT)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High(H)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High.Impact*High.Likelihood=High.Risk>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High.Impact*Medium.Likelihood=High.Risk>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#LargeLanguageModels(LLM)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Local(L)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low(L)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low.Impact*High.Likelihood=Medium.Risk>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low.Impact*Low.Likelihood=Low.Risk>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low.Impact*Medium.Likelihood=Medium.Risk>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Medium.Impact*High.Likelihood=High.Risk>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Medium.Impact*Low.Likelihood=Medium.Risk>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Medium.Impact*Medium.Likelihood=Medium.Risk>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Network(N)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#None(N)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Physical(P)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Required(R)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Software-DefinedNetworking(SDN)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Storage&ManagementPlatforms>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#SystemOnAChip(SoC)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Unchanged(U)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#VirtualPrivateNetworks(VPNs)>))
Declaration(NamedIndividual(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#high.Impact*Low.Likelihood=Medium.Risk>))
Declaration(AnnotationProperty(swrla:isRuleEnabled))
Declaration(AnnotationProperty(hes:AttackScenarios))
Declaration(AnnotationProperty(hes:Description))
Declaration(AnnotationProperty(hes:Examples))
Declaration(AnnotationProperty(hes:Mitigations))
Declaration(AnnotationProperty(hes:Name))
Declaration(AnnotationProperty(hes:ProposedMitigation))
Declaration(AnnotationProperty(hes:Reasons))
Declaration(AnnotationProperty(hes:Threat))
Declaration(AnnotationProperty(hes:ThreatName))
Declaration(AnnotationProperty(hes:Values))
Declaration(AnnotationProperty(:DASE_RULE))
############################
#   Annotation Properties
############################

# Annotation Property: hes:AttackScenarios (hes:AttackScenarios)

SubAnnotationPropertyOf(hes:AttackScenarios rdfs:comment)

# Annotation Property: hes:Mitigations (hes:Mitigations)

AnnotationAssertion(hes:Description hes:Mitigations "Mitigation refers to a suggested solution or strategy designed to reduce, minimize, or eliminate a specific  vulnerability, or threat. In the context of threat modeling, mitigations are actions, controls, or safeguards that address identified issues to improve system resilience, protect assets, and prevent potential attacks or failures.")

# Annotation Property: hes:Name (hes:Name)

SubAnnotationPropertyOf(hes:Name rdfs:comment)

# Annotation Property: hes:Values (hes:Values)

SubAnnotationPropertyOf(hes:Values rdfs:comment)


############################
#   Object Properties
############################

# Object Property: hes:Allocates (hes:Allocates)

SubObjectPropertyOf(hes:Allocates hes:SWRLRuleProperties)

# Object Property: hes:AlterCommunicationOf (hes:AlterCommunicationOf)

SubObjectPropertyOf(hes:AlterCommunicationOf hes:SWRLRuleProperties)

# Object Property: hes:AreIncludedIn (hes:AreIncludedIn)

SubObjectPropertyOf(hes:AreIncludedIn hes:IsIncludedIn)

# Object Property: hes:Cause (hes:Cause)

ObjectPropertyDomain(hes:Cause hes:SecurityThreats)
ObjectPropertyDomain(hes:Cause hes:Weaknesses)
ObjectPropertyRange(hes:Cause hes:Risk)

# Object Property: hes:ComposedOf (hes:ComposedOf)

ObjectPropertyDomain(hes:ComposedOf hes:AIEnabledSystems)
ObjectPropertyRange(hes:ComposedOf hes:Assets)

# Object Property: hes:ConsistOf (hes:ConsistOf)

ObjectPropertyDomain(hes:ConsistOf hes:Tactics)
ObjectPropertyRange(hes:ConsistOf hes:Techniques)

# Object Property: hes:Contains (hes:Contains)

SubObjectPropertyOf(hes:Contains hes:SWRLRuleProperties)

# Object Property: hes:Exploits (hes:Exploits)

ObjectPropertyDomain(hes:Exploits hes:SecurityThreats)
ObjectPropertyRange(hes:Exploits hes:Weaknesses)

# Object Property: hes:ExposedBy (hes:ExposedBy)

SubObjectPropertyOf(hes:ExposedBy hes:SWRLRuleProperties)

# Object Property: hes:Has (hes:Has)

ObjectPropertyDomain(hes:Has hes:SecurityThreats)
ObjectPropertyRange(hes:Has hes:Risk)

# Object Property: hes:HasWeaknesses (hes:HasWeaknesses)

SubObjectPropertyOf(hes:HasWeaknesses owl:topObjectProperty)
ObjectPropertyDomain(hes:HasWeaknesses hes:Assets)
ObjectPropertyRange(hes:HasWeaknesses hes:Weaknesses)

# Object Property: hes:Injects (hes:Injects)

SubObjectPropertyOf(hes:Injects hes:SWRLRuleProperties)

# Object Property: hes:IsAccessedBy (hes:IsAccessedBy)

SubObjectPropertyOf(hes:IsAccessedBy hes:SWRLRuleProperties)

# Object Property: hes:IsBypassedBy (hes:IsBypassedBy)

SubObjectPropertyOf(hes:IsBypassedBy hes:SWRLRuleProperties)

# Object Property: hes:IsDeducedBy (hes:IsDeducedBy)

SubObjectPropertyOf(hes:IsDeducedBy hes:SWRLRuleProperties)

# Object Property: hes:IsDetectedBy (hes:IsDetectedBy)

SubObjectPropertyOf(hes:IsDetectedBy hes:SWRLRuleProperties)

# Object Property: hes:IsDisruptedBy (hes:IsDisruptedBy)

SubObjectPropertyOf(hes:IsDisruptedBy hes:SWRLRuleProperties)

# Object Property: hes:IsEvadedBy (hes:IsEvadedBy)

SubObjectPropertyOf(hes:IsEvadedBy hes:SWRLRuleProperties)

# Object Property: hes:IsExploitedBy (hes:IsExploitedBy)

SubObjectPropertyOf(hes:IsExploitedBy hes:SWRLRuleProperties)
ObjectPropertyDomain(hes:IsExploitedBy hes:Weaknesses)
ObjectPropertyRange(hes:IsExploitedBy hes:SecurityThreats)

# Object Property: hes:IsHijakedBy (hes:IsHijakedBy)

SubObjectPropertyOf(hes:IsHijakedBy hes:SWRLRuleProperties)

# Object Property: hes:IsImpactedBy (hes:IsImpactedBy)

SubObjectPropertyOf(hes:IsImpactedBy owl:topObjectProperty)
ObjectPropertyDomain(hes:IsImpactedBy hes:Assets)
ObjectPropertyRange(hes:IsImpactedBy hes:Risk)

# Object Property: hes:IsIncludedIn (hes:IsIncludedIn)

ObjectPropertyDomain(hes:IsIncludedIn hes:Mitigations)
ObjectPropertyRange(hes:IsIncludedIn hes:AILifeCycle)

# Object Property: hes:IsInitiatedBy (hes:IsInitiatedBy)

ObjectPropertyDomain(hes:IsInitiatedBy hes:SecurityThreats)
ObjectPropertyRange(hes:IsInitiatedBy hes:Adversary)

# Object Property: hes:IsInjectedInto (hes:IsInjectedInto)

SubObjectPropertyOf(hes:IsInjectedInto hes:SWRLRuleProperties)

# Object Property: hes:IsInjectedWith (hes:IsInjectedWith)

SubObjectPropertyOf(hes:IsInjectedWith hes:SWRLRuleProperties)

# Object Property: hes:IsInsertedInto (hes:IsInsertedInto)

SubObjectPropertyOf(hes:IsInsertedInto hes:SWRLRuleProperties)

# Object Property: hes:IsManipulatedBy (hes:IsManipulatedBy)

SubObjectPropertyOf(hes:IsManipulatedBy hes:SWRLRuleProperties)

# Object Property: hes:IsMisconfigured (hes:IsMisconfigured)

SubObjectPropertyOf(hes:IsMisconfigured hes:SWRLRuleProperties)

# Object Property: hes:IsMitigatedBy (hes:IsMitigatedBy)

ObjectPropertyDomain(hes:IsMitigatedBy hes:SecurityThreats)
ObjectPropertyRange(hes:IsMitigatedBy hes:Mitigations)

# Object Property: hes:IsPoisionedBy (hes:IsPoisionedBy)

SubObjectPropertyOf(hes:IsPoisionedBy hes:SWRLRuleProperties)

# Object Property: hes:IsPreventedBy (hes:IsPreventedBy)

ObjectPropertyDomain(hes:IsPreventedBy hes:Techniques)
ObjectPropertyRange(hes:IsPreventedBy hes:Mitigations)

# Object Property: hes:IsProtectedBy (hes:IsProtectedBy)

ObjectPropertyDomain(hes:IsProtectedBy hes:Assets)
ObjectPropertyRange(hes:IsProtectedBy hes:SecurityMechanism)

# Object Property: hes:IsReplicatedBy (hes:IsReplicatedBy)

SubObjectPropertyOf(hes:IsReplicatedBy hes:SWRLRuleProperties)

# Object Property: hes:IsReverseEngineersBy (hes:IsReverseEngineersBy)

SubObjectPropertyOf(hes:IsReverseEngineersBy hes:SWRLRuleProperties)

# Object Property: hes:IsThreatenedBy (hes:IsThreatenedBy)

ObjectPropertyDomain(hes:IsThreatenedBy hes:Assets)
ObjectPropertyRange(hes:IsThreatenedBy hes:SecurityThreats)

# Object Property: hes:IsUnder (hes:IsUnder)

SubObjectPropertyOf(hes:IsUnder hes:SWRLRuleProperties)

# Object Property: hes:IsUnderDoSAttack (hes:IsUnderDoSAttack)

SubObjectPropertyOf(hes:IsUnderDoSAttack hes:SWRLRuleProperties)

# Object Property: hes:IsVulnerableTo (hes:IsVulnerableTo)

SubObjectPropertyOf(hes:IsVulnerableTo hes:SWRLRuleProperties)

# Object Property: hes:LeadsTo (hes:LeadsTo)

ObjectPropertyDomain(hes:LeadsTo hes:Techniques)
ObjectPropertyRange(hes:LeadsTo hes:SecurityThreats)

# Object Property: hes:Performs (hes:Performs)

ObjectPropertyDomain(hes:Performs hes:Adversary)
ObjectPropertyRange(hes:Performs hes:MalaciousActivity)

# Object Property: hes:ProtectsFrom (hes:ProtectsFrom)

SubObjectPropertyOf(hes:ProtectsFrom owl:topObjectProperty)
ObjectPropertyDomain(hes:ProtectsFrom hes:SecurityMechanism)
ObjectPropertyRange(hes:ProtectsFrom hes:SecurityThreats)

# Object Property: hes:SWRLRuleProperties (hes:SWRLRuleProperties)

AnnotationAssertion(hes:Description hes:SWRLRuleProperties "These properties define different relations in rules.")

# Object Property: hes:Uses (hes:Uses)

ObjectPropertyDomain(hes:Uses hes:Adversary)
ObjectPropertyRange(hes:Uses hes:Tactics)
ObjectPropertyRange(hes:Uses hes:Techniques)

# Object Property: hes:UsesTactic (hes:UsesTactic)

ObjectPropertyDomain(hes:UsesTactic hes:Adversary)
ObjectPropertyRange(hes:UsesTactic hes:Tactics)

# Object Property: hes:Violates (hes:Violates)

ObjectPropertyDomain(hes:Violates hes:SecurityThreats)
ObjectPropertyRange(hes:Violates hes:CIA)



############################
#   Classes
############################

# Class: hes:AIEnabledSystems (hes:AIEnabledSystems)

AnnotationAssertion(hes:Description hes:AIEnabledSystems "AI-enabled systems are systems or applications that leverage artificial intelligence (AI) technologies to perform tasks such as data analysis, decision-making, pattern recognition, and automation. These systems utilize AI models, including machine learning algorithms, neural networks, and natural language processing, to enhance capabilities across various domains, including cybersecurity, healthcare, finance, and autonomous systems.")

# Class: hes:AILifeCycle (hes:AILifeCycle)

AnnotationAssertion(hes:Description hes:AILifeCycle "The AI Lifecycle refers to the various stages involved in developing, deploying, and maintaining an AI system.")

# Class: hes:AMLA0001 (hes:AMLA0001)

SubClassOf(hes:AMLA0001 hes:SecurityThreats)

# Class: hes:AMLA0002 (hes:AMLA0002)

SubClassOf(hes:AMLA0002 hes:SecurityThreats)

# Class: hes:AMLA0003 (hes:AMLA0003)

SubClassOf(hes:AMLA0003 hes:SecurityThreats)

# Class: hes:AMLA0004 (hes:AMLA0004)

SubClassOf(hes:AMLA0004 hes:SecurityThreats)

# Class: hes:AMLA0005 (hes:AMLA0005)

SubClassOf(hes:AMLA0005 hes:SecurityThreats)

# Class: hes:AMLA0006 (hes:AMLA0006)

SubClassOf(hes:AMLA0006 hes:SecurityThreats)

# Class: hes:AMLA0007 (hes:AMLA0007)

SubClassOf(hes:AMLA0007 hes:SecurityThreats)

# Class: hes:AMLA0008 (hes:AMLA0008)

SubClassOf(hes:AMLA0008 hes:SecurityThreats)

# Class: hes:AMLA0009 (hes:AMLA0009)

SubClassOf(hes:AMLA0009 hes:SecurityThreats)

# Class: hes:AMLA0010 (hes:AMLA0010)

SubClassOf(hes:AMLA0010 hes:SecurityThreats)

# Class: hes:AMLA0011 (hes:AMLA0011)

SubClassOf(hes:AMLA0011 hes:SecurityThreats)

# Class: hes:AMLA0012 (hes:AMLA0012)

SubClassOf(hes:AMLA0012 hes:SecurityThreats)

# Class: hes:AMLA0013 (hes:AMLA0013)

SubClassOf(hes:AMLA0013 hes:SecurityThreats)

# Class: hes:AMLA0014 (hes:AMLA0014)

SubClassOf(hes:AMLA0014 hes:SecurityThreats)

# Class: hes:AMLA0015 (hes:AMLA0015)

SubClassOf(hes:AMLA0015 hes:SecurityThreats)

# Class: hes:AMLA0016 (hes:AMLA0016)

SubClassOf(hes:AMLA0016 hes:SecurityThreats)

# Class: hes:AMLA0017 (hes:AMLA0017)

SubClassOf(hes:AMLA0017 hes:SecurityThreats)

# Class: hes:AMLA0018 (hes:AMLA0018)

SubClassOf(hes:AMLA0018 hes:SecurityThreats)

# Class: hes:AMLA0019 (hes:AMLA0019)

SubClassOf(hes:AMLA0019 hes:SecurityThreats)

# Class: hes:AMLA0020 (hes:AMLA0020)

SubClassOf(hes:AMLA0020 hes:SecurityThreats)

# Class: hes:AMLA0021 (hes:AMLA0021)

SubClassOf(hes:AMLA0021 hes:SecurityThreats)

# Class: hes:AMLA0022 (hes:AMLA0022)

SubClassOf(hes:AMLA0022 hes:SecurityThreats)

# Class: hes:AMLA0023 (hes:AMLA0023)

SubClassOf(hes:AMLA0023 hes:SecurityThreats)

# Class: hes:AccessControlTechniques (hes:AccessControlTechniques)

SubClassOf(hes:AccessControlTechniques hes:SecurityMechanism)

# Class: hes:AdversarialSamples (hes:AdversarialSamples)

SubClassOf(hes:AdversarialSamples hes:MalaciousActivity)

# Class: hes:Adversary (hes:Adversary)

AnnotationAssertion(hes:Description hes:Adversary "An adversary refers to an individual, group, or entity that attempts to exploit vulnerabilities in a system, typically with malicious intent. In the context of AI, adversaries may aim to compromise machine learning models, manipulate data, or disrupt AI-driven processes by executing attacks such as adversarial machine learning, data poisoning, or model inversion.")

# Class: hes:Algorithm (hes:Algorithm)

SubClassOf(hes:Algorithm hes:Assets)

# Class: hes:AlgorithmWeaknesses (hes:AlgorithmWeaknesses)

SubClassOf(hes:AlgorithmWeaknesses hes:Weaknesses)

# Class: hes:AnomalyDetector (hes:AnomalyDetector)

SubClassOf(hes:AnomalyDetector hes:SecurityMechanism)

# Class: hes:ArbitraryCodeExecution (hes:ArbitraryCodeExecution)

SubClassOf(hes:ArbitraryCodeExecution hes:MalaciousActivity)

# Class: hes:Assessment (hes:Assessment)

AnnotationAssertion(hes:Description hes:Assessment "Risk and Vulnerability Assessment is a systematic process used to identify, evaluate, and prioritize potential threats, and weaknesses, of a systems, assets, or processes, particularly in the context of security.")

# Class: hes:Assets (hes:Assets)

AnnotationAssertion(hes:Description hes:Assets "Assets in the context of AI and security refer to the valuable components or resources within an organization or system that require protection.")
SubClassOf(hes:Assets hes:AIEnabledSystems)

# Class: hes:AttackStaging (hes:AttackStaging)

SubClassOf(hes:AttackStaging hes:Tactics)

# Class: hes:AuthenticationEvasion (hes:AuthenticationEvasion)

SubClassOf(hes:AuthenticationEvasion hes:MalaciousActivity)

# Class: hes:Backdoor (hes:Backdoor)

SubClassOf(hes:Backdoor hes:MalaciousActivity)

# Class: hes:BackdoorDetector (hes:BackdoorDetector)

SubClassOf(hes:BackdoorDetector hes:SecurityMechanism)

# Class: hes:BotDetector (hes:BotDetector)

SubClassOf(hes:BotDetector hes:SecurityMechanism)

# Class: hes:CAPECA0001 (hes:CAPECA0001)

AnnotationAssertion(hes:Name hes:CAPECA0001 "Abusing authentication mechanisms can provide unauthorized access or control over AI hardware systems.")
AnnotationAssertion(hes:Name hes:CAPECA0001 "Authentication Abuse")
SubClassOf(hes:CAPECA0001 hes:SecurityThreats)

# Class: hes:CAPECA0002 (hes:CAPECA0002)

AnnotationAssertion(hes:Description hes:CAPECA0002 "AI systems often share resources such as GPUs and TPUs; manipulating these can lead to denial of service or data leakage.")
AnnotationAssertion(hes:Name hes:CAPECA0002 "Shared Resource Manipulation")
SubClassOf(hes:CAPECA0002 hes:SecurityThreats)

# Class: hes:CAPECA0003 (hes:CAPECA0003)

AnnotationAssertion(hes:Description hes:CAPECA0003 "Misrepresenting content or identities can mislead AI systems into making incorrect decisions based on false data.")
AnnotationAssertion(hes:Name hes:CAPECA0003 "Identity Spoofing")
SubClassOf(hes:CAPECA0003 hes:SecurityThreats)

# Class: hes:CAPECA0004 (hes:CAPECA0004)

AnnotationAssertion(hes:Description hes:CAPECA0004 "The insertion of malicious logic into hardware (e.g., firmware, chips) can create persistent threats that are difficult to detect and mitigate, especially in complex AI systems.")
AnnotationAssertion(hes:Name hes:CAPECA0004 "Malicious Logic Insertion")
SubClassOf(hes:CAPECA0004 hes:SecurityThreats)

# Class: hes:CAPECA0005 (hes:CAPECA0005)

AnnotationAssertion(hes:Description hes:CAPECA0005 "Introducing faults into hardware components to cause errors or malfunctions can be particularly devastating in systems that rely on precise calculations and data integrity, such as AI.")
AnnotationAssertion(hes:Name hes:CAPECA0005 "Hardware Fault Injection")
SubClassOf(hes:CAPECA0005 hes:SecurityThreats)

# Class: hes:CAPECA0006 (hes:CAPECA0006)

AnnotationAssertion(hes:Description hes:CAPECA0006 "This involves an attacker intercepting or altering communications between two parties without their knowledge. This can compromise the integrity and confidentiality of data exchanges in AI systems, such as model updates or data feeds.")
AnnotationAssertion(hes:Name hes:CAPECA0006 "Adversary in the Middle (AiTM)”")
SubClassOf(hes:CAPECA0006 hes:SecurityThreats)

# Class: hes:CAPECA0007 (hes:CAPECA0007)

AnnotationAssertion(hes:Description hes:CAPECA0007 "By manipulating the protocols used for network communication, attackers can disrupt, reroute, or degrade the communications essential for AI operations, potentially leading to faulty outputs or system failures.")
AnnotationAssertion(hes:Name hes:CAPECA0007 "Protocol Manipulation")
SubClassOf(hes:CAPECA0007 hes:SecurityThreats)

# Class: hes:CAPECA0008 (hes:CAPECA0008)

AnnotationAssertion(hes:Description hes:CAPECA0008 "Injecting malicious or misleading data into network traffic can alter AI behaviors or decisions, especially in systems that rely on real-time data feeds.")
AnnotationAssertion(hes:Description hes:CAPECA0008 "Traffic Injection")
SubClassOf(hes:CAPECA0008 hes:SecurityThreats)

# Class: hes:CAPECA0009 (hes:CAPECA0009)

AnnotationAssertion(hes:Description hes:CAPECA0009 "Allocating excessive resources can lead to performance degradation or denial of service, affecting AI system availability.")
AnnotationAssertion(hes:Name hes:CAPECA0009 "Excessive Allocation")
SubClassOf(hes:CAPECA0009 hes:SecurityThreats)

# Class: hes:CAPECA0010 (hes:CAPECA0010)

AnnotationAssertion(hes:Description hes:CAPECA0010 "Injecting unauthorized resources into a network can alter the behavior of AI systems or lead to execution of malicious code.")
AnnotationAssertion(hes:Name hes:CAPECA0010 "Resource Injection")
SubClassOf(hes:CAPECA0010 hes:SecurityThreats)

# Class: hes:CIA (hes:CIA)

AnnotationAssertion(hes:Description hes:CIA "CIA in the context of cybersecurity refers to the Confidentiality, Integrity, and Availability triad, which represents the three core principles of information security:
1. Confidentiality: Ensures that sensitive information is only accessible to authorized users and systems. It involves protecting data from unauthorized access or disclosure.
2. Integrity: Guarantees that data is accurate, complete, and unaltered. It ensures that information remains reliable and unmodified, whether in storage or transit, unless authorized changes are made.
3. Availability: Ensures that authorized users can reliably access the information and systems when needed, without disruption or delay.")

# Class: hes:ChatBot (hes:ChatBot)

SubClassOf(hes:ChatBot hes:Tool)

# Class: hes:ChatSystem (hes:ChatSystem)

SubClassOf(hes:ChatSystem hes:Tool)

# Class: hes:Collection (hes:Collection)

SubClassOf(hes:Collection hes:Tactics)

# Class: hes:CompromisedDuringDevelopment (hes:CompromisedDuringDevelopment)

SubClassOf(hes:CompromisedDuringDevelopment hes:MalaciousActivity)

# Class: hes:ConcurrentExecution (hes:ConcurrentExecution)

SubClassOf(hes:ConcurrentExecution hes:SoftwareWeaknesses)

# Class: hes:CraftedPrompts (hes:CraftedPrompts)

SubClassOf(hes:CraftedPrompts hes:MalaciousActivity)

# Class: hes:CredentialAccess (hes:CredentialAccess)

SubClassOf(hes:CredentialAccess hes:Tactics)

# Class: hes:CryptographicTechniques (hes:CryptographicTechniques)

SubClassOf(hes:CryptographicTechniques hes:SecurityMechanism)

# Class: hes:Data (hes:Data)

SubClassOf(hes:Data hes:Assets)

# Class: hes:DataWeaknesses (hes:DataWeaknesses)

SubClassOf(hes:DataWeaknesses hes:Weaknesses)

# Class: hes:DeepLearningDetector (hes:DeepLearningDetector)

SubClassOf(hes:DeepLearningDetector hes:SecurityMechanism)

# Class: hes:DefenseEvasion (hes:DefenseEvasion)

SubClassOf(hes:DefenseEvasion hes:Tactics)

# Class: hes:DeserializationOfUntrustedData (hes:DeserializationOfUntrustedData)

SubClassOf(hes:DeserializationOfUntrustedData hes:SoftwareWeaknesses)

# Class: hes:Discovery (hes:Discovery)

SubClassOf(hes:Discovery hes:Tactics)

# Class: hes:EmbeddedSecurity (hes:EmbeddedSecurity)

SubClassOf(hes:EmbeddedSecurity hes:SecurityMechanism)

# Class: hes:ExcessiveAgency (hes:ExcessiveAgency)

SubClassOf(hes:ExcessiveAgency hes:SoftwareWeaknesses)

# Class: hes:Execution (hes:Execution)

SubClassOf(hes:Execution hes:Tactics)

# Class: hes:Exfiltration (hes:Exfiltration)

SubClassOf(hes:Exfiltration hes:Tactics)

# Class: hes:FaultyInputs (hes:FaultyInputs)

SubClassOf(hes:FaultyInputs hes:MalaciousActivity)

# Class: hes:Hardware (hes:Hardware)

SubClassOf(hes:Hardware hes:Assets)

# Class: hes:HardwareSecurity (hes:HardwareSecurity)

SubClassOf(hes:HardwareSecurity hes:SecurityMechanism)

# Class: hes:HardwareWeaknesses (hes:HardwareWeaknesses)

SubClassOf(hes:HardwareWeaknesses hes:Weaknesses)

# Class: hes:Human-centricFactors (hes:Human-centricFactors)

SubClassOf(hes:Human-centricFactors hes:OrganizationalWeaknesses)

# Class: hes:Impact (hes:Impact)

SubClassOf(hes:Impact hes:Risk)

# Class: hes:Impacts (hes:Impacts)

SubClassOf(hes:Impacts hes:Tactics)

# Class: hes:ImproperAuthentication (hes:ImproperAuthentication)

SubClassOf(hes:ImproperAuthentication hes:SoftwareWeaknesses)

# Class: hes:ImproperControl (hes:ImproperControl)

SubClassOf(hes:ImproperControl hes:SoftwareWeaknesses)

# Class: hes:ImproperInputValidation (hes:ImproperInputValidation)

SubClassOf(hes:ImproperInputValidation hes:SoftwareWeaknesses)

# Class: hes:ImproperNeutralization (hes:ImproperNeutralization)

SubClassOf(hes:ImproperNeutralization hes:SoftwareWeaknesses)

# Class: hes:ImproperPrivilegeManagement (hes:ImproperPrivilegeManagement)

SubClassOf(hes:ImproperPrivilegeManagement hes:SoftwareWeaknesses)

# Class: hes:IncorrectAuthorization (hes:IncorrectAuthorization)

SubClassOf(hes:IncorrectAuthorization hes:SoftwareWeaknesses)

# Class: hes:Information (hes:Information)

SubClassOf(hes:Information hes:Data)

# Class: hes:InfrastructureWeaknesses (hes:InfrastructureWeaknesses)

SubClassOf(hes:InfrastructureWeaknesses hes:Weaknesses)

# Class: hes:InitialAccess (hes:InitialAccess)

SubClassOf(hes:InitialAccess hes:Tactics)

# Class: hes:InsecureObjectHandling (hes:InsecureObjectHandling)

SubClassOf(hes:InsecureObjectHandling hes:SoftwareWeaknesses)

# Class: hes:InsecurePluginDesign (hes:InsecurePluginDesign)

SubClassOf(hes:InsecurePluginDesign hes:SoftwareWeaknesses)

# Class: hes:IntegerOverflow (hes:IntegerOverflow)

SubClassOf(hes:IntegerOverflow hes:SoftwareWeaknesses)

# Class: hes:IntrusionDetector (hes:IntrusionDetector)

SubClassOf(hes:IntrusionDetector hes:SecurityMechanism)

# Class: hes:IntrusionPrevention (hes:IntrusionPrevention)

SubClassOf(hes:IntrusionPrevention hes:SecurityMechanism)

# Class: hes:LLM (hes:LLM)

SubClassOf(hes:LLM hes:Model)

# Class: hes:Library (hes:Library)

SubClassOf(hes:Library hes:Assets)

# Class: hes:Likelihood (hes:Likelihood)

SubClassOf(hes:Likelihood hes:Risk)

# Class: hes:MalaciousActivity (hes:MalaciousActivity)

AnnotationAssertion(hes:Description hes:MalaciousActivity "Malicious activity refers to any action or behavior carried out with harmful intent, typically aimed at causing damage, disrupting systems, stealing sensitive information, or gaining unauthorized access to networks or resources. Moreover, malicious activities can include hacking, phishing, data breaches, injecting malware, or manipulating AI models through adversarial attacks to exploit system vulnerabilities for malicious purposes.")

# Class: hes:MaliciousData (hes:MaliciousData)

SubClassOf(hes:MaliciousData hes:MalaciousActivity)

# Class: hes:MaliciousLogic (hes:MaliciousLogic)

SubClassOf(hes:MaliciousLogic hes:MalaciousActivity)

# Class: hes:MalwareDetector (hes:MalwareDetector)

SubClassOf(hes:MalwareDetector hes:SecurityMechanism)

# Class: hes:Misconfiguation (hes:Misconfiguation)

SubClassOf(hes:Misconfiguation hes:MalaciousActivity)

# Class: hes:MissingAuthentication (hes:MissingAuthentication)

SubClassOf(hes:MissingAuthentication hes:SoftwareWeaknesses)

# Class: hes:MissingAuthorization (hes:MissingAuthorization)

SubClassOf(hes:MissingAuthorization hes:SoftwareWeaknesses)

# Class: hes:Model (hes:Model)

SubClassOf(hes:Model hes:Assets)

# Class: hes:ModelAccess (hes:ModelAccess)

SubClassOf(hes:ModelAccess hes:Tactics)

# Class: hes:ModelDenialOfService (hes:ModelDenialOfService)

SubClassOf(hes:ModelDenialOfService hes:SoftwareWeaknesses)

# Class: hes:ModelTheft (hes:ModelTheft)

SubClassOf(hes:ModelTheft hes:SoftwareWeaknesses)

# Class: hes:ModelWeaknesses (hes:ModelWeaknesses)

SubClassOf(hes:ModelWeaknesses hes:Weaknesses)

# Class: hes:Network (hes:Network)

SubClassOf(hes:Network hes:Assets)

# Class: hes:NetworkSecurity (hes:NetworkSecurity)

SubClassOf(hes:NetworkSecurity hes:SecurityMechanism)

# Class: hes:NetworkWeaknesses (hes:NetworkWeaknesses)

SubClassOf(hes:NetworkWeaknesses hes:Weaknesses)

# Class: hes:OrganizationalWeaknesses (hes:OrganizationalWeaknesses)

SubClassOf(hes:OrganizationalWeaknesses hes:Weaknesses)

# Class: hes:Out-of-boundsRead (hes:Out-of-boundsRead)

SubClassOf(hes:Out-of-boundsRead hes:SoftwareWeaknesses)

# Class: hes:Out-of-boundsWrite (hes:Out-of-boundsWrite)

SubClassOf(hes:Out-of-boundsWrite hes:SoftwareWeaknesses)

# Class: hes:Overreliance (hes:Overreliance)

SubClassOf(hes:Overreliance hes:SoftwareWeaknesses)

# Class: hes:Persistence (hes:Persistence)

SubClassOf(hes:Persistence hes:Tactics)

# Class: hes:Platform (hes:Platform)

SubClassOf(hes:Platform hes:Assets)

# Class: hes:Plugins (hes:Plugins)

SubClassOf(hes:Plugins hes:Assets)

# Class: hes:PrivilegeEscalation (hes:PrivilegeEscalation)

SubClassOf(hes:PrivilegeEscalation hes:Tactics)

# Class: hes:PromptInjection (hes:PromptInjection)

SubClassOf(hes:PromptInjection hes:SoftwareWeaknesses)

# Class: hes:Protocol (hes:Protocol)

SubClassOf(hes:Protocol hes:Network)

# Class: hes:Reconnaissance (hes:Reconnaissance)

SubClassOf(hes:Reconnaissance hes:Tactics)

# Class: hes:Resource (hes:Resource)

SubClassOf(hes:Resource hes:Assets)

# Class: hes:ResourceDevelopment (hes:ResourceDevelopment)

SubClassOf(hes:ResourceDevelopment hes:Tactics)

# Class: hes:Risk (hes:Risk)

AnnotationAssertion(hes:Description hes:Risk "Risk  focuses on identifying potential risks  based on threats that could affect an operations, such as data breaches, cyberattacks, or system failures. It evaluates the likelihood of these threats occurring and their potential impact, allowing organizations to prioritize which threat need immediate attention and what mitigation strategies should be applied.")

# Class: hes:RiskAssessment (hes:RiskAssessment)

SubClassOf(hes:RiskAssessment hes:Assessment)

# Class: hes:RootkitHunter (hes:RootkitHunter)

SubClassOf(hes:RootkitHunter hes:SecurityMechanism)

# Class: hes:SecurityMechanism (hes:SecurityMechanism)

SubClassOf(hes:SecurityMechanism hes:Assets)

# Class: hes:SecurityThreats (hes:SecurityThreats)

AnnotationAssertion(hes:Description hes:SecurityThreats "Security threats are potential dangers that could exploit vulnerabilities in systems, resulting in damage, unauthorized access, or disruption.")

# Class: hes:SensitiveInformationDisclosure (hes:SensitiveInformationDisclosure)

SubClassOf(hes:SensitiveInformationDisclosure hes:SoftwareWeaknesses)

# Class: hes:Software (hes:Software)

SubClassOf(hes:Software hes:Assets)

# Class: hes:SoftwareWeaknesses (hes:SoftwareWeaknesses)

SubClassOf(hes:SoftwareWeaknesses hes:Weaknesses)

# Class: hes:SourceFile (hes:SourceFile)

SubClassOf(hes:SourceFile hes:Assets)

# Class: hes:Sourcecode (hes:Sourcecode)

SubClassOf(hes:Sourcecode hes:Assets)

# Class: hes:SpywareScanner (hes:SpywareScanner)

SubClassOf(hes:SpywareScanner hes:SecurityMechanism)

# Class: hes:SupplyChainVulnerabilities (hes:SupplyChainVulnerabilities)

SubClassOf(hes:SupplyChainVulnerabilities hes:SoftwareWeaknesses)

# Class: hes:System (hes:System)

SubClassOf(hes:System hes:Assets)

# Class: hes:Tactics (hes:Tactics)

AnnotationAssertion(hes:Description hes:Tactics "Tactics are tactical adversary goals during an attack. They represent the “why” of a technique: the reason for performing an action. Tactics serve as useful contextual categories for individual techniques and cover standard notations for things adversaries do during an operation.")

# Class: hes:Techniques (hes:Techniques)

AnnotationAssertion(hes:Description hes:Techniques "Techniques describe the means by which adversaries achieve tactical goals. They represent “how” an adversary achieves a tactical objective by performing an action. For example, an adversary may gain initial access by compromising the machine learning (ML) supply chain.

Techniques may also represent “what” an adversary gains by performing an action. This is a useful distinction for the ML Attack Staging tactic,where the adversary is typically creating or modifying an ML artifact that will be used in a subsequent tactical objective. There can be multiple techniques in each tactic category as there are many ways to achieve tactical objectives.")

# Class: hes:Tool (hes:Tool)

SubClassOf(hes:Tool hes:Assets)

# Class: hes:TrainingDataPoisioning (hes:TrainingDataPoisioning)

SubClassOf(hes:TrainingDataPoisioning hes:SoftwareWeaknesses)

# Class: hes:UnauthorizedResources (hes:UnauthorizedResources)

SubClassOf(hes:UnauthorizedResources hes:MalaciousActivity)

# Class: hes:UseOfHard-codedCredentials (hes:UseOfHard-codedCredentials)

SubClassOf(hes:UseOfHard-codedCredentials hes:SoftwareWeaknesses)

# Class: hes:VulnerabilityAssessment (hes:VulnerabilityAssessment)

SubClassOf(hes:VulnerabilityAssessment hes:Assessment)

# Class: hes:Weaknesses (hes:Weaknesses)

SubClassOf(hes:Weaknesses hes:AIEnabledSystems)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#1.Business&DataUnderstanding> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#1.Business&DataUnderstanding>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#1.Business&DataUnderstanding> "This phase focuses on establishing the scope, objectives, and feasibility of the ML application. Mitigations in this phase should ensure the robustness and security of the business requirements and the data.")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#1.Business&DataUnderstanding> hes:AILifeCycle)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#2.DataPreparation> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#2.DataPreparation>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#2.DataPreparation> "This phase involves the preparation of data, including cleaning, feature engineering, and balancing.")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#2.DataPreparation> hes:AILifeCycle)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#3.ModelEngineering> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#3.ModelEngineering>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#3.ModelEngineering> "This phase involves selecting, training, and refining the ML models. It’s also about ensuring the robustness of these models.")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#3.ModelEngineering> hes:AILifeCycle)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#4.ModelEvaluation> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#4.ModelEvaluation>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#4.ModelEvaluation> "This phase focuses on evaluating models for accuracy, robustness, and security before deployment.")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#4.ModelEvaluation> hes:AILifeCycle)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#5.Deployment> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#5.Deployment>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#5.Deployment> "This phase involves integrating the ML model into the production environment.")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#5.Deployment> hes:AILifeCycle)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#6.Monitoring&Maintenance> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#6.Monitoring&Maintenance>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#6.Monitoring&Maintenance> "This phase focuses on continuously monitoring the ML model’s performance and maintaining its security over time.")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#6.Monitoring&Maintenance> hes:AILifeCycle)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.AdditionalPhases> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.AdditionalPhases>)

SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.AdditionalPhases> hes:AILifeCycle)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.1.AI-BasedAccessManagement> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.1.AI-BasedAccessManagement>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.1.AI-BasedAccessManagement> "Access Management secures the system by controlling who can access it.")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.1.AI-BasedAccessManagement> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.AdditionalPhases>)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.2.AI-BasedSoftwareSecurityEngineering> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.2.AI-BasedSoftwareSecurityEngineering>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.2.AI-BasedSoftwareSecurityEngineering> "Software Security Engineering focuses on securing the software aspects of the system.")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.2.AI-BasedSoftwareSecurityEngineering> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.AdditionalPhases>)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.3.AI-BasedHardwareSecurityEngineering> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.3.AI-BasedHardwareSecurityEngineering>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.3.AI-BasedHardwareSecurityEngineering> "Hardware Security Engineering addresses the physical security of the hardware components.")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.3.AI-BasedHardwareSecurityEngineering> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.AdditionalPhases>)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.4.AdditionalActivities> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.4.AdditionalActivities>)

SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.4.AdditionalActivities> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.AdditionalPhases>)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackComplexity(AC)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackComplexity(AC)>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackComplexity(AC)> "Describes the conditions beyond the attacker's control that must exist to exploit the vulnerability.")
AnnotationAssertion(hes:Values <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackComplexity(AC)> "- Low (L): (No special conditions) 
- High (H): (Special conditions required)")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackComplexity(AC)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)>)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackVector(AV)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackVector(AV)>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackVector(AV)> "Reflects the context by which vulnerability exploitation is possible.	-")
AnnotationAssertion(hes:Values <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackVector(AV)> "- Network (N): (Exploitable remotely over a network) 
 - Adjacent Network (A):  (Exploitable from an adjacent network)
 - Local (L): (Exploitable with local access)
 - Physical (P): (Requires physical interaction)")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackVector(AV)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)>)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Availability(A)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Availability(A)>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Availability(A)> "Measures the impact on the availability of the affected component due to a successful exploit.")
AnnotationAssertion(hes:Values <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Availability(A)> "- None (N): (No impact) 
- Low (L): (Some impact, temporary interruptions) 
- High (H):  (Significant impact, extended unavailability)")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Availability(A)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)>)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)> "The Common Vulnerability Scoring System (CVSS) is a framework for rating the severity of security vulnerabilities in software. It provides a standardized way to capture the principal characteristics of a vulnerability and produce a numerical score reflecting its severity.
https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)> hes:VulnerabilityAssessment)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Confidentiality(C)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Confidentiality(C)>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Confidentiality(C)> "Measures the impact on the confidentiality of the information resources managed by a software component due to a successful exploit.	-")
AnnotationAssertion(hes:Values <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Confidentiality(C)> "- None (N): (No impact) 
 - Low (L): (Some impact, limited access) 
 - High (H): (Significant impact, sensitive information accessible)")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Confidentiality(C)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)>)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Integrity(I)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Integrity(I)>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Integrity(I)> "Measures the impact on the integrity of the information resources managed by a software component due to a successful exploit.")
AnnotationAssertion(hes:Values <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Integrity(I)> "- None (N): (No impact) 
- Low (L): (Some impact, limited modifications) 
- High (H):  (Significant impact, extensive modifications)")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Integrity(I)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)>)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures>)

SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:OrganizationalWeaknesses)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#PrivilegesRequired(PR)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#PrivilegesRequired(PR)>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#PrivilegesRequired(PR)> "Indicates the level of privileges an attacker must possess before successfully exploiting the vulnerability.")
AnnotationAssertion(hes:Values <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#PrivilegesRequired(PR)> "- None (N): (No privileges required) 
- Low (L): (Basic user capabilities required) 
- High (H): (Significant control required)")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#PrivilegesRequired(PR)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)>)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Scope(S)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Scope(S)>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Scope(S)> "Reflects the impact of the vulnerability on other components beyond the vulnerable component.")
AnnotationAssertion(hes:Values <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Scope(S)> "- Unchanged (U): (Affects only resources managed by the same authority)
- Changed (C): (Affects resources beyond the authority of the vulnerable component)")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Scope(S)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)>)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Server-SideRequestForgery(SSRF)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Server-SideRequestForgery(SSRF)>)

SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Server-SideRequestForgery(SSRF)> hes:SoftwareWeaknesses)

# Class: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#UserInteraction(UI)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#UserInteraction(UI)>)

AnnotationAssertion(hes:Description <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#UserInteraction(UI)> "Captures the requirement for user participation in the successful exploitation of the vulnerability.")
AnnotationAssertion(hes:Values <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#UserInteraction(UI)> "- None (N):  (No user interaction) 
 - Required (R): (User interaction required)")
SubClassOf(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#UserInteraction(UI)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#CommonVulnerabilityScoringSystem(CVSS)>)


############################
#   Named Individuals
############################

# Individual: hes:AISystem (hes:AISystem)

ClassAssertion(hes:System hes:AISystem)

# Individual: hes:AML.A0001 (hes:AML.A0001)

AnnotationAssertion(hes:Description hes:AML.A0001 "This type of threat/attack involves techniques designed to evade detection from AI-powered security systems. Attackers may use methods that manipulate data inputs in a way that causes the AI model to misclassify or overlook malicious activities. This can include the use of adversarial examples that subtly alter data points so they appear benign to AI models but are actually malicious.")
AnnotationAssertion(hes:Name hes:AML.A0001 "Detection Evasion")
ClassAssertion(hes:AMLA0001 hes:AML.A0001)
ObjectPropertyAssertion(Annotation(hes:Description "Vulnerabilities in input validation allow attackers to manipulate data inputs to evade detection.") hes:Exploits hes:AML.A0001 hes:CWE-20)
ObjectPropertyAssertion(Annotation(hes:Description "Malicious data manipulation can bypass security checks during deserialization.") hes:Exploits hes:AML.A0001 hes:CWE-502)
ObjectPropertyAssertion(Annotation(hes:Description "Manipulating training data to create adversarial examples that evade AI detection.") hes:Exploits hes:AML.A0001 hes:LLM03-TrainingDataPoisoning)
ObjectPropertyAssertion(hes:Has hes:AML.A0001 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0001 hes:Bots)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0001 hes:AML.M0003)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0001 hes:AML.M0008)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0001 hes:AML.M0015)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0001 hes:OWASP.M0019)
ObjectPropertyAssertion(hes:Violates hes:AML.A0001 hes:Confidentiality)
ObjectPropertyAssertion(hes:Violates hes:AML.A0001 hes:Integrity)

# Individual: hes:AML.A0002 (hes:AML.A0002)

AnnotationAssertion(hes:Description hes:AML.A0002 "Similar to detection evasion, bypassing detection focuses on avoiding triggering alerts from security systems. This might be achieved through techniques that exploit specific weaknesses in the detection algorithm, allowing malicious activities to go unnoticed.")
AnnotationAssertion(hes:Name hes:AML.A0002 "Bypassing Detection")
ClassAssertion(hes:AMLA0002 hes:AML.A0002)
ObjectPropertyAssertion(Annotation(hes:Description "Misconfigured systems might not detect specific patterns or activities that should trigger alerts.") hes:Exploits hes:AML.A0002 hes:CWE-16)
ObjectPropertyAssertion(Annotation(hes:Description "Unauthorized access to AI model details may help in crafting undetectable attacks.") hes:Exploits hes:AML.A0002 hes:CWE-200)
ObjectPropertyAssertion(Annotation(hes:Description "Weak authentication mechanisms can be bypassed, allowing attackers to avoid detection.") hes:Exploits hes:AML.A0002 hes:CWE-287)
ObjectPropertyAssertion(hes:Has hes:AML.A0002 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0002 hes:Bots)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0002 hes:AML.M0003)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0002 hes:AML.M0008)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0002 hes:AML.M0015)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0002 hes:AML.M0017)
ObjectPropertyAssertion(hes:Violates hes:AML.A0002 hes:Integrity)

# Individual: hes:AML.A0003 (hes:AML.A0003)

AnnotationAssertion(hes:Description hes:AML.A0003 "In this scenario, an attacker takes control of computational resources dedicated to AI operations (like GPUs or cloud computing resources). The hijacked resources are then used for other purposes, such as cryptocurrency mining or hosting malicious services, often resulting in performance degradation or financial loss.")
AnnotationAssertion(hes:Name hes:AML.A0003 "Resource Hijacking")
ClassAssertion(hes:AMLA0003 hes:AML.A0003)
ObjectPropertyAssertion(Annotation(hes:Description "Attackers can exploit shared resources to hijack computing power for unauthorized purposes.") hes:Exploits hes:AML.A0003 hes:CWE-1189)
ObjectPropertyAssertion(Annotation(hes:Description "Attackers may gain control of resources through insecure debug interfaces.") hes:Exploits hes:AML.A0003 hes:CWE-1191)
ObjectPropertyAssertion(Annotation(hes:Description "Side-channel attacks could be used to hijack AI resources like GPUs.") hes:Exploits hes:AML.A0003 hes:CWE-1300)
ObjectPropertyAssertion(hes:Has hes:AML.A0003 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0003 hes:Hacker)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0003 hes:AML.M0004)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0003 hes:AML.M0039)
ObjectPropertyAssertion(hes:Violates hes:AML.A0003 hes:Availability)

# Individual: hes:AML.A0004 (hes:AML.A0004)

AnnotationAssertion(hes:Description hes:AML.A0004 "An attacker replicates an AI model by training a new model on similar data or stealing the model architecture. This can lead to adversarial attacks where the replicated model is used to understand and exploit the target model’s weaknesses, leading to incorrect outputs or system failures.")
AnnotationAssertion(hes:Name hes:AML.A0004 "Replication Attack")
ClassAssertion(hes:AMLA0004 hes:AML.A0004)
ObjectPropertyAssertion(Annotation(hes:Description "Exploiting out-of-bounds reads to reverse-engineer or replicate AI models.") hes:Exploits hes:AML.A0004 hes:CWE-125)
ObjectPropertyAssertion(Annotation(hes:Description "Inadequate privilege controls can allow unauthorized model access and replication.") hes:Exploits hes:AML.A0004 hes:CWE-269)
ObjectPropertyAssertion(Annotation(hes:Description "Unauthorized access to AI models can lead to replication or adversarial attacks.") hes:Exploits hes:AML.A0004 hes:CWE-287)
ObjectPropertyAssertion(hes:Has hes:AML.A0004 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0004 hes:Competitors)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0004 hes:AML.M0003)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0004 hes:AML.M0006)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0004 hes:AML.M0018)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0004 hes:AML.M0020)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0004 hes:AML.M0022)
ObjectPropertyAssertion(hes:Violates hes:AML.A0004 hes:Confidentiality)
ObjectPropertyAssertion(hes:Violates hes:AML.A0004 hes:Integrity)

# Individual: hes:AML.A0005 (hes:AML.A0005)

AnnotationAssertion(hes:Description hes:AML.A0005 "This threat/attack exploits errors or oversights in configuration files of AI applications or repositories where these files are stored. Misconfigurations can expose sensitive data, allow unauthorized access, or enable the deployment of malicious code within an AI ecosystem.")
AnnotationAssertion(hes:Name hes:AML.A0005 "Misconfiguration of Source file/Repository")
ClassAssertion(hes:AMLA0005 hes:AML.A0005)
ObjectPropertyAssertion(Annotation(hes:Description "Misconfiguration might allow improper handling of inputs, leading to vulnerabilities.") hes:Exploits hes:AML.A0005 hes:CWE-1236)
ObjectPropertyAssertion(Annotation(hes:Description "Incorrect configurations can expose repositories to attacks, leading to unauthorized access or manipulation.") hes:Exploits hes:AML.A0005 hes:CWE-16)
ObjectPropertyAssertion(Annotation(hes:Description "Misconfiguration can result in improper access controls, allowing unauthorized changes.") hes:Exploits hes:AML.A0005 hes:CWE-284)
ObjectPropertyAssertion(hes:Has hes:AML.A0005 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0005 hes:DisgruntledEmployee)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0005 hes:AML.M0014)
ObjectPropertyAssertion(hes:Violates hes:AML.A0005 hes:Availability)
ObjectPropertyAssertion(hes:Violates hes:AML.A0005 hes:Confidentiality)
ObjectPropertyAssertion(hes:Violates hes:AML.A0005 hes:Integrity)

# Individual: hes:AML.A0006 (hes:AML.A0006)

AnnotationAssertion(hes:Description hes:AML.A0006 "This involves feeding a chatbot malicious input during its training phase or runtime, causing it to learn and then regurgitate incorrect or offensive information. This can damage the bot's reliability and manipulate it to serve the attacker's purposes.")
AnnotationAssertion(hes:Name hes:AML.A0006 "Chatbot Poisoning")
ClassAssertion(hes:AMLA0006 hes:AML.A0006)
ObjectPropertyAssertion(Annotation(hes:Description "Poor restrictions can be exploited to inject harmful data into chatbots.") hes:Exploits hes:AML.A0006 hes:CWE-1256)
ObjectPropertyAssertion(Annotation(hes:Description "Inadequate input validation can allow attackers to feed malicious inputs during chatbot training or usage.") hes:Exploits hes:AML.A0006 hes:CWE-20)
ObjectPropertyAssertion(Annotation(hes:Description "Injecting malicious data into chatbot training to cause harmful outputs.") hes:Exploits hes:AML.A0006 hes:LLM03-TrainingDataPoisoning)
ObjectPropertyAssertion(hes:Has hes:AML.A0006 hes:MediumRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0006 hes:Bots)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0006 hes:AML.M0007)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0006 hes:AML.M0008)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0006 hes:AML.M0018)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0006 hes:OWASP.M0019)
ObjectPropertyAssertion(hes:Violates hes:AML.A0006 hes:Integrity)

# Individual: hes:AML.A0007 (hes:AML.A0007)

AnnotationAssertion(hes:Description hes:AML.A0007 "DoS attacks in AI systems can occur when services that rely on AI computations are overwhelmed by requests. This might be achieved by feeding complex inputs that cause the AI model to consume excessive computational resources, effectively slowing down the system or crashing it.")
AnnotationAssertion(hes:Name hes:AML.A0007 "Denial of Services")
ClassAssertion(hes:AMLA0007 hes:AML.A0007)
ObjectPropertyAssertion(Annotation(hes:Description "Side-channel vulnerabilities may allow resource exhaustion, causing DoS.") hes:Exploits hes:AML.A0007 hes:CWE-1300)
ObjectPropertyAssertion(Annotation(hes:Description "Overloading AI systems by triggering out-of-bounds writes can lead to crashes or denials of service.") hes:Exploits hes:AML.A0007 hes:CWE-787)
ObjectPropertyAssertion(Annotation(hes:Description "Consuming excessive AI resources through crafted interactions, leading to DoS.") hes:Exploits hes:AML.A0007 hes:LLM04-ModelDenialOfService)
ObjectPropertyAssertion(hes:Has hes:AML.A0007 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0007 hes:Bots)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0007 hes:AML.M0004)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0007 hes:AML.M0039)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0007 hes:OWASP.M0011)
ObjectPropertyAssertion(hes:Violates hes:AML.A0007 hes:Availability)

# Individual: hes:AML.A0008 (hes:AML.A0008)

AnnotationAssertion(hes:Description hes:AML.A0008 "Specifically targeting generative models like GPT (Generative Pre-trained Transformer), this involves manipulating the training process or data to cause the model to generate biased or malicious content, misleading users or automating the spread of disinformation.")
AnnotationAssertion(hes:Name hes:AML.A0008 "GPT Poisoning")
ClassAssertion(hes:AMLA0008 hes:AML.A0008)
ObjectPropertyAssertion(Annotation(hes:Description "Poor input validation can allow harmful data to be used in training, corrupting model behavior.") hes:Exploits hes:AML.A0008 hes:CWE-20)
ObjectPropertyAssertion(Annotation(hes:Description "Manipulating GPT model training to generate biased or malicious outputs.") hes:Exploits hes:AML.A0008 hes:LLM03-TrainingDataPoisoning)
ObjectPropertyAssertion(Annotation(hes:Description "Poisoning could expose sensitive data through manipulated GPT outputs.") hes:Exploits hes:AML.A0008 hes:LLM06-SensitiveInformationDisclosure)
ObjectPropertyAssertion(hes:Has hes:AML.A0008 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0008 hes:Hacker)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0008 hes:AML.M0007)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0008 hes:AML.M0008)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0008 hes:AML.M0019)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0008 hes:OWASP.M0018)
ObjectPropertyAssertion(hes:Violates hes:AML.A0008 hes:Integrity)

# Individual: hes:AML.A0009 (hes:AML.A0009)

AnnotationAssertion(hes:Description hes:AML.A0009 "This threat/attack aims to circumvent AI-driven authentication mechanisms. Techniques might include the use of morphed images or synthetic biometric data (like voices or faces) that are recognized as legitimate users by AI systems, allowing unauthorized access.")
AnnotationAssertion(hes:Name hes:AML.A0009 "Authentication Evasion")
ClassAssertion(hes:AMLA0009 hes:AML.A0009)
ObjectPropertyAssertion(Annotation(hes:Description "Poor privilege management may allow evasion of AI-based authentication.") hes:Exploits hes:AML.A0009 hes:CWE-269)
ObjectPropertyAssertion(Annotation(hes:Description "Weak or improperly implemented authentication mechanisms can be evaded by attackers.") hes:Exploits hes:AML.A0009 hes:CWE-287)
ObjectPropertyAssertion(Annotation(hes:Description "Missing authentication checks allow attackers to bypass normal authentication processes.") hes:Exploits hes:AML.A0009 hes:CWE-306)
ObjectPropertyAssertion(hes:Has hes:AML.A0009 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0009 hes:PhishingAgents)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0009 hes:AML.M0009)
ObjectPropertyAssertion(hes:Violates hes:AML.A0009 hes:Confidentiality)

# Individual: hes:AML.A0010 (hes:AML.A0010)

AnnotationAssertion(hes:Description hes:AML.A0010 "A backdoor in an AI system is a hidden entry point that bypasses normal authentication and allows attackers to remotely access the system or data. In AI, this might involve embedding hidden behaviors in models that can be triggered by specific inputs known only to the attacker.")
AnnotationAssertion(hes:Name hes:AML.A0010 "Backdoor Attack")
ClassAssertion(hes:AMLA0010 hes:AML.A0010)
ObjectPropertyAssertion(Annotation(hes:Description "Exploiting debug interfaces to introduce backdoors.") hes:Exploits hes:AML.A0010 hes:CWE-1244)
ObjectPropertyAssertion(Annotation(hes:Description "Backdoors can be introduced via deserialization of malicious data.") hes:Exploits hes:AML.A0010 hes:CWE-502)
ObjectPropertyAssertion(Annotation(hes:Description "Failing protection mechanisms can allow backdoor insertion.") hes:Exploits hes:AML.A0010 hes:CWE-693)
ObjectPropertyAssertion(hes:Has hes:AML.A0010 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0010 hes:Hacker)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0010 hes:AML.M0003)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0010 hes:AML.M0005)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0010 hes:AML.M0013)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0010 hes:AML.M0033)
ObjectPropertyAssertion(hes:Violates hes:AML.A0010 hes:Confidentiality)
ObjectPropertyAssertion(hes:Violates hes:AML.A0010 hes:Integrity)

# Individual: hes:AML.A0011 (hes:AML.A0011)

AnnotationAssertion(hes:Description hes:AML.A0011 "This involves compromising the integrity of the AI model during its development stage. It can include inserting malicious code or data into AI libraries or dependencies, which are then passed on to users and businesses, compromising security.")
AnnotationAssertion(hes:Name hes:AML.A0011 "Supply Chain Poisoning")
ClassAssertion(hes:AMLA0011 hes:AML.A0011)
ObjectPropertyAssertion(Annotation(hes:Description "Improper authorization checks allow supply chain poisoning.") hes:Exploits hes:AML.A0011 hes:CWE-285)
ObjectPropertyAssertion(Annotation(hes:Description "Poisoning the AI supply chain through the injection of malicious code in the absence of integrity checks.") hes:Exploits hes:AML.A0011 hes:CWE-494)
ObjectPropertyAssertion(Annotation(hes:Description "Weaknesses in the supply chain allow the insertion of malicious code or data.") hes:Exploits hes:AML.A0011 hes:LLM05-SupplyChainVulnerabilitieç)
ObjectPropertyAssertion(hes:Has hes:AML.A0011 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0011 hes:Third-PartyVendors)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0011 hes:AML.M0001)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0011 hes:AML.M0016)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0011 hes:OWASP.M0007)
ObjectPropertyAssertion(hes:Violates hes:AML.A0011 hes:Confidentiality)
ObjectPropertyAssertion(hes:Violates hes:AML.A0011 hes:Integrity)

# Individual: hes:AML.A0012 (hes:AML.A0012)

AnnotationAssertion(hes:Description hes:AML.A0012 "Similar to authentication evasion, this focuses on methods to trick AI-based systems into granting access without proper credentials. This might involve fooling biometric systems or exploiting flaws in the authentication process.")
AnnotationAssertion(hes:Name hes:AML.A0012 "Bypassing Authentication")
ClassAssertion(hes:AMLA0012 hes:AML.A0012)
ObjectPropertyAssertion(Annotation(hes:Description "Misconfiguration might allow authentication processes to be bypassed.") hes:Exploits hes:AML.A0012 hes:CWE-16)
ObjectPropertyAssertion(Annotation(hes:Description "Inadequate privilege controls facilitate bypassing authentication mechanisms.") hes:Exploits hes:AML.A0012 hes:CWE-269)
ObjectPropertyAssertion(Annotation(hes:Description "Flaws in authentication mechanisms enable attackers to bypass them.") hes:Exploits hes:AML.A0012 hes:CWE-287)
ObjectPropertyAssertion(hes:Has hes:AML.A0012 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0012 hes:Hacker)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0012 hes:PhishingAgents)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0012 hes:AML.M0009)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0012 hes:AML.M0019)
ObjectPropertyAssertion(hes:Violates hes:AML.A0012 hes:Confidentiality)

# Individual: hes:AML.A0013 (hes:AML.A0013)

AnnotationAssertion(hes:Description hes:AML.A0013 "In AI systems, especially those dealing with natural language processing, prompt injection involves manipulating the system's output by injecting crafted inputs or prompts. This can mislead the AI to generate responses that could be harmful or unauthorized.")
AnnotationAssertion(hes:Name hes:AML.A0013 "Prompt Injection")
ClassAssertion(hes:AMLA0013 hes:AML.A0013)
ObjectPropertyAssertion(Annotation(hes:Description "Poor input validation allows for the injection of harmful prompts") hes:Exploits hes:AML.A0013 hes:CWE-20)
ObjectPropertyAssertion(Annotation(hes:Description "Race conditions might allow the injection of prompts to alter AI behavior.") hes:Exploits hes:AML.A0013 hes:CWE-362)
ObjectPropertyAssertion(Annotation(hes:Description "Manipulating AI system responses by injecting crafted prompts.") hes:Exploits hes:AML.A0013 hes:LLM01-PromptInjectionVulnerabiliy)
ObjectPropertyAssertion(hes:Has hes:AML.A0013 hes:MediumRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0013 hes:Bots)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0013 hes:AML.M0003)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0013 hes:AML.M0015)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0013 hes:OWASP.M0010)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0013 hes:OWASP.M0019)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0013 hes:OWASP.M0022)
ObjectPropertyAssertion(hes:Violates hes:AML.A0013 hes:Integrity)

# Individual: hes:AML.A0014 (hes:AML.A0014)

AnnotationAssertion(hes:Description hes:AML.A0014 "This severe security vulnerability allows an attacker to execute arbitrary code on the AI system’s host machine. It can be exploited via inputs that interact with the AI system, such as data inputs in an AI-driven application.")
AnnotationAssertion(hes:Name hes:AML.A0014 "Arbitrary Code Execution")
ClassAssertion(hes:AMLA0014 hes:AML.A0014)
ObjectPropertyAssertion(Annotation(hes:Description "Arbitrary code execution can be triggered through deserialization of malicious inputs.") hes:Exploits hes:AML.A0014 hes:CWE-502)
ObjectPropertyAssertion(Annotation(hes:Description "Exploiting out-of-bounds writes to execute arbitrary code.") hes:Exploits hes:AML.A0014 hes:CWE-787)
ObjectPropertyAssertion(Annotation(hes:Description "Exploiting code injection vulnerabilities to execute arbitrary code on AI systems.") hes:Exploits hes:AML.A0014 hes:CWE-94)
ObjectPropertyAssertion(hes:Has hes:AML.A0014 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0014 hes:DisgruntledEmployee)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0014 hes:Hacker)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0014 hes:AML.M0011)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0014 hes:AML.M0013)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0014 hes:AML.M0016)
ObjectPropertyAssertion(hes:Violates hes:AML.A0014 hes:Availability)
ObjectPropertyAssertion(hes:Violates hes:AML.A0014 hes:Integrity)

# Individual: hes:AML.A0015 (hes:AML.A0015)

AnnotationAssertion(hes:Description hes:AML.A0015 "In the context of AI, privacy leaks can occur when sensitive information is inadvertently included in the AI’s training data or outputs. Such leaks might expose personal data to unauthorized parties, violating privacy and compliance regulations.")
AnnotationAssertion(hes:Name hes:AML.A0015 "Privacy Leak")
ClassAssertion(hes:AMLA0015 hes:AML.A0015)
ObjectPropertyAssertion(Annotation(hes:Description "Out-of-bounds reads could expose private or sensitive data.") hes:Exploits hes:AML.A0015 hes:CWE-125)
ObjectPropertyAssertion(Annotation(hes:Description "AI systems may inadvertently expose sensitive information to unauthorized entities.") hes:Exploits hes:AML.A0015 hes:CWE-200)
ObjectPropertyAssertion(Annotation(hes:Description "Inadvertent leakage of sensitive information through AI outputs.") hes:Exploits hes:AML.A0015 hes:LLM06-SensitiveInformationDisclosure)
ObjectPropertyAssertion(hes:Has hes:AML.A0015 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0015 hes:DisgruntledEmployee)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0015 hes:PhishingAgents)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0015 hes:AML.M0012)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0015 hes:AML.M0020)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0015 hes:AML.M0029)
ObjectPropertyAssertion(hes:Violates hes:AML.A0015 hes:Confidentiality)

# Individual: hes:AML.A0016 (hes:AML.A0016)

AnnotationAssertion(hes:Description hes:AML.A0016 "Information about the system is deduced from alternative information streams such as voltage measurements or response timing,")
AnnotationAssertion(hes:Name hes:AML.A0016 "Side Channel Attack")
ClassAssertion(hes:AMLA0016 hes:AML.A0016)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0016 hes:CWE-1300)
ObjectPropertyAssertion(hes:Has hes:AML.A0016 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0016 hes:Third-PartyVendors)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0016 hes:AML.M0037)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0016 hes:AML.M0039)
ObjectPropertyAssertion(hes:Violates hes:AML.A0016 hes:Confidentiality)

# Individual: hes:AML.A0017 (hes:AML.A0017)

AnnotationAssertion(hes:Description hes:AML.A0017 "Systems are actively disrupted by faulty input data or physical environment disruptions.")
AnnotationAssertion(hes:Name hes:AML.A0017 "Fault Injection Attacks")
ClassAssertion(hes:AMLA0017 hes:AML.A0017)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0017 hes:CWE-1256)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0017 hes:CWE-190)
ObjectPropertyAssertion(hes:Has hes:AML.A0017 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0017 hes:Hacker)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0017 hes:AML.M0032)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0017 hes:AML.M0034)
ObjectPropertyAssertion(hes:Violates hes:AML.A0017 hes:Availability)
ObjectPropertyAssertion(hes:Violates hes:AML.A0017 hes:Integrity)

# Individual: hes:AML.A0018 (hes:AML.A0018)

AnnotationAssertion(hes:Description hes:AML.A0018 "Malicious backdoors are inserted into the hardware of the systems including GPUs and other platform circuitry.")
AnnotationAssertion(hes:Name hes:AML.A0018 "Hardware Trojan Attacks")
ClassAssertion(hes:AMLA0018 hes:AML.A0018)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0018 hes:CWE-1191)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0018 hes:CWE-1244)
ObjectPropertyAssertion(hes:Has hes:AML.A0018 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0018 hes:Hacker)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0018 hes:Third-PartyVendors)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0018 hes:AML.M0031)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0018 hes:AML.M0033)
ObjectPropertyAssertion(hes:Violates hes:AML.A0018 hes:Availability)
ObjectPropertyAssertion(hes:Violates hes:AML.A0018 hes:Confidentiality)
ObjectPropertyAssertion(hes:Violates hes:AML.A0018 hes:Integrity)

# Individual: hes:AML.A0019 (hes:AML.A0019)

AnnotationAssertion(hes:Description hes:AML.A0019 "Model theft attacks occur when an attacker gains access to the model’s parameters.")
AnnotationAssertion(hes:Name hes:AML.A0019 "Model Theft")
ClassAssertion(hes:AMLA0019 hes:AML.A0019)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0019 hes:CWE-287)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0019 hes:CWE-798)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0019 hes:LLM10-ModelTheft)
ObjectPropertyAssertion(hes:Has hes:AML.A0019 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0019 hes:Competitors)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0019 hes:AML.M0020)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0019 hes:AML.M0022)
ObjectPropertyAssertion(hes:Violates hes:AML.A0019 hes:Confidentiality)

# Individual: hes:AML.A0020 (hes:AML.A0020)

AnnotationAssertion(hes:Description hes:AML.A0020 "Model inversion attacks occur when an attacker reverse-engineers the model to extract information from it.")
AnnotationAssertion(hes:Name hes:AML.A0020 "Model Inversion Attack")
ClassAssertion(hes:AMLA0020 hes:AML.A0020)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0020 hes:CWE-125)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0020 hes:CWE-502)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0020 hes:LLM06-SensitiveInformationDisclosure)
ObjectPropertyAssertion(hes:Has hes:AML.A0020 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0020 hes:Competitors)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0020 hes:AML.M0025)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0020 hes:AML.M0028)
ObjectPropertyAssertion(hes:Violates hes:AML.A0020 hes:Confidentiality)

# Individual: hes:AML.A0021 (hes:AML.A0021)

AnnotationAssertion(hes:Description hes:AML.A0021 "In an Output Integrity Attack scenario, an attacker aims to modify or manipulate the output of a machine learning model in order to change its behavior or cause harm to the system it is used in.")
AnnotationAssertion(hes:Name hes:AML.A0021 "Output Integrity Attack")
ClassAssertion(hes:AMLA0021 hes:AML.A0021)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0021 hes:CWE-20)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0021 hes:CWE-284)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0021 hes:CWE-362)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0021 hes:CWE-787)
ObjectPropertyAssertion(hes:Has hes:AML.A0021 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0021 hes:DisgruntledEmployee)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0021 hes:Hacker)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0021 hes:AML.M0040)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0021 hes:OWASP.M0017)
ObjectPropertyAssertion(hes:Violates hes:AML.A0021 hes:Integrity)

# Individual: hes:AML.A0022 (hes:AML.A0022)

AnnotationAssertion(hes:Description hes:AML.A0022 "Membership inference attacks occur when an attacker manipulates the model’s training data in order to cause it to behave in a way that exposes sensitive information.")
AnnotationAssertion(hes:Name hes:AML.A0022 "Membership Inference Attack.")
ClassAssertion(hes:AMLA0022 hes:AML.A0022)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0022 hes:CWE-20)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0022 hes:LLM03-TrainingDataPoisoning)
ObjectPropertyAssertion(hes:Has hes:AML.A0022 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0022 hes:Hacker)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0022 hes:AML.M0027)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0022 hes:AML.M0029)
ObjectPropertyAssertion(hes:Violates hes:AML.A0022 hes:Confidentiality)
ObjectPropertyAssertion(hes:Violates hes:AML.A0022 hes:Integrity)

# Individual: hes:AML.A0023 (hes:AML.A0023)

AnnotationAssertion(hes:Description hes:AML.A0023 "Transfer learning attacks occur when an attacker trains a model on one task and then fine-tunes it on another task to cause it to behave in an undesirable way.")
AnnotationAssertion(hes:Name hes:AML.A0023 "Model Skewing Attacks.")
ClassAssertion(hes:AMLA0023 hes:AML.A0023)
ObjectPropertyAssertion(hes:Exploits hes:AML.A0023 hes:LLM03-TrainingDataPoisoning)
ObjectPropertyAssertion(hes:Has hes:AML.A0023 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:AML.A0023 hes:Competitors)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0023 hes:AML.M0008)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:AML.A0023 hes:OWASP.M0009)
ObjectPropertyAssertion(hes:Violates hes:AML.A0023 hes:Integrity)

# Individual: hes:AML.M0001 (hes:AML.M0001)

AnnotationAssertion(hes:Description hes:AML.M0001 "Limit the public release of technical information about the machine learning stack used in an organization's products or services. Technical knowledge of how machine learning is used can be leveraged by adversaries to perform targeting and tailor attacks to the target system. Additionally, consider limiting the release of organizational information - including physical locations, researcher names, and department structures - from which technical details such as machine learning techniques, model architectures, or datasets may be inferred.")
AnnotationAssertion(hes:Name hes:AML.M0001 "Limit Release of Public Information")
ClassAssertion(hes:Mitigations hes:AML.M0001)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0001 hes:AccessManagement)

# Individual: hes:AML.M0002 (hes:AML.M0002)

AnnotationAssertion(hes:Description hes:AML.M0002 "Decreasing the fidelity of model outputs provided to the end user can reduce an adversaries ability to extract information about the model and optimize attacks for the model.")
AnnotationAssertion(hes:Name hes:AML.M0002 "Passive ML Output Obfuscation")
ClassAssertion(hes:Mitigations hes:AML.M0002)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0002 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:AML.M0003 (hes:AML.M0003)

AnnotationAssertion(hes:Description hes:AML.M0003 "Use techniques to make machine learning models robust to adversarial inputs such as adversarial training or network distillation.")
AnnotationAssertion(hes:Name hes:AML.M0003 "Model Hardening")
ClassAssertion(hes:Mitigations hes:AML.M0003)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0003 hes:ModelEngineering)

# Individual: hes:AML.M0004 (hes:AML.M0004)

AnnotationAssertion(hes:Description hes:AML.M0004 "Limit the total number and rate of queries a user can perform.")
AnnotationAssertion(hes:Name hes:AML.M0004 "Restrict Number of Queries")
ClassAssertion(hes:Mitigations hes:AML.M0004)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0004 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:AML.M0005 (hes:AML.M0005)

AnnotationAssertion(hes:Description hes:AML.M0005 "Establish access controls on internal model registries and limit internal access to production models. Limit access to training data only to approved users.")
AnnotationAssertion(hes:Name hes:AML.M0005 "Control Access to ML Models and Data at Rest")
ClassAssertion(hes:Mitigations hes:AML.M0005)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0005 hes:AccessManagement)

# Individual: hes:AML.M0006 (hes:AML.M0006)

AnnotationAssertion(hes:Description hes:AML.M0006 "Use an ensemble of models for inference to increase robustness to adversarial inputs. Some attacks may effectively evade one model or model family but be ineffective against others.")
AnnotationAssertion(hes:Name hes:AML.M0006 "Use Ensemble Methods")
ClassAssertion(hes:Mitigations hes:AML.M0006)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0006 hes:ModelEngineering)

# Individual: hes:AML.M0007 (hes:AML.M0007)

AnnotationAssertion(hes:Description hes:AML.M0007 "Detect and remove or remediate poisoned training data. Training data should be sanitized prior to model training and recurrently for an active learning model.
Implement a filter to limit ingested training data. Establish a content policy that would remove unwanted content such as certain explicit or offensive language from being used.")
AnnotationAssertion(hes:Name hes:AML.M0007 "Sanitize Training Data")
ClassAssertion(hes:Mitigations hes:AML.M0007)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0007 hes:DataPreparation)

# Individual: hes:AML.M0008 (hes:AML.M0008)

AnnotationAssertion(hes:Description hes:AML.M0008 "Validate that machine learning models perform as intended by testing for backdoor triggers or adversarial bias. Monitor model for concept drift and training data drift, which may indicate data tampering and poisoning.")
AnnotationAssertion(hes:Name hes:AML.M0008 "Validate ML Model")
ClassAssertion(hes:Mitigations hes:AML.M0008)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0008 hes:ModelEvaluation)

# Individual: hes:AML.M0009 (hes:AML.M0009)

AnnotationAssertion(hes:Description hes:AML.M0009 "Incorporate various biometric sensors to improve authentication accuracy and resilience.")
AnnotationAssertion(hes:Name hes:AML.M0009 "Use Multi-Modal Sensors")
ClassAssertion(hes:Mitigations hes:AML.M0009)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0009 hes:ModelEngineering)

# Individual: hes:AML.M0010 (hes:AML.M0010)

AnnotationAssertion(hes:Description hes:AML.M0010 "Preprocess all inference data to nullify or reverse potential adversarial perturbations.")
AnnotationAssertion(hes:Name hes:AML.M0010 "Input Data Restoration")
ClassAssertion(hes:Mitigations hes:AML.M0010)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0010 hes:ModelEngineering)

# Individual: hes:AML.M0011 (hes:AML.M0011)

AnnotationAssertion(hes:Description hes:AML.M0011 "Prevent abuse of library loading mechanisms in the operating system and software to load untrusted code by configuring appropriate library loading mechanisms and investigating potential vulnerable software.
File formats such as pickle files that are commonly used to store machine learning models can contain exploits that allow for loading of malicious libraries.")
AnnotationAssertion(hes:Name hes:AML.M0011 "Restrict Library Loading")
ClassAssertion(hes:Mitigations hes:AML.M0011)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0011 hes:SoftwareSecurityEngineering)

# Individual: hes:AML.M0012 (hes:AML.M0012)

AnnotationAssertion(hes:Description hes:AML.M0012 "Encrypt sensitive data such as ML models to protect against adversaries attempting to access sensitive data.")
AnnotationAssertion(hes:Name hes:AML.M0012 "Encrypt Sensitive Information")
ClassAssertion(hes:Mitigations hes:AML.M0012)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0012 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:AML.M0013 (hes:AML.M0013)

AnnotationAssertion(hes:Description hes:AML.M0013 "Enforce binary and application integrity with digital signature verification to prevent untrusted code from executing. Adversaries can embed malicious code in ML software, models and Library. Enforcement of code signing can prevent the compromise of the machine learning supply chain and prevent execution of malicious code.")
AnnotationAssertion(hes:Name hes:AML.M0013 "Code Signing")
ClassAssertion(hes:Mitigations hes:AML.M0013)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0013 hes:SoftwareSecurityEngineering)

# Individual: hes:AML.M0014 (hes:AML.M0014)

AnnotationAssertion(hes:Description hes:AML.M0014 "Verify the cryptographic checksum of all machine learning artifacts to verify that the file or repositiory was not modified by an attacker.")
AnnotationAssertion(hes:Name hes:AML.M0014 "Verify ML Artifacts")
ClassAssertion(hes:Mitigations hes:AML.M0014)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0014 hes:ModelEngineering)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0014 hes:ModelEvaluation)

# Individual: hes:AML.M0015 (hes:AML.M0015)

AnnotationAssertion(hes:Description hes:AML.M0015 "Detect and block adversarial inputs or atypical queries that deviate from known benign behavior, exhibit behavior patterns observed in previous attacks or that come from potentially malicious IPs. Incorporate adversarial detection algorithms into the ML system prior to the ML model.")
AnnotationAssertion(hes:Name hes:AML.M0015 "Adversarial Input Detection")
ClassAssertion(hes:Mitigations hes:AML.M0015)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0015 hes:ModelEngineering)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0015 hes:ModelEvaluation)

# Individual: hes:AML.M0016 (hes:AML.M0016)

AnnotationAssertion(hes:Description hes:AML.M0016 "Vulnerability scanning is used to find potentially exploitable software vulnerabilities to remediate them.
File formats such as pickle files that are commonly used to store machine learning models can contain exploits that allow for arbitrary code execution. Both model artifacts and downstream products produced by models should be scanned for known vulnerabilities.")
AnnotationAssertion(hes:Name hes:AML.M0016 "Vulnerability Scanning")
ClassAssertion(hes:Mitigations hes:AML.M0016)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0016 hes:ModelEvaluation)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0016 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:AML.M0017 (hes:AML.M0017)

AnnotationAssertion(hes:Description hes:AML.M0017 "Deploying ML models to edge devices can increase the attack surface of the system. Consider serving models in the cloud to reduce the level of access the adversary has to the model. Also consider computing features in the cloud to prevent gray-box attacks, where an adversary has access to the model preprocessing methods.")
AnnotationAssertion(hes:Name hes:AML.M0017 "Model Distribution Methods")
ClassAssertion(hes:Mitigations hes:AML.M0017)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0017 hes:AccessManagement)

# Individual: hes:AML.M0018 (hes:AML.M0018)

AnnotationAssertion(hes:Description hes:AML.M0018 "Educate ML model developers on secure coding practices and ML vulnerabilities.")
AnnotationAssertion(hes:Name hes:AML.M0018 "User Training")
ClassAssertion(hes:Mitigations hes:AML.M0018)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0018 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Business&DataUnderstanding>)

# Individual: hes:AML.M0019 (hes:AML.M0019)

AnnotationAssertion(hes:Description hes:AML.M0019 "Require users to verify their identities before accessing a production model. Require authentication for API endpoints and monitor production model queries to ensure compliance with usage policies and to prevent model misuse.")
AnnotationAssertion(hes:Name hes:AML.M0019 "Control Access to ML Models and Data in Production")
ClassAssertion(hes:Mitigations hes:AML.M0019)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0019 hes:Deployment)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0019 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:AML.M0020 (hes:AML.M0020)

AnnotationAssertion(hes:Description hes:AML.M0020 "Encrypting the model’s code, training data, and other sensitive information can prevent attackers from being able to access and steal the model.")
AnnotationAssertion(hes:Name hes:AML.M0020 "Encryption")
ClassAssertion(hes:Mitigations hes:AML.M0020)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0020 hes:DataPreparation)

# Individual: hes:AML.M0021 (hes:AML.M0021)

AnnotationAssertion(hes:Description hes:AML.M0021 "Regularly backing up the model’s code, training data, and other sensitive information can ensure that it can be recovered in the event of a theft.")
AnnotationAssertion(hes:Name hes:AML.M0021 "Regular backups")
ClassAssertion(hes:Mitigations hes:AML.M0021)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0021 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:AML.M0022 (hes:AML.M0022)

AnnotationAssertion(hes:Description hes:AML.M0022 "Obfuscating the model’s code and making it difficult to reverse engineer can prevent attackers from being able to steal the model.")
AnnotationAssertion(hes:Name hes:AML.M0022 "Model Obfuscation")
ClassAssertion(hes:Mitigations hes:AML.M0022)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0022 hes:ModelEngineering)

# Individual: hes:AML.M0023 (hes:AML.M0023)

ClassAssertion(hes:Mitigations hes:AML.M0023)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0023 hes:ModelEvaluation)

# Individual: hes:AML.M0024 (hes:AML.M0024)

AnnotationAssertion(hes:Description hes:AML.M0024 "Regularly monitoring and auditing the model’s use can help detect and prevent theft by detecting when an attacker is attempting to access or steal the model.")
AnnotationAssertion(hes:Name hes:AML.M0024 "Monitoring and auditing")
ClassAssertion(hes:Mitigations hes:AML.M0024)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0024 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:AML.M0025 (hes:AML.M0025)

AnnotationAssertion(hes:Description hes:AML.M0025 "Regularly retraining the model can help to prevent the information leaked by model inversion attacks from becoming outdated. This can be done by incorporating new data and correcting any inaccuracies in the model’s predictions.")
AnnotationAssertion(hes:Name hes:AML.M0025 "Model retraining")
ClassAssertion(hes:Mitigations hes:AML.M0025)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0025 hes:ModelEngineering)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0025 hes:ModelEvaluation)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0025 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:AML.M0026 (hes:AML.M0026)

AnnotationAssertion(hes:Description hes:AML.M0026 "Communication channels between the model and the interface responsible for displaying the results should be secured using secure protocols such as SSL/TLS.")
AnnotationAssertion(hes:Name hes:AML.M0026 "Secure communication channels")
ClassAssertion(hes:Mitigations hes:AML.M0026)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0026 hes:Deployment)

# Individual: hes:AML.M0027 (hes:AML.M0027)

AnnotationAssertion(hes:Description hes:AML.M0027 "Training machine learning models on randomized or shuffled data can make it more difficult for an attacker to determine whether a particular example was included in the training dataset.")
AnnotationAssertion(hes:Name hes:AML.M0027 "Model training on randomized or shuffled data")
ClassAssertion(hes:Mitigations hes:AML.M0027)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0027 hes:DataPreparation)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0027 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Business&DataUnderstanding>)

# Individual: hes:AML.M0028 (hes:AML.M0028)

AnnotationAssertion(hes:Description hes:AML.M0028 "Regularization techniques such as L1 or L2 regularization can help prevent overfitting of the model to the training data, which can reduce the model’s ability to accurately determine whether a particular example was included in the training dataset.")
AnnotationAssertion(hes:Name hes:AML.M0028 "Regularization")
ClassAssertion(hes:Mitigations hes:AML.M0028)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0028 hes:DataPreparation)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0028 hes:ModelEngineering)

# Individual: hes:AML.M0029 (hes:AML.M0029)

AnnotationAssertion(hes:Description hes:AML.M0029 "Reducing the size of the training dataset or removing redundant or highly correlated features can help reduce the information an attacker can gain from a membership inference attack.")
AnnotationAssertion(hes:Name hes:AML.M0029 "Reducing the training data")
ClassAssertion(hes:Mitigations hes:AML.M0029)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0029 hes:DataPreparation)

# Individual: hes:AML.M0030 (hes:AML.M0030)

AnnotationAssertion(hes:Description hes:AML.M0030 "Use techniques such as digital signatures and checksums to verify that the feedback data received by the system is genuine, and reject any data that does not match the expected format.")
AnnotationAssertion(hes:Name hes:AML.M0030 "Verify the authenticity of feedback data")
ClassAssertion(hes:Mitigations hes:AML.M0030)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0030 hes:DataPreparation)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0030 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Business&DataUnderstanding>)

# Individual: hes:AML.M0031 (hes:AML.M0031)

AnnotationAssertion(hes:Description hes:AML.M0031 "Implement strict security controls and auditing processes during the manufacturing phase, including secure supply chain practices and hardware verification methods, to prevent unauthorized modifications to hardware components or firmware.")
AnnotationAssertion(hes:Name hes:AML.M0031 "Implement Security Control during hardware manufacturing")
ClassAssertion(hes:Mitigations hes:AML.M0031)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0031 hes:HardwareSecurityEngineering)

# Individual: hes:AML.M0032 (hes:AML.M0032)

AnnotationAssertion(hes:Description hes:AML.M0032 "Utilize tamper-evident packaging, secure shipping protocols, and block chain-based tracking systems to ensure the integrity of hardware and software during distribution, preventing unauthorized access or modifications.")
AnnotationAssertion(hes:Name hes:AML.M0032 "Utilize integrity controls in hardware")
ClassAssertion(hes:Mitigations hes:AML.M0032)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0032 hes:HardwareSecurityEngineering)

# Individual: hes:AML.M0033 (hes:AML.M0033)

AnnotationAssertion(hes:Description hes:AML.M0033 "Implement comprehensive hardware and firmware security validation processes, including regular integrity checks and the use of cryptographic signing, to detect and prevent the insertion of malicious logic.")
AnnotationAssertion(hes:Name hes:AML.M0033 "Malicious Logic Detection in hardware")
ClassAssertion(hes:Mitigations hes:AML.M0033)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0033 hes:HardwareSecurityEngineering)

# Individual: hes:AML.M0034 (hes:AML.M0034)

AnnotationAssertion(hes:Name hes:AML.M0034 "Design hardware with fault-tolerant architectures, employ error-correcting codes (ECC), and conduct regular fault injection testing to identify and mitigate vulnerabilities that could be exploited through hardware fault injection.")
AnnotationAssertion(hes:Name hes:AML.M0034 "Hardware fault-tolerant architectures")
ClassAssertion(hes:Mitigations hes:AML.M0034)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0034 hes:HardwareSecurityEngineering)

# Individual: hes:AML.M0035 (hes:AML.M0035)

AnnotationAssertion(hes:Description hes:AML.M0035 "Protect against reverse engineering by using obfuscation techniques, encryption of sensitive hardware components, and applying intellectual property protection measures to prevent the exploitation of vulnerabilities and unauthorized copying.")
AnnotationAssertion(hes:Name hes:AML.M0035 "Protection from Reverse Engineering")
ClassAssertion(hes:Mitigations hes:AML.M0035)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0035 hes:HardwareSecurityEngineering)

# Individual: hes:AML.M0036 (hes:AML.M0036)

AnnotationAssertion(hes:Description hes:AML.M0036 "Implement robust access control mechanisms, enforce the principle of least privilege, and regularly update and patch systems to close security gaps that could be exploited for privilege escalation.")
AnnotationAssertion(hes:Name hes:AML.M0036 "Principle of least privilege")
ClassAssertion(hes:Mitigations hes:AML.M0036)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0036 hes:Deployment)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0036 hes:HardwareSecurityEngineering)

# Individual: hes:AML.M0037 (hes:AML.M0037)

AnnotationAssertion(hes:Description hes:AML.M0037 "Conduct thorough risk assessments and enforce usage policies to limit the misuse of hardware functionalities, alongside implementing hardware security modules (HSMs) to ensure only intended uses of hardware features.")
AnnotationAssertion(hes:Name hes:AML.M0037 "Hardware security modules")
ClassAssertion(hes:Mitigations hes:AML.M0037)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0037 hes:HardwareSecurityEngineering)

# Individual: hes:AML.M0038 (hes:AML.M0038)

AnnotationAssertion(hes:Description hes:AML.M0038 "Secure hardware interfaces by using strong authentication methods, encryption, and interface-specific security protocols to prevent unauthorized manipulation and ensure control over AI hardware systems.")
AnnotationAssertion(hes:Name hes:AML.M0038 "Prevent Manipulation")
ClassAssertion(hes:Mitigations hes:AML.M0038)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0038 hes:HardwareSecurityEngineering)

# Individual: hes:AML.M0039 (hes:AML.M0039)

AnnotationAssertion(hes:Description hes:AML.M0039 "Implement resource isolation techniques, such as virtualization and containerization, and monitor resource usage to prevent manipulation that could lead to denial of service or data leakage in shared environments.")
AnnotationAssertion(hes:Name hes:AML.M0039 "Implement resource isolation techniques")
ClassAssertion(hes:Mitigations hes:AML.M0039)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0039 hes:Deployment)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0039 hes:HardwareSecurityEngineering)

# Individual: hes:AML.M0040 (hes:AML.M0040)

AnnotationAssertion(hes:Description hes:AML.M0040 "Deploy advanced content verification mechanisms, such as digital signatures and trusted identity frameworks, to ensure the authenticity of content and identities, preventing AI systems from making decisions based on spoofed or false data.")
AnnotationAssertion(hes:Name hes:AML.M0040 "Deploy advanced verification mechanisms")
ClassAssertion(hes:Mitigations hes:AML.M0040)
ObjectPropertyAssertion(hes:IsIncludedIn hes:AML.M0040 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:AML.T0000 (hes:AML.T0000)

ClassAssertion(hes:Reconnaissance hes:AML.T0000)
ClassAssertion(hes:Techniques hes:AML.T0000)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0000 hes:AML.A0001)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0000 hes:AML.A0002)

# Individual: hes:AML.T0000.000 (hes:AML.T0000.000)

AnnotationAssertion(hes:Description hes:AML.T0000.000 "Many of the publications accepted at premier machine learning conferences and journals come from commercial labs. Some journals and conferences are open access, others may require paying for access or a membership. These publications will often describe in detail all aspects of a particular approach for reproducibility. This information can be used by adversaries to implement the paper.")
AnnotationAssertion(hes:Name hes:AML.T0000.000 "Journals and Conference Proceedings")
ClassAssertion(hes:Reconnaissance hes:AML.T0000.000)
ClassAssertion(hes:Techniques hes:AML.T0000.000)

# Individual: hes:AML.T0000.001 (hes:AML.T0000.001)

AnnotationAssertion(hes:Description hes:AML.T0000.001 "Pre-Print repositories, such as arXiv, contain the latest academic research papers that haven't been peer reviewed. They may contain research notes, or technical reports that aren't typically published in journals or conference proceedings. Pre-print repositories also serve as a central location to share papers that have been accepted to journals. Searching pre-print repositories provide adversaries with a relatively up-to-date view of what researchers in the victim organization are working on.")
AnnotationAssertion(hes:Name hes:AML.T0000.001 "Pre-Print Repositories")
ClassAssertion(hes:Reconnaissance hes:AML.T0000.001)
ClassAssertion(hes:Techniques hes:AML.T0000.001)

# Individual: hes:AML.T0000.002 (hes:AML.T0000.002)

AnnotationAssertion(hes:Description hes:AML.T0000.002 "Research labs at academic institutions and Company R&D divisions often have blogs that highlight their use of machine learning and its application to the organizations unique problems. Individual researchers also frequently document their work in blogposts. An adversary may search for posts made by the target victim organization or its employees. In comparison to Journals and Conference Proceedings and Pre-Print Repositories this material will often contain more practical aspects of the machine learning system. This could include underlying technologies and frameworks used, and possibly some information about the API access and use case. This will help the adversary better understand how that organization is using machine learning internally and the details of their approach that could aid in tailoring an attack.")
AnnotationAssertion(hes:Name hes:AML.T0000.002 "Technical Blogs")
ClassAssertion(hes:Reconnaissance hes:AML.T0000.002)
ClassAssertion(hes:Techniques hes:AML.T0000.002)

# Individual: hes:AML.T0001 (hes:AML.T0001)

AnnotationAssertion(hes:Description hes:AML.T0001 "Much like the Search for Victim's Publicly Available Research Materials, there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models. This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may obtain Adversarial ML Attack Implementations or develop their own Adversarial ML Attacks if necessary.")
AnnotationAssertion(hes:Name hes:AML.T0001 "Search for Publicly Available Adversarial Vulnerability Analysis")
ClassAssertion(hes:Reconnaissance hes:AML.T0001)
ClassAssertion(hes:Techniques hes:AML.T0001)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0001 hes:AML.A0004)

# Individual: hes:AML.T0002 (hes:AML.T0002)

AnnotationAssertion(hes:Description hes:AML.T0002 "Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts. These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters. An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment. Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. Search Victim-Owned Websites or Search for Victim's Publicly Available Research Materials). These ML artifacts often provide adversaries with details of the ML task and approach.

ML artifacts can aid in an adversary's ability to Create Proxy ML Model. If these artifacts include pieces of the actual model in production, they can be used to directly Craft Adversarial Data. Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to Establish Accounts.

Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.")
AnnotationAssertion(hes:Name hes:AML.T0002 "Acquire Public ML Artifacts")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0002)
ClassAssertion(hes:Techniques hes:AML.T0002)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0002 hes:AML.A0004)

# Individual: hes:AML.T0002.000 (hes:AML.T0002.000)

AnnotationAssertion(hes:Description hes:AML.T0002.000 "Adversaries may collect public datasets to use in their operations. Datasets used by the victim organization or datasets that are representative of the data used by the victim organization may be valuable to adversaries. Datasets can be stored in cloud storage, or on victim-owned websites. Some datasets require the adversary to Establish Accounts for access.
Acquired datasets help the adversary advance their operations, stage attacks, and tailor attacks to the victim organization.")
AnnotationAssertion(hes:Name hes:AML.T0002.000 "Datasets")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0002.000)
ClassAssertion(hes:Techniques hes:AML.T0002.000)

# Individual: hes:AML.T0002.001 (hes:AML.T0002.001)

AnnotationAssertion(hes:Description hes:AML.T0002.001 "Adversaries may acquire public models to use in their operations. Adversaries may seek models used by the victim organization or models that are representative of those used by the victim organization. Representative models may include model architectures, or pre-trained models which define the architecture as well as model parameters from training on a dataset. The adversary may search public sources for common model architecture configuration file formats such as YAML or Python configuration files, and common model storage file formats such as ONNX (.onnx), HDF5 (.h5), Pickle (.pkl), PyTorch (.pth), or TensorFlow (.pb, .tflite).
Acquired models are useful in advancing the adversary's operations and are frequently used to tailor attacks to the victim model.")
AnnotationAssertion(hes:Name hes:AML.T0002.001 "Models")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0002.001)
ClassAssertion(hes:Techniques hes:AML.T0002.001)

# Individual: hes:AML.T0003 (hes:AML.T0003)

AnnotationAssertion(hes:Name hes:AML.T0003 "Adversaries may search websites owned by the victim for information that can be used during targeting. Victim-owned websites may contain technical details about their ML-enabled products or services. Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info. These sites may also have details highlighting business operations and relationships.

Adversaries may search victim-owned websites to gather actionable information. This information may help adversaries tailor their attacks (e.g. Adversarial ML Attacks or Manual Modification). Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. Search for Victim's Publicly Available Research Materials or Search for Publicly Available Adversarial Vulnerability Analysis)")
AnnotationAssertion(hes:Name hes:AML.T0003 "Search Victim-Owned Websites")
ClassAssertion(hes:Reconnaissance hes:AML.T0003)
ClassAssertion(hes:Techniques hes:AML.T0003)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0003 hes:AML.A0005)

# Individual: hes:AML.T0004 (hes:AML.T0004)

AnnotationAssertion(hes:Description hes:AML.T0004 "Adversaries may search open application repositories during targeting. Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.
Adversaries may craft search queries seeking applications that contain a ML-enabled components. Frequently, the next step is to Acquire Public ML Artifacts.")
AnnotationAssertion(hes:Name hes:AML.T0004 "Search Application Repositories")
ClassAssertion(hes:Reconnaissance hes:AML.T0004)
ClassAssertion(hes:Techniques hes:AML.T0004)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0004 hes:AML.A0005)

# Individual: hes:AML.T0005 (hes:AML.T0005)

AnnotationAssertion(hes:Description hes:AML.T0005 "Adversaries may obtain models to serve as proxies for the target model in use at the victim organization. Proxy models are used to simulate complete access to the target model in a fully offline manner.
Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.")
AnnotationAssertion(hes:Name hes:AML.T0005 "Create Proxy ML Model")
ClassAssertion(hes:AttackStaging hes:AML.T0005)
ClassAssertion(hes:Techniques hes:AML.T0005)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0005 hes:AML.A0004)

# Individual: hes:AML.T0005.000 (hes:AML.T0005.000)

AnnotationAssertion(hes:Description hes:AML.T0005.000 "Proxy models may be trained from ML artifacts (such as data, model architectures, and pre-trained models) that are representative of the target model gathered by the adversary. This can be used to develop attacks that require higher levels of access than the adversary has available or as a means to validate pre-existing attacks without interacting with the target model.")
AnnotationAssertion(hes:Name hes:AML.T0005.000 "Train Proxy via Gathered ML Artifacts")
ClassAssertion(hes:AttackStaging hes:AML.T0005.000)
ClassAssertion(hes:Techniques hes:AML.T0005.000)

# Individual: hes:AML.T0005.001 (hes:AML.T0005.001)

AnnotationAssertion(hes:Description hes:AML.T0005.001 "Adversaries may replicate a private model. By repeatedly querying the victim's ML Model Inference API Access, the adversary can collect the target model's inferences into a dataset. The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model.
A replicated model that closely mimic's the target model is a valuable resource in staging the attack. The adversary can use the replicated model to Craft Adversarial Data for various purposes (e.g. Evade ML Model, Spamming ML System with Chaff Data).")
AnnotationAssertion(hes:Name hes:AML.T0005.001 "Train Proxy via Replication")
ClassAssertion(hes:AttackStaging hes:AML.T0005.001)
ClassAssertion(hes:Techniques hes:AML.T0005.001)

# Individual: hes:AML.T0005.002 (hes:AML.T0005.002)

AnnotationAssertion(hes:Description hes:AML.T0005.002 "Adversaries may use an off-the-shelf pre-trained model as a proxy for the victim model to aid in staging the attack.")
AnnotationAssertion(hes:Name hes:AML.T0005.002 "Use Pre-Trained Model")
ClassAssertion(hes:AttackStaging hes:AML.T0005.002)
ClassAssertion(hes:Techniques hes:AML.T0005.002)

# Individual: hes:AML.T0006 (hes:AML.T0006)

AnnotationAssertion(hes:Description hes:AML.T0006 "An adversary may probe or scan the victim system to gather information for targeting. This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.")
AnnotationAssertion(hes:Name hes:AML.T0006 "Active Scanning")
ClassAssertion(hes:Reconnaissance hes:AML.T0006)
ClassAssertion(hes:Techniques hes:AML.T0006)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0006 hes:AML.A0001)

# Individual: hes:AML.T0007 (hes:AML.T0007)

AnnotationAssertion(hes:Description hes:AML.T0007 "Adversaries may search private sources to identify machine learning artifacts that exist on the system and gather information about them. These artifacts can include the software stack used to train and deploy models, training and testing data management systems, container registries, software repositories, and model zoos.
This information can be used to identify targets for further collection, exfiltration, or disruption, and to tailor and improve attacks.")
AnnotationAssertion(hes:Name hes:AML.T0007 "Discover ML Artifacts")
ClassAssertion(hes:Discovery hes:AML.T0007)
ClassAssertion(hes:Techniques hes:AML.T0007)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0007 hes:AML.A0004)

# Individual: hes:AML.T0008 (hes:AML.T0008)

AnnotationAssertion(hes:Description hes:AML.T0008 "Adversaries may buy, lease, or rent infrastructure for use throughout their operation. A wide variety of infrastructure exists for hosting and orchestrating adversary operations. Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services. Free resources may also be used, but they are typically limited.

Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation. Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services. Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.")
AnnotationAssertion(hes:Name hes:AML.T0008 "Acquire Infrastructure")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0008)
ClassAssertion(hes:Techniques hes:AML.T0008)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0008 hes:AML.A0003)

# Individual: hes:AML.T0008.000 (hes:AML.T0008.000)

AnnotationAssertion(hes:Description hes:AML.T0008.000 "Developing and staging machine learning attacks often requires expensive compute resources. Adversaries may need access to one or many GPUs in order to develop an attack. They may try to anonymously use free resources such as Google Colaboratory, or cloud resources such as AWS, Azure, or Google Cloud as an efficient way to stand up temporary resources to conduct operations. Multiple workspaces may be used to avoid detection.")
AnnotationAssertion(hes:Name hes:AML.T0008.000 "ML Development Workspaces")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0008.000)
ClassAssertion(hes:Techniques hes:AML.T0008.000)

# Individual: hes:AML.T0008.001 (hes:AML.T0008.001)

AnnotationAssertion(hes:Description hes:AML.T0008.001 "Adversaries may acquire consumer hardware to conduct their attacks. Owning the hardware provides the adversary with complete control of the environment. These devices can be hard to trace.")
AnnotationAssertion(hes:Name hes:AML.T0008.001 "Consumer Hardware")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0008.001)
ClassAssertion(hes:Techniques hes:AML.T0008.001)

# Individual: hes:AML.T0010 (hes:AML.T0010)

AnnotationAssertion(hes:Description hes:AML.T0010 "Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain. This could include GPU Hardware, Data and its annotations, parts of the ML ML Software stack, or the Model itself. In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.")
AnnotationAssertion(hes:Name hes:AML.T0010 "ML Supply Chain Compromise")
ClassAssertion(hes:InitialAccess hes:AML.T0010)
ClassAssertion(hes:Techniques hes:AML.T0010)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0010 hes:AML.A0011)

# Individual: hes:AML.T0010.000 (hes:AML.T0010.000)

AnnotationAssertion(hes:Description hes:AML.T0010.000 "Most machine learning systems require access to certain specialized hardware, typically GPUs. Adversaries can target machine learning systems by specifically targeting the GPU supply chain.")
AnnotationAssertion(hes:Name hes:AML.T0010.000 "GPU Hardware")
ClassAssertion(hes:InitialAccess hes:AML.T0010.000)
ClassAssertion(hes:Techniques hes:AML.T0010.000)

# Individual: hes:AML.T0010.001 (hes:AML.T0010.001)

AnnotationAssertion(hes:Description hes:AML.T0010.001 "ML Software")
AnnotationAssertion(hes:Description hes:AML.T0010.001 "Most machine learning systems rely on a limited set of machine learning frameworks. An adversary could get access to a large number of machine learning systems through a comprise of one of their supply chains. Many machine learning projects also rely on other open source implementations of various algorithms. These can also be compromised in a targeted way to get access to specific systems.")
ClassAssertion(hes:InitialAccess hes:AML.T0010.001)
ClassAssertion(hes:Techniques hes:AML.T0010.001)

# Individual: hes:AML.T0010.002 (hes:AML.T0010.002)

AnnotationAssertion(hes:Description hes:AML.T0010.002 "Data is a key vector of supply chain compromise for adversaries. Every machine learning project will require some form of data. Many rely on large open source datasets that are publicly available. An adversary could rely on compromising these sources of data. The malicious data could be a result of Poison Training Data or include traditional malware.

An adversary can also target private datasets in the labeling phase. The creation of private datasets will often require the hiring of outside labeling services. An adversary can poison a dataset by modifying the labels being generated by the labeling service.")
AnnotationAssertion(hes:Name hes:AML.T0010.002 "Data")
ClassAssertion(hes:InitialAccess hes:AML.T0010.002)
ClassAssertion(hes:Techniques hes:AML.T0010.002)

# Individual: hes:AML.T0010.003 (hes:AML.T0010.003)

AnnotationAssertion(hes:Description hes:AML.T0010.003 "Machine learning systems often rely on open sourced models in various ways. Most commonly, the victim organization may be using these models for fine tuning. These models will be downloaded from an external source and then used as the base for the model as it is tuned on a smaller, private dataset. Loading models often requires executing some saved code in the form of a saved model file. These can be compromised with traditional malware, or through some adversarial machine learning techniques.")
AnnotationAssertion(hes:Name hes:AML.T0010.003 "Model")
ClassAssertion(hes:InitialAccess hes:AML.T0010.003)
ClassAssertion(hes:Techniques hes:AML.T0010.003)

# Individual: hes:AML.T0011 (hes:AML.T0011)

AnnotationAssertion(hes:Description hes:AML.T0011 "An adversary may rely upon specific actions by a user in order to gain execution. Users may inadvertently execute unsafe code introduced via ML Supply Chain Compromise. Users may be subjected to social engineering to get them to execute malicious code by, for example, opening a malicious document file or link.")
AnnotationAssertion(hes:Name hes:AML.T0011 "User Execution")
ClassAssertion(hes:Execution hes:AML.T0011)
ClassAssertion(hes:Techniques hes:AML.T0011)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0011 hes:AML.A0006)

# Individual: hes:AML.T0011.000 (hes:AML.T0011.000)

AnnotationAssertion(hes:Description hes:AML.T0011.000 "Adversaries may develop unsafe ML artifacts that when executed have a deleterious effect. The adversary can use this technique to establish persistent access to systems. These models may be introduced via a ML Supply Chain Compromise.
Serialization of models is a popular technique for model storage, transfer, and loading. However, this format without proper checking presents an opportunity for code execution.")
AnnotationAssertion(hes:Name hes:AML.T0011.000 "Unsafe ML Artifacts")
ClassAssertion(hes:Execution hes:AML.T0011.000)
ClassAssertion(hes:Techniques hes:AML.T0011.000)

# Individual: hes:AML.T0012 (hes:AML.T0012)

AnnotationAssertion(hes:Description hes:AML.T0012 "Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access. Credentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services.
Compromised credentials may provide access to additional ML artifacts and allow the adversary to perform Discover ML Artifacts. Compromised credentials may also grant an adversary increased privileges such as write access to ML artifacts used during development or production.")
AnnotationAssertion(hes:Name hes:AML.T0012 "Valid Accounts")
ClassAssertion(hes:InitialAccess hes:AML.T0012)
ClassAssertion(hes:Techniques hes:AML.T0012)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0012 hes:AML.A0009)

# Individual: hes:AML.T0013 (hes:AML.T0013)

AnnotationAssertion(hes:Description hes:AML.T0013 "Adversaries may discover the ontology of a machine learning model's output space, for example, the types of objects a model can detect. The adversary may discovery the ontology by repeated queries to the model, forcing it to enumerate its output space. Or the ontology may be discovered in a configuration file or in documentation about the model.

The model ontology helps the adversary understand how the model is being used by the victim. It is useful to the adversary in creating targeted attacks.")
AnnotationAssertion(hes:Name hes:AML.T0013 "Discover ML Model Ontology")
ClassAssertion(hes:Discovery hes:AML.T0013)
ClassAssertion(hes:Techniques hes:AML.T0013)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0013 hes:AML.A0014)

# Individual: hes:AML.T0014 (hes:AML.T0014)

AnnotationAssertion(hes:Description hes:AML.T0014 "Adversaries may discover the general family of model. General information about the model may be revealed in documentation, or the adversary may use carefully constructed examples and analyze the model's responses to categorize it.
Knowledge of the model family can help the adversary identify means of attacking the model and help tailor the attack.")
AnnotationAssertion(hes:Name hes:AML.T0014 "Discover ML Model Family")
ClassAssertion(hes:Discovery hes:AML.T0014)
ClassAssertion(hes:Techniques hes:AML.T0014)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0014 hes:AML.A0014)

# Individual: hes:AML.T0015 (hes:AML.T0015)

AnnotationAssertion(hes:Description hes:AML.T0015 "Adversaries can Craft Adversarial Data that prevent a machine learning model from correctly identifying the contents of the data. This technique can be used to evade a downstream task where machine learning is utilized. The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.")
AnnotationAssertion(hes:Name hes:AML.T0015 "Evade ML Model")
ClassAssertion(hes:DefenseEvasion hes:AML.T0015)
ClassAssertion(hes:Impacts hes:AML.T0015)
ClassAssertion(hes:InitialAccess hes:AML.T0015)
ClassAssertion(hes:Techniques hes:AML.T0015)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0015 hes:AML.A0001)

# Individual: hes:AML.T0016 (hes:AML.T0016)

AnnotationAssertion(hes:Description hes:AML.T0016 "Adversaries may search for and obtain software capabilities for use in their operations. Capabilities may be specific to ML-based attacks Adversarial ML Attack Implementations or generic software tools repurposed for malicious intent (Software Tools). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.")
AnnotationAssertion(hes:Name hes:AML.T0016 "Obtain Capabilities")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0016)
ClassAssertion(hes:Techniques hes:AML.T0016)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0016 hes:AML.A0004)

# Individual: hes:AML.T0016.000 (hes:AML.T0016.000)

AnnotationAssertion(hes:Description hes:AML.T0016.000 "Adversaries may search for existing open source implementations of machine learning attacks. The research community often publishes their code for reproducibility and to further future research. Libraries intended for research purposes, such as CleverHans, the Adversarial Robustness Toolbox, and FoolBox, can be weaponized by an adversary. Adversaries may also obtain and use tools that were not originally designed for adversarial ML attacks as part of their attack.")
AnnotationAssertion(hes:Name hes:AML.T0016.000 "Adversarial ML Attack Implementations")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0016.000)
ClassAssertion(hes:Techniques hes:AML.T0016.000)

# Individual: hes:AML.T0016.001 (hes:AML.T0016.001)

AnnotationAssertion(hes:Description hes:AML.T0016.001 "Adversaries may search for and obtain software tools to support their operations. Software designed for legitimate use may be repurposed by an adversary for malicious intent. An adversary may modify or customize software tools to achieve their purpose. Software tools used to support attacks on ML systems are not necessarily ML-based themselves.")
AnnotationAssertion(hes:Name hes:AML.T0016.001 "Software Tools")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0016.001)
ClassAssertion(hes:Techniques hes:AML.T0016.001)

# Individual: hes:AML.T0017 (hes:AML.T0017)

AnnotationAssertion(hes:Description hes:AML.T0017 "Adversaries may develop their own capabilities to support operations. This process encompasses identifying requirements, building solutions, and deploying capabilities. Capabilities used to support attacks on ML systems are not necessarily ML-based themselves. Examples include setting up websites with adversarial information or creating Jupyter notebooks with obfuscated exfiltration code.")
AnnotationAssertion(hes:Name hes:AML.T0017 "Develop Capabilities")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0017)
ClassAssertion(hes:Techniques hes:AML.T0017)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0017 hes:AML.A0010)

# Individual: hes:AML.T0017.000 (hes:AML.T0017.000)

AnnotationAssertion(hes:Description hes:AML.T0017.000 "Adversarial ML Attacks")
AnnotationAssertion(hes:Description hes:AML.T0017.000 "Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point (Adversarial ML Attack Implementations). They may implement ideas described in public research papers or develop custom made attacks for the victim model.")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0017.000)
ClassAssertion(hes:Techniques hes:AML.T0017.000)

# Individual: hes:AML.T0018 (hes:AML.T0018)

AnnotationAssertion(hes:Description hes:AML.T0018 "Adversaries may introduce a backdoor into a ML model. A backdoored model operates performs as expected under typical conditions, but will produce the adversary's desired output when a trigger is introduced to the input data. A backdoored model provides the adversary with a persistent artifact on the victim system. The embedded vulnerability is typically activated at a later time by data samples with an Insert Backdoor Trigger.")
AnnotationAssertion(hes:Name hes:AML.T0018 "Backdoor ML Model")
ClassAssertion(hes:AttackStaging hes:AML.T0018)
ClassAssertion(hes:Persistence hes:AML.T0018)
ClassAssertion(hes:Techniques hes:AML.T0018)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0018 hes:AML.A0010)

# Individual: hes:AML.T0018.000 (hes:AML.T0018.000)

AnnotationAssertion(hes:Description hes:AML.T0018.000 "Adversaries may introduce a backdoor by training the model poisoned data, or by interfering with its training process. The model learns to associate an adversary-defined trigger with the adversary's desired output.")
AnnotationAssertion(hes:Name hes:AML.T0018.000 "Poison ML Model")
ClassAssertion(hes:AttackStaging hes:AML.T0018.000)
ClassAssertion(hes:Persistence hes:AML.T0018.000)
ClassAssertion(hes:Techniques hes:AML.T0018.000)

# Individual: hes:AML.T0018.001 (hes:AML.T0018.001)

AnnotationAssertion(hes:Description hes:AML.T0018.001 "Adversaries may introduce a backdoor into a model by injecting a payload into the model file. The payload detects the presence of the trigger and bypasses the model, instead producing the adversary's desired output.")
AnnotationAssertion(hes:Name hes:AML.T0018.001 "Inject Payload")
ClassAssertion(hes:AttackStaging hes:AML.T0018.001)
ClassAssertion(hes:Persistence hes:AML.T0018.001)
ClassAssertion(hes:Techniques hes:AML.T0018.001)

# Individual: hes:AML.T0019 (hes:AML.T0019)

AnnotationAssertion(hes:Description hes:AML.T0019 "Adversaries may Poison Training Data and publish it to a public location. The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset. This data may be introduced to a victim system via ML Supply Chain Compromise.")
AnnotationAssertion(hes:Name hes:AML.T0019 "Publish Poisoned Datasets")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0019)
ClassAssertion(hes:Techniques hes:AML.T0019)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0019 hes:AML.A0008)

# Individual: hes:AML.T0020 (hes:AML.T0020)

AnnotationAssertion(hes:Description hes:AML.T0020 "Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels. This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable. Data poisoning attacks may or may not require modifying the labels. The embedded vulnerability is activated at a later time by data samples with an Insert Backdoor Trigger
Poisoned data can be introduced via ML Supply Chain Compromise or the data may be poisoned after the adversary gains Initial Access to the system.")
AnnotationAssertion(hes:Name hes:AML.T0020 "Poison Training Data")
ClassAssertion(hes:Persistence hes:AML.T0020)
ClassAssertion(hes:ResourceDevelopment hes:AML.T0020)
ClassAssertion(hes:Techniques hes:AML.T0020)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0020 hes:AML.A0008)

# Individual: hes:AML.T0021 (hes:AML.T0021)

AnnotationAssertion(hes:Description hes:AML.T0021 "Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in ML Attack Staging, or for victim impersonation.")
AnnotationAssertion(hes:Name hes:AML.T0021 "Establish Accounts")
ClassAssertion(hes:ResourceDevelopment hes:AML.T0021)
ClassAssertion(hes:Techniques hes:AML.T0021)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0021 hes:AML.A0009)

# Individual: hes:AML.T0024 (hes:AML.T0024)

AnnotationAssertion(hes:Description hes:AML.T0024 "Adversaries may exfiltrate private information via ML Model Inference API Access. ML Models have been shown leak private information about their training data (e.g. Infer Training Data Membership, Invert ML Model). The model itself may also be extracted (Extract ML Model) for the purposes of ML Intellectual Property Theft.
Exfiltration of information relating to private training data raises privacy concerns. Private training data may include personally identifiable information, or other protected data.")
AnnotationAssertion(hes:Name hes:AML.T0024 "Exfiltration via ML Inference API")
ClassAssertion(hes:Exfiltration hes:AML.T0024)
ClassAssertion(hes:Techniques hes:AML.T0024)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0024 hes:AML.A0010)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0024 hes:AML.A0020)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0024 hes:AML.A0022)

# Individual: hes:AML.T0024.000 (hes:AML.T0024.000)

AnnotationAssertion(hes:Description hes:AML.T0024.000 "Adversaries may infer the membership of a data sample in its training set, which raises privacy concerns. Some strategies make use of a shadow model that could be obtained via Train Proxy via Replication, others use statistics of model prediction scores.
This can cause the victim model to leak private information, such as PII of those in the training set or other forms of protected IP.")
AnnotationAssertion(hes:Name hes:AML.T0024.000 "Infer Training Data Membership")
ClassAssertion(hes:Exfiltration hes:AML.T0024.000)
ClassAssertion(hes:Techniques hes:AML.T0024.000)

# Individual: hes:AML.T0024.001 (hes:AML.T0024.001)

AnnotationAssertion(hes:Description hes:AML.T0024.001 "Machine learning models' training data could be reconstructed by exploiting the confidence scores that are available via an inference API. By querying the inference API strategically, adversaries can back out potentially private information embedded within the training data. This could lead to privacy violations if the attacker can reconstruct the data of sensitive features used in the algorithm.")
AnnotationAssertion(hes:Name hes:AML.T0024.001 "Invert ML Model")
ClassAssertion(hes:Exfiltration hes:AML.T0024.001)
ClassAssertion(hes:Techniques hes:AML.T0024.001)

# Individual: hes:AML.T0024.002 (hes:AML.T0024.002)

AnnotationAssertion(hes:Description hes:AML.T0024.002 "Adversaries may extract a functional copy of a private model. By repeatedly querying the victim's ML Model Inference API Access, the adversary can collect the target model's inferences into a dataset. The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model.
Adversaries may extract the model to avoid paying per query in a machine learning as a service setting. Model extraction is used for ML Intellectual Property Theft.")
AnnotationAssertion(hes:Name hes:AML.T0024.002 "Extract ML Model")
ClassAssertion(hes:Exfiltration hes:AML.T0024.002)
ClassAssertion(hes:Techniques hes:AML.T0024.002)

# Individual: hes:AML.T0025 (hes:AML.T0025)

AnnotationAssertion(hes:Description hes:AML.T0025 "Adversaries may exfiltrate ML artifacts or other information relevant to their goals via traditional cyber means.")
AnnotationAssertion(hes:Name hes:AML.T0025 "Exfiltration via Cyber Means")
ClassAssertion(hes:Exfiltration hes:AML.T0025)
ClassAssertion(hes:Techniques hes:AML.T0025)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0025 hes:AML.A0020)

# Individual: hes:AML.T0029 (hes:AML.T0029)

AnnotationAssertion(hes:Description hes:AML.T0029 "Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service. Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded. Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.")
AnnotationAssertion(hes:Name hes:AML.T0029 "Denial of ML Service")
ClassAssertion(hes:Impacts hes:AML.T0029)
ClassAssertion(hes:Techniques hes:AML.T0029)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0029 hes:AML.A0007)

# Individual: hes:AML.T0031 (hes:AML.T0031)

AnnotationAssertion(hes:Description hes:AML.T0031 "Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time. This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.")
AnnotationAssertion(hes:Name hes:AML.T0031 "Erode ML Model Integrity")
ClassAssertion(hes:Impacts hes:AML.T0031)
ClassAssertion(hes:Techniques hes:AML.T0031)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0031 hes:AML.A0008)

# Individual: hes:AML.T0034 (hes:AML.T0034)

AnnotationAssertion(hes:Description hes:AML.T0034 "Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization. Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.")
AnnotationAssertion(hes:Name hes:AML.T0034 "Cost Harvesting")
ClassAssertion(hes:Impacts hes:AML.T0034)
ClassAssertion(hes:Techniques hes:AML.T0034)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0034 hes:AML.A0003)

# Individual: hes:AML.T0035 (hes:AML.T0035)

AnnotationAssertion(hes:Description hes:AML.T0035 "Adversaries may collect ML artifacts for Exfiltration or for use in ML Attack Staging. ML artifacts include models and datasets as well as other telemetry data produced when interacting with a model.")
AnnotationAssertion(hes:Name hes:AML.T0035 "ML Artifact Collection")
ClassAssertion(hes:Collection hes:AML.T0035)
ClassAssertion(hes:Techniques hes:AML.T0035)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0035 hes:AML.A0004)

# Individual: hes:AML.T0036 (hes:AML.T0036)

AnnotationAssertion(hes:Description hes:AML.T0036 "Adversaries may leverage information repositories to mine valuable information. Information repositories are tools that allow for storage of information, typically to facilitate collaboration or information sharing between users, and can store a wide variety of data that may aid adversaries in further objectives, or direct access to the target information.
Information stored in a repository may vary based on the specific instance or environment. Specific common information repositories include SharePoint, Confluence, and enterprise databases such as SQL Server.")
AnnotationAssertion(hes:Name hes:AML.T0036 "Data from Information Repositories")
ClassAssertion(hes:Collection hes:AML.T0036)
ClassAssertion(hes:Techniques hes:AML.T0036)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0036 hes:AML.A0006)

# Individual: hes:AML.T0037 (hes:AML.T0037)

AnnotationAssertion(hes:Description hes:AML.T0037 "Adversaries may search local system sources, such as file systems and configuration files or local databases, to find files of interest and sensitive data prior to Exfiltration.
This can include basic fingerprinting information and sensitive data such as ssh keys.")
AnnotationAssertion(hes:Name hes:AML.T0037 "Data from Local System")
ClassAssertion(hes:Collection hes:AML.T0037)
ClassAssertion(hes:Techniques hes:AML.T0037)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0037 hes:AML.A0006)

# Individual: hes:AML.T0040 (hes:AML.T0040)

AnnotationAssertion(hes:Description hes:AML.T0040 "Adversaries may gain access to a model via legitimate access to the inference API. Inference API access can be a source of information to the adversary (Discover ML Model Ontology, Discover ML Model Family), a means of staging the attack (Verify Attack, Craft Adversarial Data), or for introducing data to the target system for Impact (Evade ML Model, Erode ML Model Integrity).")
AnnotationAssertion(hes:Name hes:AML.T0040 "ML Model Inference API Access")
ClassAssertion(hes:ModelAccess hes:AML.T0040)
ClassAssertion(hes:Techniques hes:AML.T0040)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0040 hes:AML.A0022)

# Individual: hes:AML.T0041 (hes:AML.T0041)

AnnotationAssertion(hes:Description hes:AML.T0041 "In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks. If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected. By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.")
AnnotationAssertion(hes:Name hes:AML.T0041 "Physical Environment Access")
ClassAssertion(hes:ModelAccess hes:AML.T0041)
ClassAssertion(hes:Techniques hes:AML.T0041)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0041 hes:AML.A0011)

# Individual: hes:AML.T0042 (hes:AML.T0042)

AnnotationAssertion(hes:Description hes:AML.T0042 "Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model. This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing. The adversary may verify the attack once but use it against many edge devices running copies of the target model. The adversary may verify their attack digitally, then deploy it in the Physical Environment Access at a later time. Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.")
AnnotationAssertion(hes:Name hes:AML.T0042 "Verify Attack")
ClassAssertion(hes:AttackStaging hes:AML.T0042)
ClassAssertion(hes:Techniques hes:AML.T0042)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0042 hes:AML.A0004)

# Individual: hes:AML.T0043 (hes:AML.T0043)

AnnotationAssertion(hes:Description hes:AML.T0043 "Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model. Effects can range from misclassification, to missed detections, to maximizing energy consumption. Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect. For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.
Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as White-Box Optimization, Black-Box Optimization, Black-Box Transfer, or Manual Modification.
The adversary may Verify Attack their approach works if they have white-box or inference API access to the model. This allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed. They can then use the attack at a later time to accomplish their goals. An adversary may optimize adversarial examples for Evade ML Model, or to Erode ML Model Integrity.")
AnnotationAssertion(hes:Name hes:AML.T0043 "Craft Adversarial Data")
ClassAssertion(hes:AttackStaging hes:AML.T0043)
ClassAssertion(hes:Techniques hes:AML.T0043)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0043 hes:AML.A0001)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0043 hes:AML.A0002)

# Individual: hes:AML.T0043.000 (hes:AML.T0043.000)

AnnotationAssertion(hes:Description hes:AML.T0043.000 "In White-Box Optimization, the adversary has full access to the target model and optimizes the adversarial example directly. Adversarial examples trained in this manner are most effective against the target model.")
AnnotationAssertion(hes:Name hes:AML.T0043.000 "White-Box Optimization")
ClassAssertion(hes:AttackStaging hes:AML.T0043.000)
ClassAssertion(hes:Techniques hes:AML.T0043.000)

# Individual: hes:AML.T0043.001 (hes:AML.T0043.001)

AnnotationAssertion(hes:Description hes:AML.T0043.001 "In Black-Box attacks, the adversary has black-box (i.e. ML Model Inference API Access via API access) access to the target model. With black-box attacks, the adversary may be using an API that the victim is monitoring. These attacks are generally less effective and require more inferences than White-Box Optimization attacks, but they require much less access.")
AnnotationAssertion(hes:Name hes:AML.T0043.001 "Black-Box Optimization")
ClassAssertion(hes:AttackStaging hes:AML.T0043.001)
ClassAssertion(hes:Techniques hes:AML.T0043.001)

# Individual: hes:AML.T0043.002 (hes:AML.T0043.002)

AnnotationAssertion(hes:Description hes:AML.T0043.002 "In Black-Box Transfer attacks, the adversary uses one or more proxy models (trained via Create Proxy ML Model or Train Proxy via Replication) they have full access to and are representative of the target model. The adversary uses White-Box Optimization on the proxy models to generate adversarial examples. If the set of proxy models are close enough to the target model, the adversarial example should generalize from one to another. This means that an attack that works for the proxy models will likely then work for the target model. If the adversary has ML Model Inference API Access, they may use Verify Attack to confirm the attack is working and incorporate that information into their training process.")
AnnotationAssertion(hes:Name hes:AML.T0043.002 "Black-Box Transfer")
ClassAssertion(hes:AttackStaging hes:AML.T0043.002)
ClassAssertion(hes:Techniques hes:AML.T0043.002)

# Individual: hes:AML.T0043.003 (hes:AML.T0043.003)

AnnotationAssertion(hes:Description hes:AML.T0043.003 "Adversaries may manually modify the input data to craft adversarial data. They may use their knowledge of the target model to modify parts of the data they suspect helps the model in performing its task. The adversary may use trial and error until they are able to verify they have a working adversarial input.")
AnnotationAssertion(hes:Name hes:AML.T0043.003 "Manual Modification")
ClassAssertion(hes:AttackStaging hes:AML.T0043.003)
ClassAssertion(hes:Techniques hes:AML.T0043.003)

# Individual: hes:AML.T0043.004 (hes:AML.T0043.004)

AnnotationAssertion(hes:Description hes:AML.T0043.004 "The adversary may add a perceptual trigger into inference data. The trigger may be imperceptible or non-obvious to humans. This technique is used in conjunction with Poison ML Model and allows the adversary to produce their desired effect in the target model.")
AnnotationAssertion(hes:Name hes:AML.T0043.004 "Insert Backdoor Trigger")
ClassAssertion(hes:AttackStaging hes:AML.T0043.004)
ClassAssertion(hes:Techniques hes:AML.T0043.004)

# Individual: hes:AML.T0044 (hes:AML.T0044)

AnnotationAssertion(hes:Name hes:AML.T0044 "Adversaries may gain full \"white-box\" access to a machine learning model. This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology. They may exfiltrate the model to Craft Adversarial Data and Verify Attack in an offline where it is hard to detect their behavior.")
AnnotationAssertion(hes:Name hes:AML.T0044 "Full ML Model Access")
ClassAssertion(hes:ModelAccess hes:AML.T0044)
ClassAssertion(hes:Techniques hes:AML.T0044)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0044 hes:AML.A0020)

# Individual: hes:AML.T0046 (hes:AML.T0046)

AnnotationAssertion(hes:Description hes:AML.T0046 "Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections. This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.")
AnnotationAssertion(hes:Name hes:AML.T0046 "Spamming ML System with Chaff Data")
ClassAssertion(hes:Impacts hes:AML.T0046)
ClassAssertion(hes:Techniques hes:AML.T0046)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0046 hes:AML.A0007)

# Individual: hes:AML.T0047 (hes:AML.T0047)

AnnotationAssertion(hes:Description hes:AML.T0047 "Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model. This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.")
AnnotationAssertion(hes:Name hes:AML.T0047 "ML-Enabled Product or Service")
ClassAssertion(hes:ModelAccess hes:AML.T0047)
ClassAssertion(hes:Techniques hes:AML.T0047)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0047 hes:AML.A0012)

# Individual: hes:AML.T0048 (hes:AML.T0048)

AnnotationAssertion(hes:Description hes:AML.T0048 "Adversaries may abuse their access to a victim system and use its resources or capabilities to further their goals by causing harms external to that system. These harms could affect the organization (e.g. Financial Harm, Reputational Harm), its users (e.g. User Harm), or the general public (e.g. Societal Harm).")
AnnotationAssertion(hes:Name hes:AML.T0048 "External Harms")
ClassAssertion(hes:Impacts hes:AML.T0048)
ClassAssertion(hes:Techniques hes:AML.T0048)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0048 hes:AML.A0008)

# Individual: hes:AML.T0048.000 (hes:AML.T0048.000)

AnnotationAssertion(hes:Description hes:AML.T0048.000 "Financial harm involves the loss of wealth, property, or other monetary assets due to theft, fraud or forgery, or pressure to provide financial resources to the adversary.")
AnnotationAssertion(hes:Name hes:AML.T0048.000 "Financial Harm")
ClassAssertion(hes:Impacts hes:AML.T0048.000)
ClassAssertion(hes:Techniques hes:AML.T0048.000)

# Individual: hes:AML.T0048.001 (hes:AML.T0048.001)

AnnotationAssertion(hes:Description hes:AML.T0048.001 "Reputational harm involves a degradation of public perception and trust in organizations. Examples of reputation-harming incidents include scandals or false impersonations.")
AnnotationAssertion(hes:Name hes:AML.T0048.001 "Reputational Harm")
ClassAssertion(hes:Impacts hes:AML.T0048.001)
ClassAssertion(hes:Techniques hes:AML.T0048.001)

# Individual: hes:AML.T0048.002 (hes:AML.T0048.002)

AnnotationAssertion(hes:Description hes:AML.T0048.002 "Societal harms might generate harmful outcomes that reach either the general public or specific vulnerable groups such as the exposure of children to vulgar content.")
AnnotationAssertion(hes:Name hes:AML.T0048.002 "Societal Harm")
ClassAssertion(hes:Impacts hes:AML.T0048.002)
ClassAssertion(hes:Techniques hes:AML.T0048.002)

# Individual: hes:AML.T0048.003 (hes:AML.T0048.003)

AnnotationAssertion(hes:Description hes:AML.T0048.003 "User harms may encompass a variety of harm types including financial and reputational that are directed at or felt by individual victims of the attack rather than at the organization level.")
AnnotationAssertion(hes:Name hes:AML.T0048.003 "User Harm")
ClassAssertion(hes:Impacts hes:AML.T0048.003)
ClassAssertion(hes:Techniques hes:AML.T0048.003)

# Individual: hes:AML.T0048.004 (hes:AML.T0048.004)

AnnotationAssertion(hes:Description hes:AML.T0048.004 "Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.
Proprietary training data is costly to collect and annotate and may be a target for Exfiltration and theft.
MLaaS providers charge for use of their API. An adversary who has stolen a model via Exfiltration or via Extract ML Model now has unlimited use of that service without paying the owner of the intellectual property.")
AnnotationAssertion(hes:Name hes:AML.T0048.004 "ML Intellectual Property Theft")
ClassAssertion(hes:Impacts hes:AML.T0048.004)
ClassAssertion(hes:Techniques hes:AML.T0048.004)

# Individual: hes:AML.T0049 (hes:AML.T0049)

AnnotationAssertion(hes:Description hes:AML.T0049 "Adversaries may attempt to take advantage of a weakness in an Internet-facing computer or program using software, data, or commands in order to cause unintended or unanticipated behavior. The weakness in the system can be a bug, a glitch, or a design vulnerability. These applications are often websites, but can include databases (like SQL), standard services (like SMB or SSH), network device administration and management protocols (like SNMP and Smart Install), and any other applications with Internet accessible open sockets, such as web servers and related services.")
AnnotationAssertion(hes:Name hes:AML.T0049 "Exploit Public-Facing Application&")
ClassAssertion(hes:InitialAccess hes:AML.T0049)
ClassAssertion(hes:Techniques hes:AML.T0049)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0049 hes:AML.A0005)

# Individual: hes:AML.T0050 (hes:AML.T0050)

AnnotationAssertion(hes:Description hes:AML.T0050 "Adversaries may abuse command and script interpreters to execute commands, scripts, or binaries. These interfaces and languages provide ways of interacting with computer systems and are a common feature across many different platforms. Most systems come with some built-in command-line interface and scripting capabilities, for example, macOS and Linux distributions include some flavor of Unix Shell while Windows installations include the Windows Command Shell and PowerShell.
There are also cross-platform interpreters such as Python, as well as those commonly associated with client applications such as JavaScript and Visual Basic.
Adversaries may abuse these technologies in various ways as a means of executing arbitrary commands. Commands and scripts can be embedded in Initial Access payloads delivered to victims as lure documents or as secondary payloads downloaded from an existing C2. Adversaries may also execute commands through interactive terminals/shells, as well as utilize various Remote Services in order to achieve remote Execution.")
AnnotationAssertion(hes:Name hes:AML.T0050 "Command and Scripting Interpreter")
ClassAssertion(hes:Execution hes:AML.T0050)
ClassAssertion(hes:Techniques hes:AML.T0050)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0050 hes:AML.A0014)

# Individual: hes:AML.T0051 (hes:AML.T0051)

AnnotationAssertion(hes:Description hes:AML.T0051 "An adversary may craft malicious prompts as inputs to an LLM that cause the LLM to act in unintended ways. These \"prompt injections\" are often designed to cause the model to ignore aspects of its original instructions and follow the adversary's instructions instead.
Prompt Injections can be an initial access vector to the LLM that provides the adversary with a foothold to carry out other steps in their operation. They may be designed to bypass defenses in the LLM, or allow the adversary to issue privileged commands. The effects of a prompt injection can persist throughout an interactive session with an LLM.
Malicious prompts may be injected directly by the adversary (Direct) either to leverage the LLM to generate harmful content or to gain a foothold on the system and lead to further effects. Prompts may also be injected indirectly when as part of its normal operation the LLM ingests the malicious prompt from another data source (Indirect). This type of injection can be used by the adversary to a foothold on the system or to target the user of the LLM.")
AnnotationAssertion(hes:Name hes:AML.T0051 "LLM Prompt Injection")
ClassAssertion(hes:DefenseEvasion hes:AML.T0051)
ClassAssertion(hes:InitialAccess hes:AML.T0051)
ClassAssertion(hes:Persistence hes:AML.T0051)
ClassAssertion(hes:PrivilegeEscalation hes:AML.T0051)
ClassAssertion(hes:Techniques hes:AML.T0051)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0051 hes:AML.A0013)

# Individual: hes:AML.T0051.000 (hes:AML.T0051.000)

AnnotationAssertion(hes:Description hes:AML.T0051.000 "An adversary may inject prompts directly as a user of the LLM. This type of injection may be used by the adversary to gain a foothold in the system or to misuse the LLM itself, as for example to generate harmful content.")
AnnotationAssertion(hes:Name hes:AML.T0051.000 "Direct")
ClassAssertion(hes:DefenseEvasion hes:AML.T0051.000)
ClassAssertion(hes:InitialAccess hes:AML.T0051.000)
ClassAssertion(hes:Persistence hes:AML.T0051.000)
ClassAssertion(hes:PrivilegeEscalation hes:AML.T0051.000)
ClassAssertion(hes:Techniques hes:AML.T0051.000)

# Individual: hes:AML.T0051.001 (hes:AML.T0051.001)

AnnotationAssertion(hes:Description hes:AML.T0051.001 "An adversary may inject prompts indirectly via separate data channel ingested by the LLM such as include text or multimedia pulled from databases or websites. These malicious prompts may be hidden or obfuscated from the user. This type of injection may be used by the adversary to gain a foothold in the system or to target an unwitting user of the system.")
AnnotationAssertion(hes:Name hes:AML.T0051.001 "Indirect")
ClassAssertion(hes:DefenseEvasion hes:AML.T0051.001)
ClassAssertion(hes:InitialAccess hes:AML.T0051.001)
ClassAssertion(hes:Persistence hes:AML.T0051.001)
ClassAssertion(hes:PrivilegeEscalation hes:AML.T0051.001)
ClassAssertion(hes:Techniques hes:AML.T0051.001)

# Individual: hes:AML.T0052 (hes:AML.T0052)

AnnotationAssertion(hes:Description hes:AML.T0052 "Adversaries may send phishing messages to gain access to victim systems. All forms of phishing are electronically delivered social engineering. Phishing can be targeted, known as spearphishing. In spearphishing, a specific individual, company, or industry will be targeted by the adversary. More generally, adversaries can conduct non-targeted phishing, such as in mass malware spam campaigns.
Generative AI, including LLMs that generate synthetic text, visual deepfakes of faces, and audio deepfakes of speech, is enabling adversaries to scale targeted phishing campaigns. LLMs can interact with users via text conversations and can be programmed with a meta prompt to phish for sensitive information. Deepfakes can be use in impersonation as an aid to phishing.")
AnnotationAssertion(hes:Name hes:AML.T0052 "Phishing")
ClassAssertion(hes:InitialAccess hes:AML.T0052)
ClassAssertion(hes:Techniques hes:AML.T0052)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0052 hes:AML.A0009)

# Individual: hes:AML.T0052.000 (hes:AML.T0052.000)

AnnotationAssertion(hes:Description hes:AML.T0052.000 "Adversaries may turn LLMs into targeted social engineers. LLMs are capable of interacting with users via text conversations. They can be instructed by an adversary to seek sensitive information from a user and act as effective social engineers. They can be targeted towards particular personas defined by the adversary. This allows adversaries to scale spearphishing efforts and target individuals to reveal private information such as credentials to privileged systems.")
AnnotationAssertion(hes:Name hes:AML.T0052.000 "Spearphishing via Social Engineering LLM")
ClassAssertion(hes:InitialAccess hes:AML.T0052.000)
ClassAssertion(hes:Techniques hes:AML.T0052.000)

# Individual: hes:AML.T0053 (hes:AML.T0053)

AnnotationAssertion(hes:Description hes:AML.T0053 "Adversaries may use their access to an LLM that is part of a larger system to compromise connected plugins. LLMs are often connected to other services or resources via plugins to increase their capabilities. Plugins may include integrations with other applications, access to public or private data sources, and the ability to execute code.
This may allow adversaries to execute API calls to integrated applications or plugins, providing the adversary with increased privileges on the system. Adversaries may take advantage of connected data sources to retrieve sensitive information. They may also use an LLM integrated with a command or script interpreter to execute arbitrary instructions.")
AnnotationAssertion(hes:Name hes:AML.T0053 "LLM Plugin Compromise")
ClassAssertion(hes:Execution hes:AML.T0053)
ClassAssertion(hes:PrivilegeEscalation hes:AML.T0053)
ClassAssertion(hes:Techniques hes:AML.T0053)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0053 hes:AML.A0005)

# Individual: hes:AML.T0054 (hes:AML.T0054)

AnnotationAssertion(hes:Description hes:AML.T0054 "LLM JailbreakAn adversary may use a carefully crafted LLM Prompt Injection designed to place LLM in a state in which it will freely respond to any user input, bypassing any controls, restrictions, or guardrails placed on the LLM. Once successfully jailbroken, the LLM can be used in unintended ways by the adversary.")
AnnotationAssertion(hes:Name hes:AML.T0054 "LLM Jailbreak")
ClassAssertion(hes:DefenseEvasion hes:AML.T0054)
ClassAssertion(hes:PrivilegeEscalation hes:AML.T0054)
ClassAssertion(hes:Techniques hes:AML.T0054)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0054 hes:AML.A0006)

# Individual: hes:AML.T0055 (hes:AML.T0055)

AnnotationAssertion(hes:Description hes:AML.T0055 "Adversaries may search compromised systems to find and obtain insecurely stored credentials. These credentials can be stored and/or misplaced in many locations on a system, including plaintext files (e.g. bash history), environment variables, operating system, or application-specific repositories (e.g. Credentials in Registry), or other specialized files/artifacts (e.g. private keys).")
AnnotationAssertion(hes:Name hes:AML.T0055 "Unsecured Credentials")
ClassAssertion(hes:CredentialAccess hes:AML.T0055)
ClassAssertion(hes:Techniques hes:AML.T0055)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0055 hes:AML.A0009)

# Individual: hes:AML.T0056 (hes:AML.T0056)

AnnotationAssertion(hes:Description hes:AML.T0056 "An adversary may induce an LLM to reveal its initial instructions, or \"meta prompt.\" Discovering the meta prompt can inform the adversary about the internal workings of the system. Prompt engineering is an emerging field that requires expertise and exfiltrating the meta prompt can prompt in order to steal valuable intellectual property.")
AnnotationAssertion(hes:Name hes:AML.T0056 "LLM Meta Prompt Extraction")
ClassAssertion(hes:Discovery hes:AML.T0056)
ClassAssertion(hes:Exfiltration hes:AML.T0056)
ClassAssertion(hes:Techniques hes:AML.T0056)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0056 hes:AML.A0013)

# Individual: hes:AML.T0057 (hes:AML.T0057)

AnnotationAssertion(hes:Description hes:AML.T0057 "Adversaries may craft prompts that induce the LLM to leak sensitive information. This can include private user data or proprietary information. The leaked information may come from proprietary training data, data sources the LLM is connected to, or information from other users of the LLM.")
AnnotationAssertion(hes:Name hes:AML.T0057 "LLM Data Leakage")
ClassAssertion(hes:Exfiltration hes:AML.T0057)
ClassAssertion(hes:Techniques hes:AML.T0057)
ObjectPropertyAssertion(hes:LeadsTo hes:AML.T0057 hes:AML.A0015)

# Individual: hes:AccessControlTechniques (hes:AccessControlTechniques)

ClassAssertion(hes:AccessControlTechniques hes:AccessControlTechniques)

# Individual: hes:AccessManagement (hes:AccessManagement)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.1.AI-BasedAccessManagement> hes:AccessManagement)

# Individual: hes:AdditionalActivities (hes:AdditionalActivities)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.4.AdditionalActivities> hes:AdditionalActivities)

# Individual: hes:AdversarialSamples (hes:AdversarialSamples)

ClassAssertion(hes:AdversarialSamples hes:AdversarialSamples)

# Individual: hes:Algorithm (hes:Algorithm)

ClassAssertion(hes:Algorithm hes:Algorithm)
ClassAssertion(hes:Software hes:Algorithm)

# Individual: hes:AnamalyDetector (hes:AnamalyDetector)

ClassAssertion(hes:AnomalyDetector hes:AnamalyDetector)

# Individual: hes:ApplicationProgrammingInterfaces (hes:ApplicationProgrammingInterfaces)

ClassAssertion(hes:Software hes:ApplicationProgrammingInterfaces)

# Individual: hes:ArbitraryCodeExecution (hes:ArbitraryCodeExecution)

ClassAssertion(hes:ArbitraryCodeExecution hes:ArbitraryCodeExecution)

# Individual: hes:AuthenticationEvasion (hes:AuthenticationEvasion)

ClassAssertion(hes:AuthenticationEvasion hes:AuthenticationEvasion)

# Individual: hes:Availability (hes:Availability)

ClassAssertion(hes:CIA hes:Availability)

# Individual: hes:AzureService (hes:AzureService)

ClassAssertion(hes:Software hes:AzureService)

# Individual: hes:Backdoor (hes:Backdoor)

ClassAssertion(hes:Backdoor hes:Backdoor)

# Individual: hes:BackdoorDetector (hes:BackdoorDetector)

ClassAssertion(hes:BackdoorDetector hes:BackdoorDetector)

# Individual: hes:BotDetector (hes:BotDetector)

ClassAssertion(hes:BotDetector hes:BotDetector)

# Individual: hes:Bots (hes:Bots)

ClassAssertion(hes:Adversary hes:Bots)

# Individual: hes:Browser (hes:Browser)

ClassAssertion(hes:Software hes:Browser)

# Individual: hes:CAPEC.A0001 (hes:CAPEC.A0001)

AnnotationAssertion(hes:Description hes:CAPEC.A0001 "Abusing authentication mechanisms can provide unauthorized access or control over AI  systems.")
AnnotationAssertion(hes:Name hes:CAPEC.A0001 "Authentication Abuse")
ClassAssertion(hes:CAPECA0001 hes:CAPEC.A0001)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0001 hes:CWE-284)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0001 hes:CWE-287)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0001 hes:CWE-862)
ObjectPropertyAssertion(hes:Has hes:CAPEC.A0001 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0001 hes:Hacker)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0001 hes:AML.M0023)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0001 hes:AML.M0036)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0001 hes:Confidentiality)

# Individual: hes:CAPEC.A0002 (hes:CAPEC.A0002)

AnnotationAssertion(hes:Description hes:CAPEC.A0002 "AI systems often share resources such as GPUs and TPUs; manipulating these can lead to denial of service or data leakage.")
AnnotationAssertion(hes:Name hes:CAPEC.A0002 "Shared Resource Manipulation")
ClassAssertion(hes:CAPECA0002 hes:CAPEC.A0002)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0002 hes:CWE-1189)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0002 hes:CWE-362)
ObjectPropertyAssertion(hes:Has hes:CAPEC.A0002 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0002 hes:Hacker)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0002 hes:AML.M0039)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0002 hes:CAPEC.M0005)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0002 hes:Availability)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0002 hes:Confidentiality)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0002 hes:Integrity)

# Individual: hes:CAPEC.A0003 (hes:CAPEC.A0003)

AnnotationAssertion(hes:Description hes:CAPEC.A0003 "Misrepresenting content or identities can mislead AI systems into making incorrect decisions based on false data.")
AnnotationAssertion(hes:Name hes:CAPEC.A0003 "Identity Spoofing")
ClassAssertion(hes:CAPECA0003 hes:CAPEC.A0003)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0003 hes:CWE-287)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0003 hes:CWE-940)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0003 hes:CWE-941)
ObjectPropertyAssertion(hes:Has hes:CAPEC.A0003 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0003 hes:Competitors)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0003 hes:PhishingAgents)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0003 hes:AML.M0009)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0003 hes:OWASP.M0008)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0003 hes:Confidentiality)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0003 hes:Integrity)

# Individual: hes:CAPEC.A0004 (hes:CAPEC.A0004)

AnnotationAssertion(hes:Description hes:CAPEC.A0004 "The insertion of malicious logic into hardware (e.g., firmware, chips) can create persistent threats that are difficult to detect and mitigate, especially in complex AI systems.")
AnnotationAssertion(hes:Name hes:CAPEC.A0004 "Malicious Logic Insertion")
ClassAssertion(hes:CAPECA0004 hes:CAPEC.A0004)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0004 hes:CWE-1191)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0004 hes:CWE-1256)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0004 hes:CWE-502)
ObjectPropertyAssertion(hes:Has hes:CAPEC.A0004 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0004 hes:DisgruntledEmployee)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0004 hes:Third-PartyVendors)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0004 hes:AML.M0031)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0004 hes:AML.M0033)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0004 hes:Availability)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0004 hes:Integrity)

# Individual: hes:CAPEC.A0005 (hes:CAPEC.A0005)

AnnotationAssertion(hes:Description hes:CAPEC.A0005 "Introducing faults into hardware components to cause errors or malfunctions can be particularly devastating in systems that rely on precise calculations and data integrity, such as AI.")
AnnotationAssertion(hes:Name hes:CAPEC.A0005 "Hardware Fault Injection")
ClassAssertion(hes:CAPECA0005 hes:CAPEC.A0005)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0005 hes:CWE-1244)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0005 hes:CWE-1256)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0005 hes:CWE-1300)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0005 hes:CWE-190)
ObjectPropertyAssertion(hes:Has hes:CAPEC.A0005 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0005 hes:DisgruntledEmployee)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0005 hes:Third-PartyVendors)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0005 hes:AML.M0032)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0005 hes:AML.M0034)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0005 hes:Availability)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0005 hes:Integrity)

# Individual: hes:CAPEC.A0006 (hes:CAPEC.A0006)

AnnotationAssertion(hes:Description hes:CAPEC.A0006 "This involves an attacker intercepting or altering communications between two parties without their knowledge. This can compromise the integrity and confidentiality of data exchanges in AI systems, such as model updates or data feeds.")
AnnotationAssertion(hes:Name hes:CAPEC.A0006 "Adversary in the Middle (AiTM)")
ClassAssertion(hes:CAPECA0006 hes:CAPEC.A0006)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0006 hes:CWE-322)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0006 hes:CWE-419)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0006 hes:CWE-940)
ObjectPropertyAssertion(hes:Has hes:CAPEC.A0006 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0006 hes:Competitors)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0006 hes:PhishingAgents)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0006 hes:CAPEC.M0003)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0006 hes:CAPEC.M0004)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0006 hes:Confidentiality)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0006 hes:Integrity)

# Individual: hes:CAPEC.A0007 (hes:CAPEC.A0007)

AnnotationAssertion(hes:Description hes:CAPEC.A0007 "By manipulating the protocols used for network communication, attackers can disrupt, reroute, or degrade the communications essential for AI operations, potentially leading to faulty outputs or system failures.")
AnnotationAssertion(hes:Name hes:CAPEC.A0007 "Protocol Manipulation")
ClassAssertion(hes:CAPECA0007 hes:CAPEC.A0007)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0007 hes:CWE-924)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0007 hes:CWE-940)
ObjectPropertyAssertion(hes:Has hes:CAPEC.A0007 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0007 hes:Hacker)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0007 hes:CAPEC.A0007)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0007 hes:CAPEC.M0002)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0007 hes:Integrity)

# Individual: hes:CAPEC.A0008 (hes:CAPEC.A0008)

AnnotationAssertion(hes:Description hes:CAPEC.A0008 "Injecting malicious or misleading data into network traffic can alter AI behaviors or decisions, especially in systems that rely on real-time data feeds.")
AnnotationAssertion(hes:Name hes:CAPEC.A0008 "Traffic Injection")
ClassAssertion(hes:CAPECA0008 hes:CAPEC.A0008)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0008 hes:CWE-420)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0008 hes:CWE-940)
ObjectPropertyAssertion(hes:Has hes:CAPEC.A0008 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0008 hes:Bots)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0008 hes:CAPEC.M0002)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0008 hes:OWASP.M0034)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0008 hes:Integrity)

# Individual: hes:CAPEC.A0009 (hes:CAPEC.A0009)

AnnotationAssertion(hes:Description hes:CAPEC.A0009 "Allocating excessive resources can lead to performance degradation or denial of service, affecting AI system availability.")
AnnotationAssertion(hes:Name hes:CAPEC.A0009 "Excessive Allocation")
ClassAssertion(hes:CAPECA0009 hes:CAPEC.A0009)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0009 hes:CWE-362)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0009 hes:LLM04-ModelDenialOfService)
ObjectPropertyAssertion(hes:Has hes:CAPEC.A0009 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0009 hes:Hacker)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0009 hes:OWASP.M0011)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0009 hes:OWASP.M0013)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0009 hes:Availability)

# Individual: hes:CAPEC.A0010 (hes:CAPEC.A0010)

AnnotationAssertion(hes:Description hes:CAPEC.A0010 "Injecting unauthorized resources into a network can alter the behavior of AI systems or lead to execution of malicious code.")
AnnotationAssertion(hes:Name hes:CAPEC.A0010 "Resource Injection")
ClassAssertion(hes:CAPECA0010 hes:CAPEC.A0010)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0010 hes:CWE-1272)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0010 hes:CWE-420)
ObjectPropertyAssertion(hes:Exploits hes:CAPEC.A0010 hes:CWE-94)
ObjectPropertyAssertion(hes:Has hes:CAPEC.A0010 hes:HighRisk)
ObjectPropertyAssertion(hes:IsInitiatedBy hes:CAPEC.A0010 hes:Hacker)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0010 hes:AML.M0014)
ObjectPropertyAssertion(hes:IsMitigatedBy hes:CAPEC.A0010 hes:CAPEC.M0005)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0010 hes:Availability)
ObjectPropertyAssertion(hes:Violates hes:CAPEC.A0010 hes:Integrity)

# Individual: hes:CAPEC.M0001 (hes:CAPEC.M0001)

AnnotationAssertion(hes:Description hes:CAPEC.M0001 "Employ network segmentation to limit the impact of injected traffic on critical AI systems and isolate affected components.")
AnnotationAssertion(hes:Name hes:CAPEC.M0001 "Employ network segmentation")
ClassAssertion(hes:Mitigations hes:CAPEC.M0001)
ObjectPropertyAssertion(hes:IsIncludedIn hes:CAPEC.M0001 hes:AdditionalActivities)

# Individual: hes:CAPEC.M0002 (hes:CAPEC.M0002)

AnnotationAssertion(hes:Description hes:CAPEC.M0002 "Implement deep packet inspection (DPI) to analyze and verify the content of protocol communications.")
AnnotationAssertion(hes:Name hes:CAPEC.M0002 "Implement deep packet inspection (DPI)")
ClassAssertion(hes:Mitigations hes:CAPEC.M0002)
ObjectPropertyAssertion(hes:IsIncludedIn hes:CAPEC.M0002 hes:SoftwareSecurityEngineering)

# Individual: hes:CAPEC.M0003 (hes:CAPEC.M0003)

AnnotationAssertion(hes:Description hes:CAPEC.M0003 "Implement end-to-end encryption (e.g., TLS, HTTPS) to protect the integrity and confidentiality of communications between AI systems.")
AnnotationAssertion(hes:Name hes:CAPEC.M0003 "End-to-end encryption")
ClassAssertion(hes:Mitigations hes:CAPEC.M0003)
ObjectPropertyAssertion(hes:IsIncludedIn hes:CAPEC.M0003 hes:SoftwareSecurityEngineering)

# Individual: hes:CAPEC.M0004 (hes:CAPEC.M0004)

AnnotationAssertion(hes:Name hes:CAPEC.M0004 "Employ network segmentation to limit the impact of injected traffic on critical AI systems and isolate affected components.")
AnnotationAssertion(hes:Name hes:CAPEC.M0004 "Network segmentation")
ClassAssertion(hes:Mitigations hes:CAPEC.M0004)
ObjectPropertyAssertion(hes:IsIncludedIn hes:CAPEC.M0004 hes:AdditionalActivities)

# Individual: hes:CAPEC.M0005 (hes:CAPEC.M0005)

AnnotationAssertion(hes:Description hes:CAPEC.M0005 "Implement resource allocation limits and quotas to prevent any single process or user from consuming excessive resources.")
AnnotationAssertion(hes:Name hes:CAPEC.M0005 "Resource allocation limits")
ClassAssertion(hes:Mitigations hes:CAPEC.M0005)
ObjectPropertyAssertion(hes:IsIncludedIn hes:CAPEC.M0005 hes:SoftwareSecurityEngineering)

# Individual: hes:CWE-1188 (hes:CWE-1188)

AnnotationAssertion(hes:Description hes:CWE-1188 "AI systems may start with insecure defaults, making them more susceptible to attacks.")
AnnotationAssertion(hes:Name hes:CWE-1188 "Insecure Default Initialization of Resource")
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:CWE-1188)
ObjectPropertyAssertion(hes:Has hes:CWE-1188 hes:HighSeverity)

# Individual: hes:CWE-1189 (hes:CWE-1189)

AnnotationAssertion(hes:Description hes:CWE-1189 "AI systems often run multiple concurrent processes that can share resources on an SoC. Improper isolation can lead to interference, data leakage, and security breaches, compromising the integrity and confidentiality of AI computations.")
AnnotationAssertion(hes:Name hes:CWE-1189 "Improper Isolation of Shared Resources on System-on-a-Chip (SoC)")
ClassAssertion(hes:HardwareWeaknesses hes:CWE-1189)
ClassAssertion(hes:InfrastructureWeaknesses hes:CWE-1189)
ObjectPropertyAssertion(hes:Has hes:CWE-1189 hes:HighSeverity)

# Individual: hes:CWE-1191 (hes:CWE-1191)

AnnotationAssertion(hes:Description hes:CWE-1191 "Debug and test interfaces, if not properly secured, can provide attackers with a means to extract sensitive data, modify configurations, or inject malicious code. This can severely impact the trustworthiness and security of AI systems.")
AnnotationAssertion(hes:Name hes:CWE-1191 "On-Chip Debug and Test Interface With Improper Access Control")
ClassAssertion(hes:HardwareWeaknesses hes:CWE-1191)
ClassAssertion(hes:InfrastructureWeaknesses hes:CWE-1191)
ObjectPropertyAssertion(hes:Has hes:CWE-1191 hes:HighSeverity)

# Individual: hes:CWE-1236 (hes:CWE-1236)

AnnotationAssertion(hes:Description hes:CWE-1236 "AI systems may be vulnerable to injection attacks if inputs are not properly neutralized.")
AnnotationAssertion(hes:Name hes:CWE-1236 "Improper Neutralization of Special Elements")
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:CWE-1236)
ObjectPropertyAssertion(hes:Has hes:CWE-1236 hes:CriticalSeverity)

# Individual: hes:CWE-1244 (hes:CWE-1244)

AnnotationAssertion(hes:Description hes:CWE-1244 "Exposure of internal assets to unsafe debug access levels can lead to unauthorized access and manipulation of AI algorithms and data, potentially resulting in compromised model integrity and data privacy.")
AnnotationAssertion(hes:Name hes:CWE-1244 "Internal Asset Exposed to Unsafe Debug Access Level or State")
ClassAssertion(hes:HardwareWeaknesses hes:CWE-1244)
ClassAssertion(hes:InfrastructureWeaknesses hes:CWE-1244)
ObjectPropertyAssertion(hes:Has hes:CWE-1244 hes:HighSeverity)

# Individual: hes:CWE-125 (hes:CWE-125)

AnnotationAssertion(hes:Description hes:CWE-125 "This can lead to the exposure of sensitive information or model parameters, which can be exploited to reverse engineer or manipulate AI models.")
AnnotationAssertion(hes:Name hes:CWE-125 "Out-of-bounds Read")
ClassAssertion(hes:DataWeaknesses hes:CWE-125)
ClassAssertion(hes:InfrastructureWeaknesses hes:CWE-125)
ClassAssertion(hes:Out-of-boundsRead hes:CWE-125)
ObjectPropertyAssertion(hes:Has hes:CWE-125 hes:CriticalSeverity)

# Individual: hes:CWE-1256 (hes:CWE-1256)

AnnotationAssertion(hes:Description hes:CWE-1256 "AI systems often rely on specific hardware features for performance optimization. Improper restriction of software interfaces can lead to unauthorized usage or exploitation of these features, affecting system stability and security.")
AnnotationAssertion(hes:Name hes:CWE-1256 "Improper Restriction of Software Interfaces to Hardware Features")
ClassAssertion(hes:HardwareWeaknesses hes:CWE-1256)
ClassAssertion(hes:InfrastructureWeaknesses hes:CWE-1256)
ObjectPropertyAssertion(hes:Has hes:CWE-1256 hes:HighSeverity)

# Individual: hes:CWE-1260 (hes:CWE-1260)

AnnotationAssertion(hes:Description hes:CWE-1260 "Improper Handling of Overlap Between Protected Memory Ranges")
ClassAssertion(hes:HardwareWeaknesses hes:CWE-1260)
ClassAssertion(hes:InfrastructureWeaknesses hes:CWE-1260)
ObjectPropertyAssertion(hes:Has hes:CWE-1260 hes:HighSeverity)

# Individual: hes:CWE-1272 (hes:CWE-1272)

AnnotationAssertion(hes:Description hes:CWE-1272 "AI systems often handle sensitive data, and failure to clear this information before transitions can lead to residual data being accessed by unauthorized entities, compromising confidentiality.")
AnnotationAssertion(hes:Name hes:CWE-1272 "Sensitive Information Uncleared Before Debug/Power State Transition")
ClassAssertion(hes:HardwareWeaknesses hes:CWE-1272)
ObjectPropertyAssertion(hes:Has hes:CWE-1272 hes:HighSeverity)

# Individual: hes:CWE-1274 (hes:CWE-1274)

AnnotationAssertion(hes:Description hes:CWE-1274 "The boot code initializes the system and ensures the integrity of the AI environment. Improper access control can allow tampering with the boot process, leading to persistent security vulnerabilities.")
AnnotationAssertion(hes:Name hes:CWE-1274 "Improper Access Control for Volatile Memory Containing Boot Code")
ClassAssertion(hes:HardwareWeaknesses hes:CWE-1274)
ClassAssertion(hes:InfrastructureWeaknesses hes:CWE-1274)
ObjectPropertyAssertion(hes:Has hes:CWE-1274 hes:CriticalSeverity)

# Individual: hes:CWE-1300 (hes:CWE-1300)

AnnotationAssertion(hes:Description hes:CWE-1300 "AI systems are vulnerable to side-channel attacks, where attackers exploit physical characteristics (e.g., power consumption, electromagnetic emissions) to extract sensitive information. Proper protection is essential to mitigate these risks.")
AnnotationAssertion(hes:Name hes:CWE-1300 "Improper Protection of Physical Side Channels")
ClassAssertion(hes:HardwareWeaknesses hes:CWE-1300)
ClassAssertion(hes:InfrastructureWeaknesses hes:CWE-1300)
ObjectPropertyAssertion(hes:Has hes:CWE-1300 hes:MediumSeverity)

# Individual: hes:CWE-1327 (hes:CWE-1327)

AnnotationAssertion(hes:Description hes:CWE-1327 "Binding services to unrestricted IP addresses can expose AI systems to unauthorized access and attacks. Proper IP address restrictions are necessary to limit access to trusted sources only.")
AnnotationAssertion(hes:Name hes:CWE-1327 "Binding to an Unrestricted IP Address")
ClassAssertion(hes:NetworkWeaknesses hes:CWE-1327)
ObjectPropertyAssertion(hes:Has hes:CWE-1327 hes:HighSeverity)

# Individual: hes:CWE-16 (hes:CWE-16)

AnnotationAssertion(hes:Description hes:CWE-16 "Users may be tricked into misconfiguring systems, impacting the security of AI deployments.")
AnnotationAssertion(hes:Name hes:CWE-16 "Misconfiguration")
ClassAssertion(hes:Human-centricFactors hes:CWE-16)
ObjectPropertyAssertion(hes:Has hes:CWE-16 hes:MediumSeverity)

# Individual: hes:CWE-190 (hes:CWE-190)

AnnotationAssertion(hes:Description hes:CWE-190 "Integer overflows can be exploited in the context of machine learning algorithms, particularly in buffer allocations and data indexing, potentially leading to crashes or erroneous results.")
AnnotationAssertion(hes:Name hes:CWE-190 "Integer Overflow or Wraparound")
ClassAssertion(hes:IntegerOverflow hes:CWE-190)
ClassAssertion(hes:ModelWeaknesses hes:CWE-190)
ObjectPropertyAssertion(hes:Has hes:CWE-190 hes:CriticalSeverity)

# Individual: hes:CWE-20 (hes:CWE-20)

AnnotationAssertion(hes:Description hes:CWE-20 "AI models rely heavily on input data. Improper input validation can lead to data poisoning attacks, where the model is trained or tested on malicious or incorrect data, affecting its accuracy and reliability.")
AnnotationAssertion(hes:Name hes:CWE-20 "Improper Input Validation")
ClassAssertion(hes:DataWeaknesses hes:CWE-20)
ClassAssertion(hes:ImproperInputValidation hes:CWE-20)
ObjectPropertyAssertion(hes:Has hes:CWE-20 hes:HighSeverity)

# Individual: hes:CWE-200 (hes:CWE-200)

AnnotationAssertion(hes:Description hes:CWE-200 "Employees may inadvertently disclose sensitive information, leading to data breaches or AI model exposure.")
AnnotationAssertion(hes:Name hes:CWE-200 "Exposure of Sensitive Information to an Unauthorized Actor")
ClassAssertion(hes:Human-centricFactors hes:CWE-200)
ObjectPropertyAssertion(hes:Has hes:CWE-200 hes:CriticalSeverity)

# Individual: hes:CWE-264 (hes:CWE-264)

AnnotationAssertion(hes:Description hes:CWE-264 "Manipulation can lead to the granting of excessive permissions, jeopardizing AI system integrity.")
AnnotationAssertion(hes:Name hes:CWE-264 "Permissions, Privileges, and Access Controls")
ClassAssertion(hes:Human-centricFactors hes:CWE-264)
ObjectPropertyAssertion(hes:Has hes:CWE-264 hes:MediumSeverity)

# Individual: hes:CWE-269 (hes:CWE-269)

AnnotationAssertion(hes:Description hes:CWE-269 "AI systems may be exposed to unauthorized actions or data manipulation if users have excessive or insufficient privileges.")
AnnotationAssertion(hes:Name hes:CWE-269 "Improper Privilege Management")
ClassAssertion(hes:ImproperPrivilegeManagement hes:CWE-269)
ClassAssertion(hes:ModelWeaknesses hes:CWE-269)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:CWE-269)
ObjectPropertyAssertion(hes:Has hes:CWE-269 hes:HighSeverity)

# Individual: hes:CWE-284 (hes:CWE-284)

AnnotationAssertion(hes:Description hes:CWE-284 "Poorly controlled access can be exploited to gain unauthorized insights into AI systems.")
AnnotationAssertion(hes:Name hes:CWE-284 "Improper Access Control")
ClassAssertion(hes:Human-centricFactors hes:CWE-284)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:CWE-284)
ObjectPropertyAssertion(hes:Has hes:CWE-284 hes:HighSeverity)

# Individual: hes:CWE-285 (hes:CWE-285)

AnnotationAssertion(hes:Description hes:CWE-285 "AI systems may perform unauthorized operations, leading to security breaches or unintended consequences.")
AnnotationAssertion(hes:Name hes:CWE-285 "Improper Authorization")
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:CWE-285)
ObjectPropertyAssertion(hes:Has hes:CWE-285 hes:CriticalSeverity)

# Individual: hes:CWE-287 (hes:CWE-287)

AnnotationAssertion(hes:Description hes:CWE-287 "AI systems may be susceptible to unauthorized access if authentication mechanisms are not properly implemented or enforced. AI systems often require authentication to access models or data. Improper authentication can allow unauthorized access, leading to model theft or tampering.")
AnnotationAssertion(hes:Name hes:CWE-287 "Improper Authentication")
ClassAssertion(hes:ImproperAuthentication hes:CWE-287)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:CWE-287)
ObjectPropertyAssertion(hes:Has hes:CWE-287 hes:CriticalSeverity)

# Individual: hes:CWE-306 (hes:CWE-306)

AnnotationAssertion(hes:Description hes:CWE-306 "Critical functions in AI systems, such as model training or configuration changes, should have proper authentication to prevent unauthorized access or manipulation.")
AnnotationAssertion(hes:Name hes:CWE-306 "Missing Authentication for Critical Function")
ClassAssertion(hes:MissingAuthentication hes:CWE-306)
ClassAssertion(hes:ModelWeaknesses hes:CWE-306)
ObjectPropertyAssertion(hes:Has hes:CWE-306 hes:CriticalSeverity)

# Individual: hes:CWE-307 (hes:CWE-307)

AnnotationAssertion(hes:Description hes:CWE-307 "AI systems can be vulnerable to brute-force attacks if there are no limits on authentication attempts.")
AnnotationAssertion(hes:Name hes:CWE-307 "Improper Restriction of Excessive Authentication Attempts")
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:CWE-307)
ObjectPropertyAssertion(hes:Has hes:CWE-307 hes:CriticalSeverity)

# Individual: hes:CWE-322 (hes:CWE-322)

AnnotationAssertion(hes:Description hes:CWE-322 "In AI systems, secure key exchange is critical for ensuring the confidentiality and integrity of communications. Without proper entity authentication, keys can be intercepted or manipulated, leading to unauthorized access and potential data breaches.")
AnnotationAssertion(hes:Name hes:CWE-322 "Key Exchange without Entity Authentication")
ClassAssertion(hes:NetworkWeaknesses hes:CWE-322)
ObjectPropertyAssertion(hes:Has hes:CWE-322 hes:CriticalSeverity)

# Individual: hes:CWE-353 (hes:CWE-353)

AnnotationAssertion(hes:Description hes:CWE-353 "Incorrectly trusted software sources can lead to security breaches in AI operations.")
AnnotationAssertion(hes:Name hes:CWE-353 "Use of Provenance Information in Security Decision Making")
ClassAssertion(hes:Human-centricFactors hes:CWE-353)
ObjectPropertyAssertion(hes:Has hes:CWE-353 hes:HighSeverity)

# Individual: hes:CWE-362 (hes:CWE-362)

AnnotationAssertion(hes:Description hes:CWE-362 "AI systems, especially those that are distributed or parallelized, can be vulnerable to race conditions, leading to inconsistent or erroneous results.")
AnnotationAssertion(hes:Name hes:CWE-362 "Concurrent Execution using Shared Resource with Improper Synchronization ('Race Condition')")
ClassAssertion(hes:ConcurrentExecution hes:CWE-362)
ClassAssertion(hes:InfrastructureWeaknesses hes:CWE-362)
ClassAssertion(hes:ModelWeaknesses hes:CWE-362)
ObjectPropertyAssertion(hes:Has hes:CWE-362 hes:HighSeverity)

# Individual: hes:CWE-419 (hes:CWE-419)

AnnotationAssertion(hes:Description hes:CWE-419 "AI systems often rely on secure primary communication channels to transmit sensitive data. If these channels are not adequately protected, they can be vulnerable to interception and tampering, compromising data integrity and privacy.")
AnnotationAssertion(hes:Name hes:CWE-419 "Unprotected Primary Channel")
ClassAssertion(hes:NetworkWeaknesses hes:CWE-419)
ObjectPropertyAssertion(hes:Has hes:CWE-419 hes:CriticalSeverity)

# Individual: hes:CWE-420 (hes:CWE-420)

AnnotationAssertion(hes:Description hes:CWE-420 "AI systems may use alternate channels for backup or additional communication. These channels must also be protected to prevent unauthorized access and data leakage.")
AnnotationAssertion(hes:Name hes:CWE-420 "Unprotected Alternate Channel")
ClassAssertion(hes:NetworkWeaknesses hes:CWE-420)
ObjectPropertyAssertion(hes:Has hes:CWE-420 hes:CriticalSeverity)

# Individual: hes:CWE-494 (hes:CWE-494)

AnnotationAssertion(hes:Description hes:CWE-494 "Compromised software can introduce vulnerabilities into AI systems.")
AnnotationAssertion(hes:Name hes:CWE-494 "Download of Code Without Integrity Check")
ClassAssertion(hes:Human-centricFactors hes:CWE-494)
ObjectPropertyAssertion(hes:Has hes:CWE-494 hes:CriticalSeverity)

# Individual: hes:CWE-502 (hes:CWE-502)

AnnotationAssertion(hes:Description hes:CWE-502 "Many AI frameworks and tools use serialization for model storage and transfer. Deserializing untrusted data can lead to execution of malicious code or corruption of model data.")
AnnotationAssertion(hes:Name hes:CWE-502 "Deserialization of Untrusted Data")
ClassAssertion(hes:DataWeaknesses hes:CWE-502)
ClassAssertion(hes:DeserializationOfUntrustedData hes:CWE-502)
ObjectPropertyAssertion(hes:Has hes:CWE-502 hes:CriticalSeverity)

# Individual: hes:CWE-532 (hes:CWE-532)

AnnotationAssertion(hes:Description hes:CWE-532 "Sensitive information may be exposed through log files, leading to data leaks and privacy issues.")
AnnotationAssertion(hes:Name hes:CWE-532 "Insertion of Sensitive Information into Log Files")
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:CWE-532)
ObjectPropertyAssertion(hes:Has hes:CWE-532 hes:HighSeverity)

# Individual: hes:CWE-611 (hes:CWE-611)

AnnotationAssertion(hes:Description hes:CWE-611 "AI systems can be compromised by XXE attacks if XML data is not properly restricted.")
AnnotationAssertion(hes:Name hes:CWE-611 "Improper Restriction of XML External Entity Reference ('XXE')")
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:CWE-611)
ObjectPropertyAssertion(hes:Has hes:CWE-611 hes:HighSeverity)

# Individual: hes:CWE-693 (hes:CWE-693)

AnnotationAssertion(hes:Description hes:CWE-693 "AI systems may become vulnerable to unauthorized access, data breaches, and exploitation if protection mechanisms fail or are not adequately enforced.")
AnnotationAssertion(hes:Name hes:CWE-693 "Protection Mechanism Failure")
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:CWE-693)
ObjectPropertyAssertion(hes:Has hes:CWE-693 hes:HighSeverity)

# Individual: hes:CWE-778 (hes:CWE-778)

AnnotationAssertion(hes:Description hes:CWE-778 "Insufficient logging can hinder the detection and investigation of security incidents in AI systems.")
AnnotationAssertion(hes:Name hes:CWE-778 "Insufficient Logging")
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:CWE-778)
ObjectPropertyAssertion(hes:Has hes:CWE-778 hes:MediumSeverity)

# Individual: hes:CWE-787 (hes:CWE-787)

AnnotationAssertion(hes:Description hes:CWE-787 "This weakness can lead to corruption of memory that can impact the integrity of AI models, leading to crashes or incorrect behavior.")
AnnotationAssertion(hes:Name hes:CWE-787 "Out-of-bounds Write")
ClassAssertion(hes:DataWeaknesses hes:CWE-787)
ClassAssertion(hes:InfrastructureWeaknesses hes:CWE-787)
ClassAssertion(hes:Out-of-boundsWrite hes:CWE-787)
ObjectPropertyAssertion(hes:Has hes:CWE-787 hes:HighSeverity)

# Individual: hes:CWE-798 (hes:CWE-798)

AnnotationAssertion(hes:Description hes:CWE-798 "Hard-coded credentials in AI systems can lead to unauthorized access to AI models, training data, or other sensitive components.")
AnnotationAssertion(hes:Name hes:CWE-798 "Use of Hard-coded Credentials")
ClassAssertion(hes:UseOfHard-codedCredentials hes:CWE-798)
ObjectPropertyAssertion(hes:Has hes:CWE-798 hes:CriticalSeverity)

# Individual: hes:CWE-862 (hes:CWE-862)

AnnotationAssertion(hes:Description hes:CWE-862 "Missing authorization checks can allow unauthorized users to access or modify AI models, leading to model theft or tampering.")
AnnotationAssertion(hes:Name hes:CWE-862 "Missing Authorization")
ClassAssertion(hes:MissingAuthorization hes:CWE-862)
ObjectPropertyAssertion(hes:Has hes:CWE-862 hes:CriticalSeverity)

# Individual: hes:CWE-863 (hes:CWE-863)

AnnotationAssertion(hes:Description hes:CWE-863 "Incorrectly implemented authorization can lead to privilege escalation or unauthorized access to sensitive components of AI systems.")
AnnotationAssertion(hes:Name hes:CWE-863 "IncorrectAuthorization")
ClassAssertion(hes:DataWeaknesses hes:CWE-863)
ClassAssertion(hes:IncorrectAuthorization hes:CWE-863)
ObjectPropertyAssertion(hes:Has hes:CWE-863 hes:CriticalSeverity)

# Individual: hes:CWE-89 (hes:CWE-89)

AnnotationAssertion(hes:Description hes:CWE-89 "AI systems often interface with databases for training data or predictions. SQL injection can compromise the integrity of these databases, leading to manipulated training data or unauthorized access.")
AnnotationAssertion(hes:Name hes:CWE-89 "Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')")
ClassAssertion(hes:ImproperNeutralization hes:CWE-89)
ObjectPropertyAssertion(hes:Has hes:CWE-89 hes:CriticalSeverity)

# Individual: hes:CWE-916 (hes:CWE-916)

AnnotationAssertion(hes:Description hes:CWE-916 "AI systems may be vulnerable to password cracking attacks if password hashes are not sufficiently secure.")
AnnotationAssertion(hes:Name hes:CWE-916 "Use of Password Hash with Insufficient Computational Effort")
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Policies&Procedures> hes:CWE-916)
ObjectPropertyAssertion(hes:Has hes:CWE-916 hes:HighSeverity)

# Individual: hes:CWE-918 (hes:CWE-918)

AnnotationAssertion(hes:Description hes:CWE-918 "AI systems that make external requests as part of their processing can be manipulated to send requests to unintended or malicious destinations, potentially compromising data integrity or privacy.")
AnnotationAssertion(hes:Name hes:CWE-918 "Server-Side Request Forgery (SSRF)")
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Server-SideRequestForgery(SSRF)> hes:CWE-918)
ObjectPropertyAssertion(hes:Has hes:CWE-918 hes:CriticalSeverity)

# Individual: hes:CWE-924 (hes:CWE-924)

AnnotationAssertion(hes:Description hes:CWE-924 "Ensuring message integrity is crucial for AI systems to prevent tampering and ensure the authenticity of transmitted data. Weaknesses in this area can lead to data corruption and manipulation.")
AnnotationAssertion(hes:Name hes:CWE-924 "Improper Enforcement of Message Integrity During Transmission in a Communication Channel")
ClassAssertion(hes:NetworkWeaknesses hes:CWE-924)
ObjectPropertyAssertion(hes:Has hes:CWE-924 hes:CriticalSeverity)

# Individual: hes:CWE-94 (hes:CWE-94)

AnnotationAssertion(hes:Description hes:CWE-94 "Code injection can be used to manipulate the execution of AI algorithms, potentially altering the behavior of AI models or exposing sensitive information.")
AnnotationAssertion(hes:Name hes:CWE-94 "Improper Control of Generation of Code ('Code Injection')")
ClassAssertion(hes:ImproperControl hes:CWE-94)
ClassAssertion(hes:ModelWeaknesses hes:CWE-94)
ObjectPropertyAssertion(hes:Has hes:CWE-94 hes:HighSeverity)

# Individual: hes:CWE-940 (hes:CWE-940)

AnnotationAssertion(hes:Description hes:CWE-940 "Verifying the source of communication is essential to prevent spoofing attacks where an attacker impersonates a trusted entity. This is particularly critical for AI systems that rely on accurate data and commands.")
AnnotationAssertion(hes:Name hes:CWE-940 "Improper Verification of Source of a Communication Channel")
ClassAssertion(hes:NetworkWeaknesses hes:CWE-940)
ObjectPropertyAssertion(hes:Has hes:CWE-940 hes:CriticalSeverity)

# Individual: hes:CWE-941 (hes:CWE-941)

AnnotationAssertion(hes:Description hes:CWE-941 "AI systems must ensure that data is sent to the correct destination. Incorrectly specified destinations can lead to data loss, exposure of sensitive information, and potential security breaches.")
AnnotationAssertion(hes:Name hes:CWE-941 "Incorrectly Specified Destination in a Communication Channel")
ClassAssertion(hes:NetworkWeaknesses hes:CWE-941)
ObjectPropertyAssertion(hes:Has hes:CWE-941 hes:CriticalSeverity)

# Individual: hes:ChatSystem (hes:ChatSystem)

ClassAssertion(hes:Model hes:ChatSystem)

# Individual: hes:Chatbot (hes:Chatbot)

ClassAssertion(hes:Software hes:Chatbot)

# Individual: hes:CloudComputingResources (hes:CloudComputingResources)

ClassAssertion(hes:Resource hes:CloudComputingResources)

# Individual: hes:CloudPlatforms (hes:CloudPlatforms)

ClassAssertion(hes:Platform hes:CloudPlatforms)

# Individual: hes:Colab (hes:Colab)

ClassAssertion(hes:Software hes:Colab)

# Individual: hes:CollaborativePlatforms (hes:CollaborativePlatforms)

ClassAssertion(hes:Platform hes:CollaborativePlatforms)
ClassAssertion(hes:Software hes:CollaborativePlatforms)

# Individual: hes:Competitors (hes:Competitors)

ClassAssertion(hes:Adversary hes:Competitors)

# Individual: hes:Compromise (hes:Compromise)

ClassAssertion(hes:CompromisedDuringDevelopment hes:Compromise)

# Individual: hes:Confidentiality (hes:Confidentiality)

ClassAssertion(hes:CIA hes:Confidentiality)

# Individual: hes:CraftedPrompts (hes:CraftedPrompts)

ClassAssertion(hes:CraftedPrompts hes:CraftedPrompts)

# Individual: hes:CryptographicTechniques (hes:CryptographicTechniques)

ClassAssertion(hes:CryptographicTechniques hes:CryptographicTechniques)

# Individual: hes:DataPreparation (hes:DataPreparation)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#2.DataPreparation> hes:DataPreparation)

# Individual: hes:DataRepositories (hes:DataRepositories)

ClassAssertion(hes:Software hes:DataRepositories)

# Individual: hes:DeepLearningDetector (hes:DeepLearningDetector)

ClassAssertion(hes:DeepLearningDetector hes:DeepLearningDetector)

# Individual: hes:DeepLearningModels (hes:DeepLearningModels)

ClassAssertion(hes:LLM hes:DeepLearningModels)

# Individual: hes:Deployment (hes:Deployment)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#5.Deployment> hes:Deployment)

# Individual: hes:DevelopmentPlatforms (hes:DevelopmentPlatforms)

ClassAssertion(hes:Platform hes:DevelopmentPlatforms)

# Individual: hes:DigitalData (hes:DigitalData)

ClassAssertion(hes:Data hes:DigitalData)

# Individual: hes:DisgruntledEmployee (hes:DisgruntledEmployee)

ClassAssertion(hes:Adversary hes:DisgruntledEmployee)

# Individual: hes:EmailProtectionSystem (hes:EmailProtectionSystem)

ClassAssertion(hes:Software hes:EmailProtectionSystem)

# Individual: hes:EmbeddedSecurity (hes:EmbeddedSecurity)

ClassAssertion(hes:EmbeddedSecurity hes:EmbeddedSecurity)

# Individual: hes:FaceIdentificationSystem (hes:FaceIdentificationSystem)

ClassAssertion(hes:Software hes:FaceIdentificationSystem)

# Individual: hes:FacialRecognitionSystem (hes:FacialRecognitionSystem)

ClassAssertion(hes:Software hes:FacialRecognitionSystem)

# Individual: hes:FacialRecognitionSystems (hes:FacialRecognitionSystems)

ClassAssertion(hes:Software hes:FacialRecognitionSystems)

# Individual: hes:FaultyInputs (hes:FaultyInputs)

ClassAssertion(hes:FaultyInputs hes:FaultyInputs)

# Individual: hes:Gateways (hes:Gateways)

ClassAssertion(hes:Network hes:Gateways)

# Individual: hes:GoogleColab (hes:GoogleColab)

ClassAssertion(hes:Software hes:GoogleColab)

# Individual: hes:GraphicalProcessingUnits (hes:GraphicalProcessingUnits)

ClassAssertion(hes:Hardware hes:GraphicalProcessingUnits)

# Individual: hes:Hacker (hes:Hacker)

ClassAssertion(hes:Adversary hes:Hacker)

# Individual: hes:HardwareInformation (hes:HardwareInformation)

ClassAssertion(hes:Hardware hes:HardwareInformation)
ClassAssertion(hes:Information hes:HardwareInformation)

# Individual: hes:HardwareSecurity (hes:HardwareSecurity)

ClassAssertion(hes:HardwareSecurity hes:HardwareSecurity)

# Individual: hes:HardwareSecurityEngineering (hes:HardwareSecurityEngineering)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.3.AI-BasedHardwareSecurityEngineering> hes:HardwareSecurityEngineering)

# Individual: hes:High-PerformanceCPUs (hes:High-PerformanceCPUs)

ClassAssertion(hes:Hardware hes:High-PerformanceCPUs)

# Individual: hes:High-SpeedSSDs (hes:High-SpeedSSDs)

ClassAssertion(hes:Hardware hes:High-SpeedSSDs)

# Individual: hes:HighRisk (hes:HighRisk)

AnnotationAssertion(hes:Description hes:HighRisk "Adding a watermark to the model’s code and training data can make it possible to trace the source of a theft and hold the attacker accountable.")
AnnotationAssertion(hes:Name hes:HighRisk "Watermarking")

# Individual: hes:IdentityVerificationSystem (hes:IdentityVerificationSystem)

ClassAssertion(hes:Software hes:IdentityVerificationSystem)

# Individual: hes:IdentityVerificationSystem. (hes:IdentityVerificationSystem.)

ClassAssertion(hes:Software hes:IdentityVerificationSystem.)

# Individual: hes:Impact (hes:Impact)

ClassAssertion(hes:Impact hes:Impact)

# Individual: hes:Integrity (hes:Integrity)

ClassAssertion(hes:CIA hes:Integrity)

# Individual: hes:IntrusionDetectionSystem (hes:IntrusionDetectionSystem)

ClassAssertion(hes:IntrusionDetector hes:IntrusionDetectionSystem)

# Individual: hes:IntrusionPreventionSystem (hes:IntrusionPreventionSystem)

ClassAssertion(hes:IntrusionPrevention hes:IntrusionPreventionSystem)

# Individual: hes:IoTPlatforms (hes:IoTPlatforms)

ClassAssertion(hes:Platform hes:IoTPlatforms)

# Individual: hes:LLM01-PromptInjectionVulnerabiliy (hes:LLM01-PromptInjectionVulnerabiliy)

AnnotationAssertion(hes:AttackScenarios hes:LLM01-PromptInjectionVulnerabiliy "Chatbot Romote Execution: Injection leads to unauthorized access via chatbot.
Email Deletion: Indirect injection causes email deletion.
Exfiltration via Image: Webpage prompts exfiltrate private data.
Misloading Resume: LLM incorrectly endorses a candidate.
Prompt Replay: Attacker replays system prompts for potential further attacks.")
AnnotationAssertion(hes:Description hes:LLM01-PromptInjectionVulnerabiliy "Attackers can manipulate LLMs through crafted inputs, causing it to execute the
attacker's intentions. This can be done directly by adversarially prompting the
system prompt or indirectly through manipulated external inputs, potentially
leading to data exfiltration, social engineering, and other issues.")
AnnotationAssertion(hes:Examples hes:LLM01-PromptInjectionVulnerabiliy "Direct Prompt Injection: Malicious user injects prompts to extract
sensitive information.
Indiroct Prompt Injoction: Users request sensitive data via webpage
prompts.
Scam Through Plugins: Websites exploit plugins for scams.")
ClassAssertion(hes:DataWeaknesses hes:LLM01-PromptInjectionVulnerabiliy)
ClassAssertion(hes:ModelWeaknesses hes:LLM01-PromptInjectionVulnerabiliy)
ClassAssertion(hes:PromptInjection hes:LLM01-PromptInjectionVulnerabiliy)
ObjectPropertyAssertion(hes:Has hes:LLM01-PromptInjectionVulnerabiliy hes:HighSeverity)

# Individual: hes:LLM02-InsecureOutputHandling (hes:LLM02-InsecureOutputHandling)

AnnotationAssertion(hes:AttackScenarios hes:LLM02-InsecureOutputHandling "Chatbot Shutdoìn: LLM output shuts down a plugin due to a lack of
validation.
Sensdtdve Data Capture: LLM captures and sends sensitive data to an
attacker-controlled server.
Database Table Deletdon: LLM crafts a destructive SQL Ùuery, potentially
deleting all tables.
XSS Explodtatdon: LLM returns unsanitized JavaScript payload, leading to
XSS on the victim's browser.")
AnnotationAssertion(hes:Description hes:LLM02-InsecureOutputHandling "Insecure Output Handling is a vulnerability
that arises when a downstream component
blindly accepts large language model (LLM)
output without proper scrutiny. This can
lead to XSS and CSRF in web browsers as
well as SSRF, privilege escalation, or remote
code execution on backend systems.")
AnnotationAssertion(hes:Examples hes:LLM02-InsecureOutputHandling "Remote Code Executdon: LLM output executed in system shell,
leading to code execution.
Cross-Sdte Scrdptdng (XSS): LLM-generated JavaScript or Markdown
causes browser interpretation.")
ClassAssertion(hes:DataWeaknesses hes:LLM02-InsecureOutputHandling)
ClassAssertion(hes:InsecureObjectHandling hes:LLM02-InsecureOutputHandling)
ClassAssertion(hes:ModelWeaknesses hes:LLM02-InsecureOutputHandling)
ObjectPropertyAssertion(hes:Has hes:LLM02-InsecureOutputHandling hes:CriticalSeverity)

# Individual: hes:LLM03-TrainingDataPoisoning (hes:LLM03-TrainingDataPoisoning)

AnnotationAssertion(hes:AttackScenarios hes:LLM03-TrainingDataPoisoning "Misleading Outputs: LLM generates content that promotes bias or hate.
Toxic Data Injection: Malicious users manipulate the model with biased
data.
Malicious Document Injection: Competitors insert false data during
model training.")
AnnotationAssertion(hes:Description hes:LLM03-TrainingDataPoisoning "Training Data Poisoning refers to
manipulating the data or fine-tuning process
to introduce vulnerabilities, backdoors or
biases that could compromise the model’s
security, effectiveness or ethical behavior.
This risks performance degradation,
downstream software exploitation and
reputational damage.")
AnnotationAssertion(hes:Examples hes:LLM03-TrainingDataPoisoning "Malicious Data Injection: Injecting falsified data during model
training,
Biased Training Outputs: Model reflects inaccuracies from tainted
data.
Content Injection: Malicious actors inject biased content into
training.")
ClassAssertion(hes:DataWeaknesses hes:LLM03-TrainingDataPoisoning)
ClassAssertion(hes:ModelWeaknesses hes:LLM03-TrainingDataPoisoning)
ClassAssertion(hes:TrainingDataPoisioning hes:LLM03-TrainingDataPoisoning)
ObjectPropertyAssertion(hes:Has hes:LLM03-TrainingDataPoisoning hes:CriticalSeverity)

# Individual: hes:LLM04-ModelDenialOfService (hes:LLM04-ModelDenialOfService)

AnnotationAssertion(hes:AttackScenarios hes:LLM04-ModelDenialOfService "Resource Overuse: Attacker overloads a hosted model, impacting other
users.
Webpage Request Amplification: LLM tool consumes excessive
resources due to unexpected content.
Input Flood: Overwhelm LLM with excessive input, causing slowdown.
Sequential Input Drain: Attacker exhausts context window with sequential
inputs.")
AnnotationAssertion(hes:Description hes:LLM04-ModelDenialOfService "Model Denial of Service occurs when an
attacker interacts with a Large Language
Model (LLM) in a way that consumes an
exceptionally high amount of resources.
This can result in a decline in the quality of
service for them and other users, as well as
potentially incurring high resource costs.")
AnnotationAssertion(hes:Examples hes:LLM04-ModelDenialOfService "High-volume Queuing: Attackers overload LLM with resourceintensive tasks.
REsource-Consuming Queries: Unusual queries strain system
resources.
Continuous Input OvErflow: Flooding LLM with excessive input.
Repetitive Long Inputs: Repeated long queries exhaust resources.
Recursive Context Expansion: Attackers exploit recursive behavior.")
ClassAssertion(hes:ModelDenialOfService hes:LLM04-ModelDenialOfService)
ClassAssertion(hes:ModelWeaknesses hes:LLM04-ModelDenialOfService)
ObjectPropertyAssertion(hes:Has hes:LLM04-ModelDenialOfService hes:HighSeverity)

# Individual: hes:LLM05-SupplyChainVulnerabilitieç (hes:LLM05-SupplyChainVulnerabilitieç)

AnnotationAssertion(hes:AttackScenarios hes:LLM05-SupplyChainVulnerabilitieç "Library Exploitation: Exploiting vulnerable Python libraries.
Scamming Plugin: Deploying a plugin for scams.
Package Registry Attack: Tricking developers with a compromised
package.
Misinformation Backdoor: Poisoning models for fake news.
Data Poisoning: Poisoning datasets during fine-tuning.")
AnnotationAssertion(hes:Description hes:LLM05-SupplyChainVulnerabilitieç "Supply chain vulnerabilities in LLMs can
compromise training data, ML models, and
deployment platforms, causing biased
results, security breaches, or total system
failures. Such vulnerabilities can stem from
outdated software, susceptible pre-trained
models, poisoned training data, and
insecure plugin designs")
AnnotationAssertion(hes:Examples hes:LLM05-SupplyChainVulnerabilitieç "Package Nulnerabilities: Using outdated components.
Nulnerable Models: Risky pre-trained models for fine-tuning.
Poisoned Data: Tainted crowd-sourced data.
Outdated Models: Using unmaintained models.
Unclear Terms: Data misuse due to unclear terms.")
ClassAssertion(hes:ModelWeaknesses hes:LLM05-SupplyChainVulnerabilitieç)
ClassAssertion(hes:SupplyChainVulnerabilities hes:LLM05-SupplyChainVulnerabilitieç)
ObjectPropertyAssertion(hes:Has hes:LLM05-SupplyChainVulnerabilitieç hes:HighSeverity)

# Individual: hes:LLM06-SensitiveInformationDisclosure (hes:LLM06-SensitiveInformationDisclosure)

AnnotationAssertion(hes:AttackScenarios hes:LLM06-SensitiveInformationDisclosure "Unintentional Exposure: User A exposed to other user data.
Filter Bypass: User A extracts PII by bypassing filters.
Training Data Leak: Personal data leaks during training.")
AnnotationAssertion(hes:Description hes:LLM06-SensitiveInformationDisclosure "LLM applications can inadvertently disclose
sensitive information, proprietary
algorithms, or confidential data, leading to
unauthorized access, intellectual property
theft, and privacy breaches. To mitigate
these risks, LLM applications should
employ data sanitization, implement
appropriate usage policies, and restrict the
types of data returned by the LLM.")
AnnotationAssertion(hes:Examples hes:LLM06-SensitiveInformationDisclosure "Incomplete Filtering: LLM responses may contain sensitive data.
Overfitting: LLMs memorize sensitive data during training.
Unintended Disclosure: 6ata leaks due to misinterpretation or lack of
scrubbing.")
ClassAssertion(hes:DataWeaknesses hes:LLM06-SensitiveInformationDisclosure)
ClassAssertion(hes:ModelWeaknesses hes:LLM06-SensitiveInformationDisclosure)
ClassAssertion(hes:SensitiveInformationDisclosure hes:LLM06-SensitiveInformationDisclosure)
ObjectPropertyAssertion(hes:Has hes:LLM06-SensitiveInformationDisclosure hes:CriticalSeverity)

# Individual: hes:LLM07-InsecurePluginDesign (hes:LLM07-InsecurePluginDesign)

AnnotationAssertion(hes:AttackScenarios hes:LLM07-InsecurePluginDesign "URL Manipulation: Attackers inject content via manipulated URLs.
Reconnaissance and Exploitation: Exploiting lack of validation for code
execution and data theft.
Unauthorized Access: Accessing unauthorized data through parameter
manipulation.
Repository Takeover: Exploiting insecure code management plugin for
repository takeover")
AnnotationAssertion(hes:Description hes:LLM07-InsecurePluginDesign "Plugins can be prone to malicious requests
leading to harmful consequences like data
exfiltration, remote code execution, and
privilege escalation due to insufficient
access controls and improper input
validation. Developers must follow robust
security measures to prevent exploitation,
like strict parameterized inputs and secure
access control guidelines.")
AnnotationAssertion(hes:Examples hes:LLM07-InsecurePluginDesign "Single Field Parameters: Plugins lack parameter separation.
Configuration Strings: Configurations can override settings.
Authentication Issues: Lack of specific plugin authorization.
Raw SQL or Code: Unsafe acceptance of code or SQL.")
ClassAssertion(hes:AlgorithmWeaknesses hes:LLM07-InsecurePluginDesign)
ClassAssertion(hes:InsecurePluginDesign hes:LLM07-InsecurePluginDesign)
ClassAssertion(hes:ModelWeaknesses hes:LLM07-InsecurePluginDesign)
ObjectPropertyAssertion(hes:Has hes:LLM07-InsecurePluginDesign hes:CriticalSeverity)

# Individual: hes:LLM08-ExcessiveAgency (hes:LLM08-ExcessiveAgency)

AnnotationAssertion(hes:AttackScenarios hes:LLM08-ExcessiveAgency "An LLM-based personal assistant app with excessive permissions and
autonomy is tricked by a malicious email into sending spam. This could be
prevented by limiting functionality, permissions, requiring user approval, or
implementing rate limiting.")
AnnotationAssertion(hes:Description hes:LLM08-ExcessiveAgency "Excessive Agency in LLM-based systems is
a vulnerability caused by over-functionality,
excessive permissions, or too much
autonomy. To prevent this, developers need
to limit plugin functionality, permissions,
and autonomy to what's absolutely
necessary, track user authorization, require
human approval for all actions, and
implement authorization in downstream
systems.")
AnnotationAssertion(hes:Examples hes:LLM08-ExcessiveAgency "Excessive Functionality: LLM agents have unnecessary functions, risking
misuse.
Excessive Permissions: Plugins may have excessive access to systems.
Excessive Autonomy: LLMs lack human verification for high-impact
actions.")
ClassAssertion(hes:AlgorithmWeaknesses hes:LLM08-ExcessiveAgency)
ClassAssertion(hes:ExcessiveAgency hes:LLM08-ExcessiveAgency)
ClassAssertion(hes:ModelWeaknesses hes:LLM08-ExcessiveAgency)
ObjectPropertyAssertion(hes:Has hes:LLM08-ExcessiveAgency hes:CriticalSeverity)

# Individual: hes:LLM09-Overreliance (hes:LLM09-Overreliance)

AnnotationAssertion(hes:AttackScenarios hes:LLM09-Overreliance "Disinfo Spread: Malicious actors exploit LLM-reliant news organizations.
Plagiarism: Unintentional plagiarism leads to copyright issues.
Insecure Software: LLM suggestions introduce security vulnerabilities.
Malicious Package: LLM suggests a non-existent code library.")
AnnotationAssertion(hes:Description hes:LLM09-Overreliance "Overreliance on LLMs can lead to serious
consequences such as misinformation,
legal issues, and security vulnerabilities.

It occurs when an LLM is trusted to make
critical decisions or generate content
without adequate oversight or validation.")
AnnotationAssertion(hes:Examples hes:LLM09-Overreliance "Misleading Info: LLMs can provide misleading info without validation.
Insecure Code: LLMs may suggest insecure code in software")
ClassAssertion(hes:AlgorithmWeaknesses hes:LLM09-Overreliance)
ClassAssertion(hes:ModelWeaknesses hes:LLM09-Overreliance)
ClassAssertion(hes:Overreliance hes:LLM09-Overreliance)
ObjectPropertyAssertion(hes:Has hes:LLM09-Overreliance hes:CriticalSeverity)

# Individual: hes:LLM10-ModelTheft (hes:LLM10-ModelTheft)

AnnotationAssertion(hes:AttackScenarios hes:LLM10-ModelTheft "Model Theft: Unauthorized access and use for competition.
Employee Leak: Exposure increases risks.
Shadow Model Creation: Replicating models with queries.
Side-Channel Attack: Extraction through side techniques.")
AnnotationAssertion(hes:Description hes:LLM10-ModelTheft "LLM model theft involves unauthorized
access to and exfiltration of LLM models,
risking economic loss, reputation damage,
and unauthorized access to sensitive data.
Robust security measures are essential to
protect these models.")
AnnotationAssertion(hes:Examples hes:LLM10-ModelTheft "Vulnerability Exploitation: Unauthorized access due to security flaws.
Central Model Registry: Centralized security for governance.
Insider Threat: Risk of employee model leaks.
Side-Channel Attack: Extraction of model details through side
techniques.")
ClassAssertion(hes:ModelTheft hes:LLM10-ModelTheft)
ClassAssertion(hes:ModelWeaknesses hes:LLM10-ModelTheft)
ObjectPropertyAssertion(hes:Has hes:LLM10-ModelTheft hes:CriticalSeverity)

# Individual: hes:Large-CapacityHDDs (hes:Large-CapacityHDDs)

ClassAssertion(hes:Hardware hes:Large-CapacityHDDs)

# Individual: hes:Library (hes:Library)

ClassAssertion(hes:Library hes:Library)

# Individual: hes:Likelihood (hes:Likelihood)

ClassAssertion(hes:Likelihood hes:Likelihood)

# Individual: hes:LoadBalancers (hes:LoadBalancers)

ClassAssertion(hes:Network hes:LoadBalancers)

# Individual: hes:MachineTranslationServices (hes:MachineTranslationServices)

ClassAssertion(hes:Software hes:MachineTranslationServices)

# Individual: hes:MalaciousLogic (hes:MalaciousLogic)

ClassAssertion(hes:MaliciousLogic hes:MalaciousLogic)

# Individual: hes:MaliciousData (hes:MaliciousData)

ClassAssertion(hes:MaliciousData hes:MaliciousData)

# Individual: hes:MalwareDetector (hes:MalwareDetector)

ClassAssertion(hes:MalwareDetector hes:MalwareDetector)
ClassAssertion(hes:Software hes:MalwareDetector)

# Individual: hes:MicrosoftService (hes:MicrosoftService)

ClassAssertion(hes:Software hes:MicrosoftService)

# Individual: hes:MobileApp (hes:MobileApp)

ClassAssertion(hes:Software hes:MobileApp)

# Individual: hes:MobileApplications (hes:MobileApplications)

ClassAssertion(hes:Software hes:MobileApplications)

# Individual: hes:Model (hes:Model)

ClassAssertion(hes:Model hes:Model)
ClassAssertion(hes:Software hes:Model)

# Individual: hes:ModelEngineering (hes:ModelEngineering)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#3.ModelEngineering> hes:ModelEngineering)

# Individual: hes:ModelEvaluation (hes:ModelEvaluation)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#4.ModelEvaluation> hes:ModelEvaluation)

# Individual: hes:ModelSourcecode (hes:ModelSourcecode)

ClassAssertion(hes:Sourcecode hes:ModelSourcecode)

# Individual: hes:NetworkSecurity (hes:NetworkSecurity)

ClassAssertion(hes:NetworkSecurity hes:NetworkSecurity)

# Individual: hes:NeuralNetworks (hes:NeuralNetworks)

ClassAssertion(hes:Software hes:NeuralNetworks)

# Individual: hes:NeuromorphicChips (hes:NeuromorphicChips)

ClassAssertion(hes:Hardware hes:NeuromorphicChips)

# Individual: hes:OWASP.M0001 (hes:OWASP.M0001)

AnnotationAssertion(hes:Description hes:OWASP.M0001 "Limit LLM access and apply role-based permissions.")
AnnotationAssertion(hes:Name hes:OWASP.M0001 "Privilege Control")
ClassAssertion(hes:Mitigations hes:OWASP.M0001)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0001 hes:Deployment)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0001 hes:ModelEvaluation)

# Individual: hes:OWASP.M0002 (hes:OWASP.M0002)

AnnotationAssertion(hes:Description hes:OWASP.M0002 "Require user consent for privileged actions.")
AnnotationAssertion(hes:Name hes:OWASP.M0002 "Human Approval")
ClassAssertion(hes:Mitigations hes:OWASP.M0002)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0002 hes:AccessManagement)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0002 hes:Deployment)

# Individual: hes:OWASP.M0003 (hes:OWASP.M0003)

AnnotationAssertion(hes:Description hes:OWASP.M0003 "Separate untrusted content from user prompts.")
AnnotationAssertion(hes:Name hes:OWASP.M0003 "Segregate Content")
ClassAssertion(hes:Mitigations hes:OWASP.M0003)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0003 hes:ModelEvaluation)

# Individual: hes:OWASP.M0004 (hes:OWASP.M0004)

AnnotationAssertion(hes:Description hes:OWASP.M0004 "Treat LLM as untrusted and visually highlight unreliable responses.")
AnnotationAssertion(hes:Name hes:OWASP.M0004 "Trust Boundaries")
ClassAssertion(hes:Mitigations hes:OWASP.M0004)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0004 hes:Deployment)

# Individual: hes:OWASP.M0005 (hes:OWASP.M0005)

AnnotationAssertion(hes:Description hes:OWASP.M0005 "Treat LLM output like user input validate and sanitize it properly.")
AnnotationAssertion(hes:Name hes:OWASP.M0005 "Zero-Trust Approach")
ClassAssertion(hes:Mitigations hes:OWASP.M0005)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0005 hes:ModelEvaluation)

# Individual: hes:OWASP.M0006 (hes:OWASP.M0006)

AnnotationAssertion(hes:Description hes:OWASP.M0006 "Encode LLM output to prevent code execution in JavaScript or Markdown.")
AnnotationAssertion(hes:Name hes:OWASP.M0006 "Output Encoding")
ClassAssertion(hes:Mitigations hes:OWASP.M0006)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0006 hes:ModelEvaluation)

# Individual: hes:OWASP.M0007 (hes:OWASP.M0007)

AnnotationAssertion(hes:Description hes:OWASP.M0007 "Verify external data sources and maintain \"ML-BOM\" records.")
AnnotationAssertion(hes:Name hes:OWASP.M0007 "Supply Chain Verification")
ClassAssertion(hes:Mitigations hes:OWASP.M0007)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0007 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Business&DataUnderstanding>)

# Individual: hes:OWASP.M0008 (hes:OWASP.M0008)

AnnotationAssertion(hes:Description hes:OWASP.M0008 "Ensure data legitimacy throughout training stages.")
AnnotationAssertion(hes:Name hes:OWASP.M0008 "Legitimacy Verification")
ClassAssertion(hes:Mitigations hes:OWASP.M0008)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0008 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Business&DataUnderstanding>)

# Individual: hes:OWASP.M0009 (hes:OWASP.M0009)

AnnotationAssertion(hes:Description hes:OWASP.M0009 "Create separate models for different usecases.")
AnnotationAssertion(hes:Name hes:OWASP.M0009 "Use-Case Specific Training")
ClassAssertion(hes:Mitigations hes:OWASP.M0009)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0009 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Business&DataUnderstanding>)

# Individual: hes:OWASP.M0010 (hes:OWASP.M0010)

AnnotationAssertion(hes:Description hes:OWASP.M0010 "Implement input validation and content filtering")
AnnotationAssertion(hes:Name hes:OWASP.M0010 "Input validation")
ClassAssertion(hes:Mitigations hes:OWASP.M0010)

# Individual: hes:OWASP.M0011 (hes:OWASP.M0011)

AnnotationAssertion(hes:Description hes:OWASP.M0011 "Limit resource use per request.")
AnnotationAssertion(hes:Name hes:OWASP.M0011 "Resource Caps")
ClassAssertion(hes:Mitigations hes:OWASP.M0011)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0011 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:OWASP.M0012 (hes:OWASP.M0012)

AnnotationAssertion(hes:Description hes:OWASP.M0012 "Enforce rate limits for users or IP addresses.")
AnnotationAssertion(hes:Name hes:OWASP.M0012 "API Rate Limits")
ClassAssertion(hes:Mitigations hes:OWASP.M0012)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0012 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:OWASP.M0013 (hes:OWASP.M0013)

AnnotationAssertion(hes:Description hes:OWASP.M0013 "Control queued and total actions.")
AnnotationAssertion(hes:Name hes:OWASP.M0013 "Queue Management")
ClassAssertion(hes:Mitigations hes:OWASP.M0013)

# Individual: hes:OWASP.M0014 (hes:OWASP.M0014)

AnnotationAssertion(hes:Description hes:OWASP.M0014 "Continuously monitor resource usage.")
AnnotationAssertion(hes:Name hes:OWASP.M0014 "Resource Monitoring")
ClassAssertion(hes:Mitigations hes:OWASP.M0014)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0014 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:OWASP.M0015 (hes:OWASP.M0015)

AnnotationAssertion(hes:Description hes:OWASP.M0015 "Use tested, trusted plugins.")
AnnotationAssertion(hes:Name hes:OWASP.M0015 "Plugin Testing")
ClassAssertion(hes:Mitigations hes:OWASP.M0015)

# Individual: hes:OWASP.M0016 (hes:OWASP.M0016)

AnnotationAssertion(hes:Description hes:OWASP.M0016 "Maintain an up-to-date inventory")
AnnotationAssertion(hes:Name hes:OWASP.M0016 "Inventory Management")
ClassAssertion(hes:Mitigations hes:OWASP.M0016)

# Individual: hes:OWASP.M0017 (hes:OWASP.M0017)

AnnotationAssertion(hes:Description hes:OWASP.M0017 "Sign models and code, apply anomaly detection, and monitor.")
AnnotationAssertion(hes:Name hes:OWASP.M0017 "Anomaly detection")
ClassAssertion(hes:Mitigations hes:OWASP.M0017)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0017 hes:Deployment)

# Individual: hes:OWASP.M0018 (hes:OWASP.M0018)

AnnotationAssertion(hes:Description hes:OWASP.M0018 "Use scrubbing to prevent user data in training.")
AnnotationAssertion(hes:Name hes:OWASP.M0018 "Data Sanitization")
ClassAssertion(hes:Mitigations hes:OWASP.M0018)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0018 hes:DataPreparation)

# Individual: hes:OWASP.M0019 (hes:OWASP.M0019)

AnnotationAssertion(hes:Description hes:OWASP.M0019 "Filter malicious inputs to avoid model poisoning.")
AnnotationAssertion(hes:Name hes:OWASP.M0019 "Input Validation")
ClassAssertion(hes:Mitigations hes:OWASP.M0019)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0019 hes:DataPreparation)

# Individual: hes:OWASP.M0020 (hes:OWASP.M0020)

AnnotationAssertion(hes:Description hes:OWASP.M0020 "Be careful with sensitive data in model fine-tuning.")
AnnotationAssertion(hes:Name hes:OWASP.M0020 "Fine-Tuning Caution")
ClassAssertion(hes:Mitigations hes:OWASP.M0020)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0020 hes:ModelEngineering)

# Individual: hes:OWASP.M0021 (hes:OWASP.M0021)

AnnotationAssertion(hes:Description hes:OWASP.M0021 "Limit external data source access.")
AnnotationAssertion(hes:Name hes:OWASP.M0021 "Data Access Control")
ClassAssertion(hes:Mitigations hes:OWASP.M0021)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0021 hes:SoftwareSecurityEngineering)

# Individual: hes:OWASP.M0022 (hes:OWASP.M0022)

AnnotationAssertion(hes:Description hes:OWASP.M0022 "Require manual authorization for sensitive actions.")
AnnotationAssertion(hes:Name hes:OWASP.M0022 "User Confirmation")
ClassAssertion(hes:Mitigations hes:OWASP.M0022)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0022 hes:AccessManagement)

# Individual: hes:OWASP.M0023 (hes:OWASP.M0023)

AnnotationAssertion(hes:Description hes:OWASP.M0023 "Use OAuth2 and API Keys for custom authorization.")
AnnotationAssertion(hes:Name hes:OWASP.M0023 "Authentication Identities")
ClassAssertion(hes:Mitigations hes:OWASP.M0023)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0023 hes:AccessManagement)

# Individual: hes:OWASP.M0024 (hes:OWASP.M0024)

AnnotationAssertion(hes:Description hes:OWASP.M0024 "Allow only essential functions for LLM agents.")
AnnotationAssertion(hes:Name hes:OWASP.M0024 "Limit Plugin Functions")
ClassAssertion(hes:Mitigations hes:OWASP.M0024)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0024 hes:AccessManagement)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0024 hes:ModelEngineering)

# Individual: hes:OWASP.M0025 (hes:OWASP.M0025)

AnnotationAssertion(hes:Description hes:OWASP.M0025 "Restrict functions within LLM plugins.")
AnnotationAssertion(hes:Name hes:OWASP.M0025 "Plugin Scope Control")
ClassAssertion(hes:Mitigations hes:OWASP.M0025)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0025 hes:Deployment)

# Individual: hes:OWASP.M0026 (hes:OWASP.M0026)

AnnotationAssertion(hes:Description hes:OWASP.M0026 "Avoid open-ended functions; use specific plugins.")
AnnotationAssertion(hes:Name hes:OWASP.M0026 "Granular Functionality")
ClassAssertion(hes:Mitigations hes:OWASP.M0026)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0026 hes:ModelEngineering)

# Individual: hes:OWASP.M0027 (hes:OWASP.M0027)

AnnotationAssertion(hes:Description hes:OWASP.M0027 "Limit permissions to the minimum required.")
AnnotationAssertion(hes:Name hes:OWASP.M0027 "Permissions Control")
ClassAssertion(hes:Mitigations hes:OWASP.M0027)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0027 hes:Deployment)

# Individual: hes:OWASP.M0028 (hes:OWASP.M0028)

AnnotationAssertion(hes:Description hes:OWASP.M0028 "Require human approval for actions.")
AnnotationAssertion(hes:Name hes:OWASP.M0028 "human-in-the-Loop")
ClassAssertion(hes:Mitigations hes:OWASP.M0028)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0028 hes:ModelEvaluation)

# Individual: hes:OWASP.M0029 (hes:OWASP.M0029)

AnnotationAssertion(hes:Description hes:OWASP.M0029 "Implement authorization in downstream systems.")
AnnotationAssertion(hes:Name hes:OWASP.M0029 "Downstream Authorization")
ClassAssertion(hes:Mitigations hes:OWASP.M0029)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0029 hes:AccessManagement)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0029 hes:ModelEngineering)

# Individual: hes:OWASP.M0030 (hes:OWASP.M0030)

AnnotationAssertion(hes:Description hes:OWASP.M0030 "Regularly review LLM outputs with consistency
checks")
AnnotationAssertion(hes:Name hes:OWASP.M0030 "Review Model Output")
ClassAssertion(hes:Mitigations hes:OWASP.M0030)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0030 hes:ModelEvaluation)

# Individual: hes:OWASP.M0031 (hes:OWASP.M0031)

AnnotationAssertion(hes:Description hes:OWASP.M0031 "Verify LLM output with trusted sources.")
AnnotationAssertion(hes:Name hes:OWASP.M0031 "Cross-Check")
ClassAssertion(hes:Mitigations hes:OWASP.M0031)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0031 hes:ModelEvaluation)

# Individual: hes:OWASP.M0032 (hes:OWASP.M0032)

AnnotationAssertion(hes:Description hes:OWASP.M0032 "Establish guidelines to prevent vulnerabilities.")
AnnotationAssertion(hes:Name hes:OWASP.M0032 "Secure Coding")
ClassAssertion(hes:Mitigations hes:OWASP.M0032)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0032 hes:ModelEvaluation)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0032 hes:SoftwareSecurityEngineering)

# Individual: hes:OWASP.M0033 (hes:OWASP.M0033)

AnnotationAssertion(hes:Description hes:OWASP.M0033 "Limit LLM access to resources and APIs.")
AnnotationAssertion(hes:Name hes:OWASP.M0033 "Network Restrictions")
ClassAssertion(hes:Mitigations hes:OWASP.M0033)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0033 hes:Deployment)

# Individual: hes:OWASP.M0034 (hes:OWASP.M0034)

AnnotationAssertion(hes:Description hes:OWASP.M0034 "Regular monitoring of access logs.")
AnnotationAssertion(hes:Name hes:OWASP.M0034 "Monitoring of access logs")
ClassAssertion(hes:Mitigations hes:OWASP.M0034)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0034 <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: hes:OWASP.M0035 (hes:OWASP.M0035)

AnnotationAssertion(hes:Description hes:OWASP.M0035 "Secure deployment with approval workflows.")
AnnotationAssertion(hes:Name hes:OWASP.M0035 "Secure Deployment of Models")
ClassAssertion(hes:Mitigations hes:OWASP.M0035)
ObjectPropertyAssertion(hes:IsIncludedIn hes:OWASP.M0035 hes:Deployment)

# Individual: hes:Packages (hes:Packages)

ClassAssertion(hes:Software hes:Packages)

# Individual: hes:PhishingAgents (hes:PhishingAgents)

ClassAssertion(hes:Adversary hes:PhishingAgents)

# Individual: hes:Plugin (hes:Plugin)

ClassAssertion(hes:Software hes:Plugin)

# Individual: hes:Plugins (hes:Plugins)

ClassAssertion(hes:Plugins hes:Plugins)

# Individual: hes:ProcessingPlatforms (hes:ProcessingPlatforms)

ClassAssertion(hes:Platform hes:ProcessingPlatforms)

# Individual: hes:ProcessingTools (hes:ProcessingTools)

ClassAssertion(hes:Software hes:ProcessingTools)

# Individual: hes:Protocol (hes:Protocol)

ClassAssertion(hes:Protocol hes:Protocol)

# Individual: hes:Pytorch (hes:Pytorch)

ClassAssertion(hes:Library hes:Pytorch)

# Individual: hes:QuantumProcessors (hes:QuantumProcessors)

ClassAssertion(hes:Hardware hes:QuantumProcessors)

# Individual: hes:Repository (hes:Repository)

ClassAssertion(hes:SourceFile hes:Repository)

# Individual: hes:ResearchPlatforms (hes:ResearchPlatforms)

ClassAssertion(hes:Platform hes:ResearchPlatforms)

# Individual: hes:RootkitHunter (hes:RootkitHunter)

ClassAssertion(hes:RootkitHunter hes:RootkitHunter)

# Individual: hes:Routers (hes:Routers)

ClassAssertion(hes:Network hes:Routers)

# Individual: hes:SensitiveInformation (hes:SensitiveInformation)

ClassAssertion(hes:Information hes:SensitiveInformation)

# Individual: hes:SharedResource (hes:SharedResource)

ClassAssertion(hes:Resource hes:SharedResource)

# Individual: hes:SoftwareFile (hes:SoftwareFile)

ClassAssertion(hes:Software hes:SoftwareFile)

# Individual: hes:SoftwareSecurityEngineering (hes:SoftwareSecurityEngineering)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#7.2.AI-BasedSoftwareSecurityEngineering> hes:SoftwareSecurityEngineering)

# Individual: hes:SpywarScanner (hes:SpywarScanner)

ClassAssertion(hes:SpywareScanner hes:SpywarScanner)

# Individual: hes:SystemCode (hes:SystemCode)

ClassAssertion(hes:Software hes:SystemCode)

# Individual: hes:SystemFile (hes:SystemFile)

ClassAssertion(hes:SourceFile hes:SystemFile)

# Individual: hes:TensorProcessingUnits (hes:TensorProcessingUnits)

ClassAssertion(hes:Hardware hes:TensorProcessingUnits)

# Individual: hes:Third-PartyVendors (hes:Third-PartyVendors)

ClassAssertion(hes:Adversary hes:Third-PartyVendors)

# Individual: hes:TrainingData (hes:TrainingData)

ClassAssertion(hes:Data hes:TrainingData)

# Individual: hes:Translator (hes:Translator)

ClassAssertion(hes:Software hes:Translator)

# Individual: hes:UserInterfaces (hes:UserInterfaces)

ClassAssertion(hes:Software hes:UserInterfaces)

# Individual: hes:ValidationData (hes:ValidationData)

ClassAssertion(hes:Data hes:ValidationData)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#5GNetworks> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#5GNetworks>)

ClassAssertion(hes:Network <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#5GNetworks>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AdjacentNetwork(A)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AdjacentNetwork(A)>)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackVector(AV)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AdjacentNetwork(A)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Business&DataUnderstanding> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Business&DataUnderstanding>)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#1.Business&DataUnderstanding> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Business&DataUnderstanding>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Changed(C)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Changed(C)>)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Scope(S)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Changed(C)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#ContentDeliveryNetworks(CDNs)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#ContentDeliveryNetworks(CDNs)>)

ClassAssertion(hes:Software <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#ContentDeliveryNetworks(CDNs)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#FieldProgrammableGateArrays(FPGAs)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#FieldProgrammableGateArrays(FPGAs)>)

ClassAssertion(hes:Hardware <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#FieldProgrammableGateArrays(FPGAs)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#GenerativePre-trainedTransformer(GPT)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#GenerativePre-trainedTransformer(GPT)>)

ClassAssertion(hes:LLM <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#GenerativePre-trainedTransformer(GPT)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High(H)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High(H)>)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackComplexity(AC)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High(H)>)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Availability(A)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High(H)>)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Confidentiality(C)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High(H)>)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Integrity(I)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High(H)>)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#PrivilegesRequired(PR)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High(H)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High.Impact*High.Likelihood=High.Risk> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High.Impact*High.Likelihood=High.Risk>)

ClassAssertion(hes:RiskAssessment <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High.Impact*High.Likelihood=High.Risk>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High.Impact*Medium.Likelihood=High.Risk> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High.Impact*Medium.Likelihood=High.Risk>)

ClassAssertion(hes:RiskAssessment <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#High.Impact*Medium.Likelihood=High.Risk>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#LargeLanguageModels(LLM)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#LargeLanguageModels(LLM)>)

ClassAssertion(hes:LLM <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#LargeLanguageModels(LLM)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Local(L)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Local(L)>)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackVector(AV)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Local(L)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low(L)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low(L)>)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackComplexity(AC)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low(L)>)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Availability(A)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low(L)>)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Confidentiality(C)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low(L)>)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Integrity(I)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low(L)>)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#PrivilegesRequired(PR)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low(L)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low.Impact*High.Likelihood=Medium.Risk> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low.Impact*High.Likelihood=Medium.Risk>)

ClassAssertion(hes:RiskAssessment <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low.Impact*High.Likelihood=Medium.Risk>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low.Impact*Low.Likelihood=Low.Risk> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low.Impact*Low.Likelihood=Low.Risk>)

ClassAssertion(hes:RiskAssessment <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low.Impact*Low.Likelihood=Low.Risk>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low.Impact*Medium.Likelihood=Medium.Risk> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low.Impact*Medium.Likelihood=Medium.Risk>)

ClassAssertion(hes:RiskAssessment <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Low.Impact*Medium.Likelihood=Medium.Risk>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Medium.Impact*High.Likelihood=High.Risk> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Medium.Impact*High.Likelihood=High.Risk>)

ClassAssertion(hes:RiskAssessment <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Medium.Impact*High.Likelihood=High.Risk>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Medium.Impact*Low.Likelihood=Medium.Risk> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Medium.Impact*Low.Likelihood=Medium.Risk>)

ClassAssertion(hes:RiskAssessment <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Medium.Impact*Low.Likelihood=Medium.Risk>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Medium.Impact*Medium.Likelihood=Medium.Risk> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Medium.Impact*Medium.Likelihood=Medium.Risk>)

ClassAssertion(hes:RiskAssessment <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Medium.Impact*Medium.Likelihood=Medium.Risk>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#6.Monitoring&Maintenance> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Monitoring&Maintenance>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Network(N)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Network(N)>)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackVector(AV)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Network(N)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#None(N)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#None(N)>)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Availability(A)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#None(N)>)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Confidentiality(C)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#None(N)>)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Integrity(I)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#None(N)>)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#PrivilegesRequired(PR)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#None(N)>)
ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#UserInteraction(UI)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#None(N)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Physical(P)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Physical(P)>)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#AttackVector(AV)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Physical(P)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Required(R)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Required(R)>)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#UserInteraction(UI)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Required(R)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Software-DefinedNetworking(SDN)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Software-DefinedNetworking(SDN)>)

ClassAssertion(hes:Software <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Software-DefinedNetworking(SDN)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Storage&ManagementPlatforms> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Storage&ManagementPlatforms>)

ClassAssertion(hes:Platform <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Storage&ManagementPlatforms>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#SystemOnAChip(SoC)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#SystemOnAChip(SoC)>)

ClassAssertion(hes:Hardware <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#SystemOnAChip(SoC)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Unchanged(U)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Unchanged(U)>)

ClassAssertion(<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Scope(S)> <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#Unchanged(U)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#VirtualPrivateNetworks(VPNs)> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#VirtualPrivateNetworks(VPNs)>)

ClassAssertion(hes:Software <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#VirtualPrivateNetworks(VPNs)>)

# Individual: <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#high.Impact*Low.Likelihood=Medium.Risk> (<http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#high.Impact*Low.Likelihood=Medium.Risk>)

ClassAssertion(hes:RiskAssessment <http://www.semanticweb.org/ubaid/ontologies/2023/5/HES#high.Impact*Low.Likelihood=Medium.Risk>)


DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a model or network traffic is manipulated to evade detection by an adversary, then the model or network is threatened by Detection Evasion.") Annotation(rdfs:label "Model to Detection Evasion") Body(ClassAtom(hes:AMLA0001 Variable(:T)) ObjectPropertyAtom(hes:IsManipulatedBy Variable(:M) Variable(:A)) ClassAtom(hes:Adversary Variable(:A)) ClassAtom(hes:Model Variable(:M)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:M) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If an algorithm exploits weaknesses to bypass detection, then this algorithm is threatened by bypassing detection.") Annotation(rdfs:label "Algorithm to Bypassing Detection.") Body(ClassAtom(hes:Algorithm Variable(:Al)) ObjectPropertyAtom(hes:IsExploitedBy Variable(:Al) Variable(:A)) ClassAtom(hes:AMLA0002 Variable(:T)) ClassAtom(hes:Adversary Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:Al) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a platform’s resources are hijacked, then this platform  IsThreatenedBy resource hijacking.") Annotation(rdfs:label "Platform’s resources to Resource Hijacking.") Body(ClassAtom(hes:Resource Variable(:R)) ObjectPropertyAtom(hes:IsHijakedBy Variable(:R) Variable(:A)) ClassAtom(hes:AMLA0003 Variable(:T)) ClassAtom(hes:Adversary Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:R) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a model is replicated for replication attack, then this model  Is threatened by model replication attack.") Annotation(rdfs:label "Model to Replication Attack") Body(ClassAtom(hes:Model Variable(:M)) ObjectPropertyAtom(hes:IsReplicatedBy Variable(:M) Variable(:A)) ClassAtom(hes:AMLA0004 Variable(:T)) ClassAtom(hes:Adversary Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:M) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a source file is misconfigured, then this source file  IsThreatenedBy misconfiguration of source file/repository.") Annotation(rdfs:label "Source File to Misconfiguration of source file/repository.") Body(ClassAtom(hes:SourceFile Variable(:S)) ObjectPropertyAtom(hes:IsMisconfigured Variable(:S) Variable(:C)) ClassAtom(hes:AMLA0005 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:S) Variable(:A))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a chatbot is poisoned with malicious input, then this chatbot  IsThreatenedBy chatbot poisoning.") Annotation(rdfs:label "ChatBot to Chatbot Poisoning") Body(ClassAtom(hes:ChatBot Variable(:C)) ObjectPropertyAtom(hes:IsPoisionedBy Variable(:M) Variable(:A)) ClassAtom(hes:AMLA0006 Variable(:T)) ClassAtom(hes:Adversary Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:C) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a system is under a DoS attack by an adversary, then the system is threatened by Denial of Service Attack.") Annotation(rdfs:label "System to Denial of Service Attack.") Body(ClassAtom(hes:System Variable(:S)) ObjectPropertyAtom(hes:IsUnder Variable(:S) Variable(:A)) ClassAtom(hes:AMLA0007 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:S) Variable(:A))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a Gpt model has poisoned training data, then this model  Is threatened by poisoning.") Annotation(rdfs:label "GPT Model to GPT Poisioning") Body(ClassAtom(hes:Model Variable(:M)) ObjectPropertyAtom(hes:IsPoisionedBy Variable(:M) Variable(:A)) ClassAtom(hes:AMLA0008 Variable(:T)) ClassAtom(hes:Adversary Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:M) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If an adversary injects malicious data into network traffic, then the network traffic is threatened by Traffic Injection.") Annotation(rdfs:label "Network traffic to Traffic Injection.") Body(ClassAtom(hes:Network Variable(:D)) ObjectPropertyAtom(hes:IsInsertedInto Variable(:M) Variable(:D)) ClassAtom(hes:MaliciousData Variable(:M)) ClassAtom(hes:AMLA0008 Variable(:T)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:D) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If AI system is evaded by an adversary using authentication evasion techniques, then the system is threatened by Authentication Evasion.") Annotation(rdfs:label "AI System to Authentication Evasion.") Body(ClassAtom(hes:System Variable(:S)) ObjectPropertyAtom(hes:IsEvadedBy Variable(:S) Variable(:C)) ClassAtom(hes:AMLA0009 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:S) Variable(:A))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If AI system contains a backdoor and detected by a backdoor detector, then the system is threatened by Backdoor Attack.") Annotation(rdfs:label "AI System to Backdoor Attack.") Body(ClassAtom(hes:System Variable(:S)) ClassAtom(hes:Backdoor Variable(:B)) ClassAtom(hes:BackdoorDetector Variable(:BD)) ObjectPropertyAtom(hes:IsDetectedBy Variable(:B) Variable(:BD)) ClassAtom(hes:AMLA0010 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:S) Variable(:A))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If an library is compromised during development, then this library  IsThreatenedBy supply chain poisoning.") Annotation(rdfs:label "Library to Supply Chain Poisoning.") Body(ClassAtom(hes:Library Variable(:L)) ClassAtom(hes:CompromisedDuringDevelopment Variable(:C)) ClassAtom(hes:AMLA0011 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:L) Variable(:A))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a system is bypassed by an adversary using authentication bypass techniques, then the system is threatened by Bypassing Authentication.") Annotation(rdfs:label "AI-enabled System to Authentication Bypassing.") Body(ClassAtom(hes:System Variable(:S)) ObjectPropertyAtom(hes:IsBypassedBy Variable(:S) Variable(:Ad)) ClassAtom(hes:Adversary Variable(:Ad)) ClassAtom(hes:AMLA0012 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:S) Variable(:A))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If an AI system is injected with crafted prompts by an adversary, then the system is threatened by Prompt Injection.") Annotation(rdfs:label "AI system to Prompt Injection.") Body(ClassAtom(hes:System Variable(:S)) ObjectPropertyAtom(hes:IsInjectedWith Variable(:S) Variable(:C)) ClassAtom(hes:CraftedPrompts Variable(:C)) ClassAtom(hes:AMLA0013 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:S) Variable(:A))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If AI system is vulnerable to arbitrary code execution, then the system is threatened by Arbitrary Code Execution.") Annotation(rdfs:label "AI System to Arbitrary Code Execution.") Body(ClassAtom(hes:System Variable(:S)) ObjectPropertyAtom(hes:IsVulnerableTo Variable(:S) Variable(:C)) ClassAtom(hes:AMLA0014 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:S) Variable(:A))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If data is exposed by an adversary, then the data is threatened by Privacy Leak.") Annotation(rdfs:label "Data to Privacy Leak") Body(ClassAtom(hes:Data Variable(:d)) ClassAtom(hes:Adversary Variable(:A)) ObjectPropertyAtom(hes:ExposedBy Variable(:d) Variable(:A)) ClassAtom(hes:AMLA0015 Variable(:T)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:d) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a hardware’s Information is deduced from alternative streams by an adversary, then the hardware is threatened by Side Channel Attack.") Annotation(rdfs:label "Hardware' Information to Side Channel Attack.") Body(ClassAtom(hes:Information Variable(:S)) ObjectPropertyAtom(hes:IsDeducedBy Variable(:S) Variable(:A)) ClassAtom(hes:Adversary Variable(:A)) ClassAtom(hes:AMLA0016 Variable(:T)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:S) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a hardware is disrupted by faulty inputs, then the hardware is threatened  by Fault Injection Attack.") Annotation(rdfs:label "Hardware to Fault Injection Attack.") Body(ClassAtom(hes:Hardware Variable(:H)) ClassAtom(hes:FaultyInputs Variable(:F)) ObjectPropertyAtom(hes:IsDisruptedBy Variable(:H) Variable(:F)) ClassAtom(hes:AMLA0017 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:H) Variable(:A))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a Malicious backdoor is inserted into hardware by an adversary, then the hardware is threatened by Hardware Trojan Attack.") Annotation(rdfs:label "Hardware to Hardware Trojan Attack.") Body(ClassAtom(hes:Hardware Variable(:H)) ClassAtom(hes:Backdoor Variable(:F)) ObjectPropertyAtom(hes:IsInsertedInto Variable(:F) Variable(:H)) ClassAtom(hes:AMLA0018 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:H) Variable(:A))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a model’s code is accessed by an adversary, then the model is threatened by Model Theft.") Annotation(rdfs:label "Sourcecode to Model Theft.") Body(ClassAtom(hes:Sourcecode Variable(:S)) ObjectPropertyAtom(hes:IsAccessedBy Variable(:S) Variable(:A)) ClassAtom(hes:Adversary Variable(:A)) ClassAtom(hes:AMLA0019 Variable(:T)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:S) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a Model is reverse engineers by an adversary, then the system is threatened by Inversion Attack.") Annotation(rdfs:label "Model to Inversion Attack.") Body(ClassAtom(hes:Model Variable(:S)) ObjectPropertyAtom(hes:IsReverseEngineersBy Variable(:S) Variable(:A)) ClassAtom(hes:Adversary Variable(:A)) ClassAtom(hes:AMLA0020 Variable(:T)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:S) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a model’ output or behaviour is manipulated by an adversary, then the model is threatened by Output Integrity Attack.") Annotation(rdfs:label "Model to Output Integrity Attack.") Body(ClassAtom(hes:Model Variable(:D)) ObjectPropertyAtom(hes:IsManipulatedBy Variable(:D) Variable(:A)) ClassAtom(hes:Adversary Variable(:A)) ClassAtom(hes:AMLA0021 Variable(:T)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:D) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a model’ training data is manipulated by an adversary to expose sensitive information, then the model is threatened by Membership Inference Attack.") Annotation(rdfs:label "Training Data to Membership Inference Attack.") Body(ClassAtom(hes:Data Variable(:D)) ObjectPropertyAtom(hes:IsManipulatedBy Variable(:D) Variable(:A)) ClassAtom(hes:Adversary Variable(:A)) ClassAtom(hes:AMLA0022 Variable(:T)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:D) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If a model’  training data is manipulated by an adversary to behave in an undesirable way, then the model is threatened by Model Skewing Attacks.") Annotation(rdfs:label "Training Data to Model Skewing Attacks.") Body(ClassAtom(hes:Data Variable(:D)) ObjectPropertyAtom(hes:IsManipulatedBy Variable(:D) Variable(:A)) ClassAtom(hes:Adversary Variable(:A)) ClassAtom(hes:AMLA0023 Variable(:T)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:D) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If AI shared resource such as GPUs is manipulated by an adversary, then the AI resource is threatened by Resource Manipulation.") Annotation(rdfs:label "Shared Resource to Resource Manipulation.") Body(ClassAtom(hes:Resource Variable(:D)) ObjectPropertyAtom(hes:IsManipulatedBy Variable(:D) Variable(:A)) ClassAtom(hes:Adversary Variable(:A)) ClassAtom(hes:CAPECA0002 Variable(:T)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:D) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If an adversary intercepts or alters communication between two system, then the communication channel is threatened by Adversary in the Middle attack.") Annotation(rdfs:label "Communication Channel to Adversary in the Middle attack.") Body(ClassAtom(hes:System Variable(:S)) ObjectPropertyAtom(hes:AlterCommunicationOf Variable(:A) Variable(:S)) ClassAtom(hes:Adversary Variable(:A)) ClassAtom(hes:CAPECA0006 Variable(:T)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:S) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If an adversary manipulates the communication protocols, then the communication protocol is threatened by Protocol Manipulation.") Annotation(rdfs:label "Communication Protocol to Protocol Manipulation") Body(ClassAtom(hes:Protocol Variable(:D)) ObjectPropertyAtom(hes:IsManipulatedBy Variable(:D) Variable(:A)) ClassAtom(hes:Adversary Variable(:A)) ClassAtom(hes:CAPECA0007 Variable(:T)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:D) Variable(:T))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If an adversary allocates excessive resources, then the AI resources are threatened by Excessive Allocation.") Annotation(rdfs:label "AI Resources to Excessive Allocation.") Body(ClassAtom(hes:Adversary Variable(:Ad)) ObjectPropertyAtom(hes:Allocates Variable(:Ad) Variable(:R)) ClassAtom(hes:Resource Variable(:R)) ClassAtom(hes:CAPECA0009 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:R) Variable(:A))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If an adversary inserts malicious logic into hardware, then the hardware system is threatened by Malicious Logic Insertion.") Annotation(rdfs:label "Hardware to Malicious Logic Insertion.") Body(ClassAtom(hes:Hardware Variable(:H)) ClassAtom(hes:MaliciousLogic Variable(:F)) ObjectPropertyAtom(hes:IsInsertedInto Variable(:F) Variable(:H)) ClassAtom(hes:CAPECA0004 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:H) Variable(:A))))
DLSafeRule(Annotation(swrla:isRuleEnabled "true"^^xsd:boolean) Annotation(rdfs:comment "If an adversary injects unauthorized resources into a network, then the network resources are threatened by Resource Injection.") Annotation(rdfs:label "Network to Resource Injection.") Body(ClassAtom(hes:UnauthorizedResources Variable(:H)) ClassAtom(hes:Network Variable(:F)) ObjectPropertyAtom(hes:IsInjectedInto Variable(:H) Variable(:F)) ClassAtom(hes:CAPECA0010 Variable(:A)))Head(ObjectPropertyAtom(hes:IsThreatenedBy Variable(:F) Variable(:A))))
AnnotationAssertion(hes:Description _:genid2147484017 "context System
inv: self.incomingRequests->select(request | request.timestamp = self.currentTimestamp)->size() <= self.rateLimit")
AnnotationAssertion(rdfs:seeAlso _:genid2147484018 "")
)